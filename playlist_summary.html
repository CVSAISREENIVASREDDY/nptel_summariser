
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Title</title>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: "Product Sans", Arial, sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            padding: 2rem 1rem;
            max-width: 800px;
            font-size: clamp(1rem, 2.5vw, 1.3rem); /* Fluid font sizing */
            box-sizing: border-box;
        }
        h1 {
            text-align: center;
        }
        h2 {
            margin-top: 30px;
        }
        .chapter {
            text-align: justify;
        }
    </style>
</head>
<body>
<h1>Table of Contents</h1><a href="#week_1">Week 1</a><br><a href="#week_2">Week 2</a><br><a href="#week_3">Week 3</a><br><a href="#week_4">Week 4</a><br><a href="#week_5">Week 5</a><br><a href="#week_6">Week 6</a><br><a href="#week_7">Week 7</a><br><a href="#week_8">Week 8</a><br><a href="#week_9">Week 9</a><br><a href="#week_10">Week 10</a><br><a href="#week_11">Week 11</a><br><a href="#week_12">Week 12</a><br><div class="week" id="week_1"><h1 class="week-title">Week 1</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 1| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 1| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the fundamental concepts of linear algebra introduced in the lecture, focusing on vectors, their operations, and their applications in fields like signal processing, data analytics, and machine learning.</p>

<b><h3>1. Introduction to Vectors</h3></b>
<p>The lecture begins by establishing the vector as a fundamental concept in linear algebra, essential for representing data. A vector can be visualized as a point or a directed line segment in an n-dimensional space.</p>
<p>An <b>n-dimensional vector</b>, denoted as \\(\bar{u}\\), is an ordered collection of \\(n\\) numbers, called components or elements. It is typically represented as a column:</p>
\\[ \bar{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix} \\]
<p>The space to which a vector belongs depends on the nature of its components:</p>
<ul>
    <li><b>N-dimensional Real Space (\\(\mathbb{R}^n\\))</b>: If the components \\(u_1, u_2, \dots, u_n\\) are all real numbers, the vector \\(\bar{u}\\) belongs to the n-dimensional real space. This is the default space considered in many applications.</li>
    <li><b>N-dimensional Complex Space (\\(\mathbb{C}^n\\))</b>: If the components are complex numbers, the vector belongs to the n-dimensional complex space.</li>
</ul>

<b><h4>Applications of Vectors</h4></b>
<p>Vectors are versatile tools for representing various types of data:</p>
<ul>
    <li><b>Signals</b>: An analog signal sampled at \\(n\\) points in time can be represented as an n-dimensional vector, where each component is a sample value.</li>
    <li><b>Sensor Data</b>: A series of \\(n\\) measurements (e.g., temperature, pressure) from a sensor can be organized into an n-dimensional vector.</li>
    <li><b>Physical Space</b>: A location in 3D space is commonly represented by a 3-dimensional vector with x, y, and z coordinates: \\(\bar{u} = [u_1, u_2, u_3]\\).</li>
</ul>

<b><h3>2. Basic Vector Operations</h3></b>
<p>The lecture outlines two fundamental operations on vectors: addition and scalar multiplication.</p>

<b><h4>Vector Addition</h4></b>
<p>The sum of two n-dimensional vectors, \\(\bar{u}\\) and \\(\bar{v}\\), is another n-dimensional vector obtained by adding their corresponding components.</p>
<p>If \\(\bar{u} = [u_1, u_2, \dots, u_n]\\) and \\(\bar{v} = [v_1, v_2, \dots, v_n]\\), their sum is:</p>
\\[ \bar{u} + \bar{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix} \\]

<b><h4>Scalar Multiplication</h4></b>
<p>Multiplying a vector \\(\bar{u}\\) by a scalar (a single number) \\(k\\) results in a new vector where each component of \\(\bar{u}\\) is multiplied by \\(k\\).</p>
\\[ k\bar{u} = \begin{bmatrix} ku_1 \\ ku_2 \\ \vdots \\ ku_n \end{bmatrix} \\]

<b><h3>3. Linear Combination of Vectors</h3></b>
<p>A linear combination is a crucial concept formed by combining vector addition and scalar multiplication. Given a set of \\(m\\) vectors \\(\bar{u}_1, \bar{u}_2, \dots, \bar{u}_m\\) and a corresponding set of \\(m\\) scalars \\(k_1, k_2, \dots, k_m\\), their linear combination is:</p>
\\[ k_1\bar{u}_1 + k_2\bar{u}_2 + \dots + k_m\bar{u}_m \\]
<p>This operation involves scaling each vector by its corresponding scalar and then adding the resulting vectors together.</p>

<b><h3>4. The Inner Product</h3></b>
<p>The inner product (also known as the dot product) is an operation that takes two vectors and produces a single scalar value. It is fundamental for defining concepts like length, distance, and angle (orthogonality) between vectors.</p>

<b><h4>Inner Product for Real Vectors</h4></b>
<p>For two real vectors \\(\bar{u}, \bar{v} \in \mathbb{R}^n\\), the inner product is defined as \\(\bar{u}^T \bar{v}\\). Here, \\(\bar{u}^T\\) is the <b>transpose</b> of the column vector \\(\bar{u}\\), which converts it into a row vector. The operation is the sum of the products of corresponding components:</p>
\\[ \bar{u}^T \bar{v} = \begin{bmatrix} u_1 & u_2 & \dots & u_n \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \sum_{i=1}^{n} u_i v_i = u_1v_1 + u_2v_2 + \dots + u_nv_n \\]
<p><b>Example:</b> For \\(\bar{u} = [2, 1, -1]\\) and \\(\bar{v} = [1, -1, 3]\\):</p>
\\[ \bar{u}^T \bar{v} = (2)(1) + (1)(-1) + (-1)(3) = 2 - 1 - 3 = -2 \\]

<b><h4>Inner Product for Complex Vectors</h4></b>
<p>For complex vectors, the definition is slightly modified to ensure that the inner product of a vector with itself is a real number. It uses the <b>Hermitian transpose</b> (or conjugate transpose), denoted by \\(\bar{u}^H\\), which involves taking the transpose and the complex conjugate of each element.</p>
<p>For two complex vectors \\(\bar{u}, \bar{v} \in \mathbb{C}^n\\), the inner product is:</p>
\\[ \bar{u}^H \bar{v} = \sum_{i=1}^{n} u_i^* v_i = u_1^*v_1 + u_2^*v_2 + \dots + u_n^*v_n \\]
<p>where \\(u_i^*\\) is the complex conjugate of \\(u_i\\).</p>

<b><h3>5. Orthogonality</h3></b>
<p>Two vectors are said to be <b>orthogonal</b> if their inner product is zero. Geometrically, in 2D or 3D space, this means the vectors are perpendicular to each other.</p>
<ul>
    <li>For real vectors: \\(\bar{u}^T \bar{v} = 0\\)</li>
    <li>For complex vectors: \\(\bar{u}^H \bar{v} = 0\\)</li>
</ul>

<b><h4>Example: Orthogonality of Complex Sinusoids</h4></b>
<p>A very important application of orthogonality is found in Fourier analysis. Complex sinusoids with different discrete frequencies are orthogonal. Consider two vectors representing sinusoids with discrete frequencies \\(f_1 = 1/n\\) and \\(f_2 = 2/n\\). The \\(l\\)-th element (for \\(l=0, \dots, n-1\\)) of the first vector is \\(e^{j2\pi l/n}\\) and for the second vector is \\(e^{j4\pi l/n}\\).</p>
<p>Their inner product is calculated as:</p>
\\[ \bar{u}^H \bar{v} = \sum_{l=0}^{n-1} (e^{j2\pi l/n})^* (e^{j4\pi l/n}) = \sum_{l=0}^{n-1} e^{-j2\pi l/n} e^{j4\pi l/n} = \sum_{l=0}^{n-1} e^{j2\pi l/n} \\]
<p>This is a geometric series with ratio \\(r = e^{j2\pi/n}\\). The sum is:</p>
\\[ \frac{1 - (e^{j2\pi/n})^n}{1 - e^{j2\pi/n}} = \frac{1 - e^{j2\pi}}{1 - e^{j2\pi/n}} = \frac{1 - 1}{1 - e^{j2\pi/n}} = 0 \\]
<p>This result shows that the two sinusoids are orthogonal. This property is the cornerstone of Fourier analysis, which decomposes signals into a sum of orthogonal sinusoids.</p>

<b><h3>6. The Norm of a Vector</h3></b>
<p>The <b>norm</b> of a vector, denoted \\(\|\bar{u}\|\\), is a measure of its length or magnitude. It is defined as the square root of the inner product of the vector with itself.</p>

<b><h4>Norm for Real Vectors (L2-Norm)</h4></b>
<p>For a real vector \\(\bar{u}\\), the norm is the square root of the sum of the squares of its components:</p>
\\[ \|\bar{u}\| = \sqrt{\bar{u}^T\bar{u}} = \sqrt{u_1^2 + u_2^2 + \dots + u_n^2} \\]

<b><h4>Norm for Complex Vectors</h4></b>
<p>For a complex vector, the norm is the square root of the sum of the squared magnitudes of its components:</p>
\\[ \|\bar{u}\| = \sqrt{\bar{u}^H\bar{u}} = \sqrt{|u_1|^2 + |u_2|^2 + \dots + |u_n|^2} \\]

<b><h4>Properties of the Norm</h4></b>
<ul>
    <li>The norm is always non-negative: \\(\|\bar{u}\| \geq 0\\).</li>
    <li>The norm is zero if and only if the vector is the zero vector (a vector where all components are zero): \\(\|\bar{u}\| = 0 \iff \bar{u} = \mathbf{0}\\).</li>
</ul>
</div></div><div class="chapter" id="Lecture 2 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 2 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts discussed in the transcript, focusing on the norm of a vector, the distance between vectors, the Cauchy-Schwarz inequality, and the application of these concepts in signal processing, particularly in radar systems.</p>

<b>1. The Norm of a Vector and Unit Norm Vectors</b>
<p>The <b>norm</b> of a vector, denoted as \\( \|\bar{u}\| \\), is a measure of its length or magnitude. It is derived from the inner product of the vector with itself.</p>
<ul>
    <li><b>Unit Norm Vector:</b> A vector \\( \bar{u} \\) is called a <b>unit norm vector</b> (or simply a unit vector) if its norm is equal to 1.
    \\[ \|\bar{u}\| = 1 \\]
    </li>
    <li><b>Normalization:</b> Any non-zero vector \\( \bar{u} \\) can be converted into a unit norm vector by dividing it by its norm. This process is called normalization. The resulting unit vector, let's call it \\( \hat{u} \\), points in the same direction as \\( \bar{u} \\) but has a length of 1.
    \\[ \hat{u} = \frac{\bar{u}}{\|\bar{u}\|} \\]
    </li>
</ul>

<b>2. Norm and Signal Energy</b>
<p>In the context of signal processing, a signal can be represented as a vector where each component is a sample of the signal. The square of the norm of a signal vector has a significant physical interpretation: it represents the <b>total energy</b> of the signal.</p>
<p>For a real signal vector \\( \bar{u} = (u_1, u_2, \dots, u_n) \\), the energy is given by:</p>
\\[ \|\bar{u}\|^2 = u_1^2 + u_2^2 + \dots + u_n^2 \\]

<b>Example Calculation:</b>
<p>Consider the vector \\( \bar{u} = (-1, 3, -2, 1) \\).</p>
<ul>
    <li><b>Calculating the norm:</b> The norm is the square root of the sum of the squares of its components.
    \\[ \|\bar{u}\| = \sqrt{(-1)^2 + 3^2 + (-2)^2 + 1^2} = \sqrt{1 + 9 + 4 + 1} = \sqrt{15} \\]
    </li>
    <li><b>Finding the unit norm vector:</b> To normalize \\( \bar{u} \\), we divide each of its components by its norm, \\( \sqrt{15} \\).
    \\[ \hat{u} = \frac{1}{\sqrt{15}} \begin{pmatrix} -1 \\ 3 \\ -2 \\ 1 \end{pmatrix} \\]
    This new vector \\( \hat{u} \\) is a unit norm vector.
    </li>
</ul>

<b>3. Distance Between Two Vectors</b>
<p>The concept of the norm can be extended to define the distance between two vectors (or points) \\( \bar{u} \\) and \\( \bar{v} \\) in a vector space. The distance is defined as the norm of the difference between the two vectors.</p>
<p>For two vectors \\( \bar{u} = (u_1, \dots, u_n) \\) and \\( \bar{v} = (v_1, \dots, v_n) \\), the distance \\( d(\bar{u}, \bar{v}) \\) is:</p>
\\[ d(\bar{u}, \bar{v}) = \|\bar{u} - \bar{v}\| = \sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \dots + (u_n - v_n)^2} \\]
<p>This formula corresponds to the standard Euclidean distance between two points in n-dimensional space. The distance can also be expressed using the inner product:</p>
\\[ \|\bar{u} - \bar{v}\| = \sqrt{\langle \bar{u} - \bar{v}, \bar{u} - \bar{v} \rangle} \\]

<b>4. The Cauchy-Schwarz Inequality</b>
<p>The Cauchy-Schwarz inequality is a fundamental theorem in linear algebra that establishes a relationship between the inner product of two vectors and their individual norms. It provides an upper bound on the magnitude of the inner product.</p>
<p><b>General Form:</b> The magnitude of the inner product of two vectors \\( \bar{u} \\) and \\( \bar{v} \\) is less than or equal to the product of their norms.</p>
\\[ |\langle \bar{u}, \bar{v} \rangle| \le \|\bar{u}\| \|\bar{v}\| \\]
<p>This is often expressed in its squared form, which avoids square roots:</p>
\\[ |\langle \bar{u}, \bar{v} \rangle|^2 \le \|\bar{u}\|^2 \|\bar{v}\|^2 \\]
<p>The inequality has specific forms depending on whether the vectors are real or complex:</p>
<ul>
    <li>For <b>real vectors</b>, the inner product is the dot product (\\(\bar{u}^T \bar{v}\\)):
    \\[ (\bar{u}^T \bar{v})^2 \le \|\bar{u}\|^2 \|\bar{v}\|^2 \\]
    </li>
    <li>For <b>complex vectors</b>, the inner product uses the Hermitian transpose (\\(\bar{u}^H \bar{v}\\)):
    \\[ |\bar{u}^H \bar{v}|^2 \le \|\bar{u}\|^2 \|\bar{v}\|^2 \\]
    </li>
</ul>

<b>Proof for 2D Real Vectors:</b>
<p>The transcript provides a simple proof for the 2D real vector case. Let \\( \bar{u} = (u_1, u_2) \\) and \\( \bar{v} = (v_1, v_2) \\).</p>
<ol>
    <li>Start with the square of the inner product:
    \\[ (\bar{u}^T \bar{v})^2 = (u_1 v_1 + u_2 v_2)^2 = u_1^2 v_1^2 + u_2^2 v_2^2 + 2 u_1 u_2 v_1 v_2 \\]
    </li>
    <li>Apply the Arithmetic Mean-Geometric Mean (AM-GM) inequality. A form of this inequality states that for non-negative numbers \\(a\\) and \\(b\\), \\(2ab \le a^2 + b^2\\). Let \\( a = u_1 v_2 \\) and \\( b = u_2 v_1 \\). Then \\( 2(u_1 v_2)(u_2 v_1) \le (u_1 v_2)^2 + (u_2 v_1)^2 \\).
    \\[ 2 u_1 u_2 v_1 v_2 \le u_1^2 v_2^2 + u_2^2 v_1^2 \\]
    </li>
    <li>Substitute this inequality back into the expression for \\( (\bar{u}^T \bar{v})^2 \\):
    \\[ (\bar{u}^T \bar{v})^2 \le u_1^2 v_1^2 + u_2^2 v_2^2 + u_1^2 v_2^2 + u_2^2 v_1^2 \\]
    </li>
    <li>Rearrange and factor the terms on the right-hand side:
    \\[ u_1^2 v_1^2 + u_2^2 v_1^2 + u_1^2 v_2^2 + u_2^2 v_2^2 = (u_1^2 + u_2^2)(v_1^2 + v_2^2) \\]
    </li>
    <li>Recognize that these factors are the squared norms of \\( \bar{u} \\) and \\( \bar{v} \\):
    \\[ (u_1^2 + u_2^2)(v_1^2 + v_2^2) = \|\bar{u}\|^2 \|\bar{v}\|^2 \\]
    </li>
</ol>
<p>This completes the proof, showing that \\( (\bar{u}^T \bar{v})^2 \le \|\bar{u}\|^2 \|\bar{v}\|^2 \\).</p>

<b>5. Application: Correlation in Radar Signal Processing</b>
<p>The inner product has a powerful application as a measure of similarity or <b>correlation</b> between two signals. This is used extensively in fields like radar for target detection.</p>
<p><b>Scenario:</b> A radar system transmits a signal \\( \bar{x} \\) and listens for a reflected signal \\( \bar{y} \\).</p>
<ul>
    <li><b>Target Present:</b> If an object (target) is present, the received signal \\( \bar{y} \\) will be a reflected copy of the transmitted signal \\( \bar{x} \\) corrupted by noise \\( \bar{w} \\). The noise is often modeled as Additive White Gaussian Noise (AWGN).
    \\[ \bar{y} = \bar{x} + \bar{w} \\]
    In this case, \\( \bar{y} \\) is highly correlated with \\( \bar{x} \\), meaning their inner product will be large.
    </li>
    <li><b>Target Absent:</b> If there is no object, the received signal \\( \bar{y} \\) consists only of background noise.
    \\[ \bar{y} = \bar{w} \\]
    In this case, the received signal \\( \bar{y} \\) has low correlation with the transmitted signal \\( \bar{x} \\), and their inner product will be small.
    </li>
</ul>
<p><b>Detection Strategy (Hypothesis Testing):</b></p>
<p>To decide whether a target is present, the system calculates the correlation between the received signal \\( \bar{y} \\) and the known transmitted signal \\( \bar{x} \\). This value is then compared to a pre-defined <b>threshold</b> \\( \gamma \\).</p>
<p>The decision rule is:</p>
<ul>
    <li>If \\( |\bar{x}^H \bar{y}|^2 > \gamma \\) &rarr; <b>Target is present</b>.</li>
    <li>If \\( |\bar{x}^H \bar{y}|^2 \le \gamma \\) &rarr; <b>Target is absent</b>.</li>
</ul>
<p>This is a classic example of a <b>binary hypothesis testing</b> problem, where the system must decide between two possibilities (hypotheses). This same principle can be used to measure the similarity between any two data vectors, such as images or audio signals.</p>
</div></div><div class="chapter" id="Lecture 3| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 3| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains the concept of <b>beamforming</b> in wireless communication systems, demonstrating a practical and important application of the inner product. The explanation builds from a high-level concept to a detailed mathematical model, culminating in the formula for an optimal beamformer.</p>

<h3>1. What is Beamforming?</h3>
<p>Beamforming is a signal processing technique used in systems with multiple antennas (antenna arrays) to control the directionality of signal transmission or reception. The core idea is to combine the signals from the multiple antennas in a way that forms a focused, narrow "beam" of signal sensitivity in a specific direction. </p>
<p>
    <b>Key benefits mentioned:</b>
    <ul>
        <li><b>Enhanced Signal Quality:</b> By focusing the receiver's sensitivity towards the desired user, the strength of the user's signal is increased relative to background noise and interference. This significantly improves the <b>Signal-to-Noise Ratio (SNR)</b>.</li>
        <li><b>Gain Proportional to Antennas:</b> With \\(L\\) antennas, the SNR can theoretically be increased by a factor of \\(L\\). This is a substantial improvement in signal quality and a primary motivation for using multiple antennas.</li>
    </ul>
    This technology is a cornerstone of modern wireless standards like 4G, 5G, and Wi-Fi.
</p>

<h3>2. The System Model: Single-Input Multiple-Output (SIMO)</h3>
<p>The transcript models a simple yet fundamental wireless system to explain beamforming. This is a <b>Single-Input Multiple-Output (SIMO)</b> system.</p>
<p>
    <b>Components:</b>
    <ul>
        <li>A single user (e.g., a mobile phone) with one antenna, transmitting a symbol \\(x\\). This is the "single input".</li>
        <li>A base station with \\(L\\) antennas, receiving the signal. This is the "multiple output".</li>
        <li>The signal from the user to each of the \\(L\\) receiver antennas travels through a different path. The effect of each path on the signal is modeled by a <b>channel coefficient</b>, denoted \\(h_i\\) for the \\(i^{th}\\) antenna. These are often complex numbers representing both attenuation and phase shift.</li>
        <li>Each antenna also picks up random noise, denoted \\(n_i\\).</li>
    </ul>
</p>
<p>The relationship between the transmitted symbol \\(x\\) and the symbol received at the \\(i^{th}\\) antenna, \\(y_i\\), is given by the following linear model:</p>
\\[ y_i = h_i x + n_i \\]
<p>Here:</p>
<ul>
    <li>\\(y_i\\) is the complex symbol received at antenna \\(i\\).</li>
    <li>\\(h_i\\) is the complex channel coefficient for the path to antenna \\(i\\).</li>
    <li>\\(x\\) is the complex symbol transmitted by the user.</li>
    <li>\\(n_i\\) is the complex noise received at antenna \\(i\\).</li>
</ul>

<h3>3. The Vector Model of the System</h3>
<p>To handle the signals from all \\(L\\) antennas simultaneously, the individual equations are "stacked" into a vector form. This provides a compact and powerful representation.</p>
<p>
    The received signal vector \\(\bar{y}\\), channel vector \\(\bar{h}\\), and noise vector \\(\bar{n}\\) are defined as:
    \\[ \bar{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_L \end{bmatrix}, \quad \bar{h} = \begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_L \end{bmatrix}, \quad \bar{n} = \begin{bmatrix} n_1 \\ n_2 \\ \vdots \\ n_L \end{bmatrix} \\]
</p>
<p>Using these vectors, the entire system of \\(L\\) equations can be written as a single vector equation:</p>
\\[ \bar{y} = \bar{h}x + \bar{n} \\]

<h3>4. Beamforming as a Weighted Combination (Inner Product)</h3>
<p>
    The core of beamforming at the receiver is to combine the \\(L\\) received signals (\\(y_1, y_2, \dots, y_L\\)) into a single, higher-quality output. This is done by calculating a <b>weighted linear combination</b> of the received signals. Each signal \\(y_i\\) is multiplied by a complex weight \\(w_i^*\\) (the conjugate of \\(w_i\\)) and then summed up.
</p>
<p>The combined output is:</p>
\\[ w_1^* y_1 + w_2^* y_2 + \dots + w_L^* y_L \\]
<p>This operation can be elegantly expressed using vector notation. Let \\(\bar{w}\\) be the vector of weights, called the <b>beamformer</b> or <b>beamforming vector</b>:</p>
\\[ \bar{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_L \end{bmatrix} \\]
<p>The weighted sum is then the <b>inner product</b> of the beamforming vector \\(\bar{w}\\) and the received signal vector \\(\bar{y}\\). This is written using the Hermitian transpose (conjugate transpose) of \\(\bar{w}\\), denoted \\(\bar{w}^H\\):</p>
\\[ \bar{w}^H \bar{y} = \begin{bmatrix} w_1^* & w_2^* & \dots & w_L^* \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_L \end{bmatrix} \\]
<p>This shows that the mathematical foundation of receive beamforming is the inner product. The goal is to choose the vector \\(\bar{w}\\) in a way that optimizes the output signal quality.</p>

<h3>5. Optimal Beamforming: Maximal Ratio Combining (MRC)</h3>
<p>The key question is how to choose the weights in the vector \\(\bar{w}\\) to achieve the best possible signal quality. "Optimal" in this context means choosing \\(\bar{w}\\) to maximize the Signal-to-Noise Ratio (SNR) of the combined signal.</p>
<p>The transcript presents the optimal beamforming vector, which is known as the <b>Maximal Ratio Combiner (MRC)</b>. The formula for the optimal \\(\bar{w}\\) is:</p>
\\[ \bar{w}_{\text{opt}} = \frac{\bar{h}}{||\bar{h}||} \\]
<p>Let's break down this important result:</p>
<ul>
    <li><b>Matched Filter:</b> The optimal beamformer \\(\bar{w}_{\text{opt}}\\) is proportional to the channel vector \\(\bar{h}\\). This means the beamformer is "matched" to the channel conditions. It aligns with the channel to coherently combine the signal components from all antennas, effectively amplifying the signal as much as possible.</li>
    <li><b>Unit Norm Vector:</b> The vector \\(\bar{h}\\) is divided by its norm, \\(||\bar{h}||\\). This normalization ensures that the beamforming vector \\(\bar{w}_{\text{opt}}\\) has a length (norm) of 1. This is a standard constraint to prevent the beamformer from adding arbitrary amounts of power.</li>
</ul>
<p>In summary, the MRC technique provides the highest possible SNR at the receiver in a SIMO system by choosing a beamforming vector that is a normalized version of the channel vector itself. This is a classic and powerful result in wireless communications, directly linking the concept of an inner product to a method for optimizing signal reception.</p>
</div></div><div class="chapter" id="Lecture 4 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 4 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the fundamental concepts of matrices as introduced in the transcript, including their definition, notation, and basic operations such as addition, scalar multiplication, and matrix multiplication.</p>

<b>1. Introduction to Matrices</b>
<p>A matrix is a fundamental object in linear algebra. It is a rectangular array or grid of numbers, symbols, or expressions, arranged in rows and columns.</p>

<p><b>Definition:</b> An \\( m \times n \\) (read as "m by n") matrix is a rectangular array of scalars with \\( m \\) rows and \\( n \\) columns. If \\( n = 1 \\), the matrix is a column vector.</p>

<p>A general \\( m \times n \\) matrix, denoted by \\( A \\), is written as:</p>
\\[
A =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\\]
<p>Here:</p>
<ul>
    <li>\\( m \\) is the number of rows.</li>
    <li>\\( n \\) is the number of columns.</li>
    <li>\\( a_{ij} \\) represents the element (or entry, coefficient) in the <b>i-th row</b> and the <b>j-th column</b>. The first subscript, \\( i \\), is the <b>row index</b> (\\( 1 \le i \le m \\)), and the second subscript, \\( j \\), is the <b>column index</b> (\\( 1 \le j \le n \\)).</li>
</ul>

<p>A matrix can also be viewed as a collection of column vectors placed side-by-side. If we let \\( \mathbf{a_j} \\) be the j-th column vector, the matrix \\( A \\) can be represented as:</p>
\\[ A = \begin{bmatrix} | & | & & | \\ \mathbf{a_1} & \mathbf{a_2} & \cdots & \mathbf{a_n} \\ | & | & & | \end{bmatrix} \\]
<p>where each \\( \mathbf{a_j} \\) is a vector of size \\( m \times 1 \\).</p>

<p><b>Example:</b></p>
<p>Consider the matrix:</p>
\\[ A = \begin{pmatrix} 3 & -1 & 2 \\ -2 & 4 & 3 \end{pmatrix} \\]
<p>This is a \\( 2 \times 3 \\) matrix because it has 2 rows and 3 columns. Some of its elements are:</p>
<ul>
    <li>\\( a_{22} \\) (the element in the second row, second column) is 4.</li>
    <li>\\( a_{23} \\) (the element in the second row, third column) is 3.</li>
</ul>

<p><b>Types of Matrices (based on elements):</b></p>
<ul>
    <li><b>Real Matrix:</b> A matrix is called a real matrix if all of its elements \\( a_{ij} \\) are real numbers. We use the notation \\( A \in \mathbb{R}^{m \times n} \\) to denote that \\( A \\) is a member of the set of all \\( m \times n \\) real matrices.</li>
    <li><b>Complex Matrix:</b> A matrix is called a complex matrix if its elements can be complex numbers. We use the notation \\( A \in \mathbb{C}^{m \times n} \\) for this set.</li>
</ul>

<b>2. Basic Matrix Operations</b>

<b>a) Matrix Addition</b>
<p>Two matrices can be added together only if they have the exact same dimensions (i.e., the same number of rows and the same number of columns).</p>
<p><b>Rule:</b> If \\( A \\) and \\( B \\) are both \\( m \times n \\) matrices, their sum, \\( C = A + B \\), is also an \\( m \times n \\) matrix. The addition is performed element-wise.</p>
<p><b>Formula:</b> The element in the i-th row and j-th column of the resulting matrix \\( C \\) is the sum of the corresponding elements in \\( A \\) and \\( B \\).</p>
\\[ c_{ij} = a_{ij} + b_{ij} \\]

<p><b>Example:</b></p>
<p>Let \\( A = \begin{pmatrix} 3 & -1 & 2 \\ -2 & 4 & 3 \end{pmatrix} \\) and \\( B = \begin{pmatrix} 4 & -3 & 6 \\ 2 & 2 & -1 \end{pmatrix} \\). Both are \\( 2 \times 3 \\) matrices.</p>
<p>Their sum \\( C = A + B \\) is:</p>
\\[
C = \begin{pmatrix} 3+4 & -1+(-3) & 2+6 \\ -2+2 & 4+2 & 3+(-1) \end{pmatrix} = \begin{pmatrix} 7 & -4 & 8 \\ 0 & 6 & 2 \end{pmatrix}
\\]

<b>b) Scalar Multiplication</b>
<p>Any matrix can be multiplied by a scalar (a single number). The result is a new matrix of the same dimensions.</p>
<p><b>Rule:</b> To multiply a matrix \\( A \\) by a scalar \\( k \\), you multiply every element of \\( A \\) by \\( k \\).</p>
<p><b>Formula:</b> If \\( C = kA \\), then the elements of \\( C \\) are given by:</p>
\\[ c_{ij} = k \cdot a_{ij} \\]

<p><b>Example:</b></p>
<p>Let \\( A = \begin{pmatrix} 3 & -1 & 2 \\ -2 & 4 & 3 \end{pmatrix} \\) and the scalar \\( k = 2 \\).</p>
<p>Then \\( 2A \\) is:</p>
\\[
2A = 2 \cdot \begin{pmatrix} 3 & -1 & 2 \\ -2 & 4 & 3 \end{pmatrix} = \begin{pmatrix} 2 \cdot 3 & 2 \cdot (-1) & 2 \cdot 2 \\ 2 \cdot (-2) & 2 \cdot 4 & 2 \cdot 3 \end{pmatrix} = \begin{pmatrix} 6 & -2 & 4 \\ -4 & 8 & 6 \end{pmatrix}
\\]

<b>c) Matrix Multiplication</b>
<p>Matrix multiplication is more complex than addition or scalar multiplication. Two matrices \\( A \\) and \\( B \\) can be multiplied to form the product \\( AB \\) only if a specific condition on their dimensions is met.</p>
<p><b>Condition:</b> For the product \\( C = AB \\) to be defined, the number of columns of the first matrix (\\( A \\)) must be equal to the number of rows of the second matrix (\\( B \\)).</p>
<p>If \\( A \\) is an \\( m \times n \\) matrix and \\( B \\) is a \\( p \times q \\) matrix, the product \\( AB \\) is defined only if \\( n = p \\). The resulting matrix \\( C \\) will have dimensions \\( m \times q \\).</p>

<p><b>Formula:</b> The element \\( c_{ij} \\) in the i-th row and j-th column of the product matrix \\( C = AB \\) is calculated by taking the <b>inner product</b> (or dot product) of the <b>i-th row of A</b> and the <b>j-th column of B</b>.</p>
<p>The formula for \\( c_{ij} \\) is:</p>
\\[ c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} \\]

<p><b>Example:</b></p>
<p>Let \\( A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \\) and \\( B = \begin{pmatrix} 1 & -2 \\ -1 & 1 \\ 1 & 3 \end{pmatrix} \\).</p>
<ul>
    <li>\\( A \\) is a \\( 2 \times 3 \\) matrix.</li>
    <li>\\( B \\) is a \\( 3 \times 2 \\) matrix.</li>
</ul>
<p>The number of columns of \\( A \\) (3) equals the number of rows of \\( B \\) (3), so the product \\( C = AB \\) is defined. The resulting matrix \\( C \\) will be of size \\( 2 \times 2 \\).</p>

<p>Let's calculate the elements of \\( C = \begin{pmatrix} c_{11} & c_{12} \\ c_{21} & c_{22} \end{pmatrix} \\):</p>
<ul>
    <li><b>\\( c_{11} \\)</b>: Inner product of the 1st row of A and the 1st column of B.<br>
    \\( c_{11} = (1 \cdot 1) + (2 \cdot -1) + (3 \cdot 1) = 1 - 2 + 3 = 2 \\)</li>
    <br>
    <li><b>\\( c_{12} \\)</b>: Inner product of the 1st row of A and the 2nd column of B.<br>
    \\( c_{12} = (1 \cdot -2) + (2 \cdot 1) + (3 \cdot 3) = -2 + 2 + 9 = 9 \\)</li>
    <br>
    <li><b>\\( c_{21} \\)</b>: Inner product of the 2nd row of A and the 1st column of B.<br>
    \\( c_{21} = (4 \cdot 1) + (5 \cdot -1) + (6 \cdot 1) = 4 - 5 + 6 = 5 \\)</li>
    <br>
    <li><b>\\( c_{22} \\)</b>: Inner product of the 2nd row of A and the 2nd column of B.<br>
    \\( c_{22} = (4 \cdot -2) + (5 \cdot 1) + (6 \cdot 3) = -8 + 5 + 18 = 15 \\)</li>
</ul>

<p>So, the final product is:</p>
\\[ AB = \begin{pmatrix} 2 & 9 \\ 5 & 15 \end{pmatrix} \\]

</div></div><div class="chapter" id="Lecture 5 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 5 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts discussed in the transcript, focusing on the rank of a matrix, column space, linear independence, and the process of Gaussian elimination to determine the rank.</p>

<b>1. The Matrix and its Column Vectors</b>
<p>The discussion begins by considering a general \\(m \times n\\) matrix, denoted as <b>A</b>. This matrix has \\(m\\) rows and \\(n\\) columns.</p>
\\[
A =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\\]
<p>An important perspective is to view this matrix as a collection of \\(n\\) column vectors, where each column is a vector of size \\(m \times 1\\). These column vectors are denoted as \\(\bar{A_1}, \bar{A_2}, \dots, \bar{A_n}\\).</p>
\\[
A = \begin{bmatrix} | & | & & | \\ \bar{A_1} & \bar{A_2} & \cdots & \bar{A_n} \\ | & | & & | \end{bmatrix}
\\]

<b>2. Column Space</b>
<p>The <b>column space</b> of a matrix <b>A</b>, denoted as \\(C(A)\\), is the set of all possible vectors that can be formed by a linear combination of its column vectors.</p>
<p>A vector \\(\bar{y}\\) is in the column space of <b>A</b> if it can be expressed as the product of the matrix <b>A</b> and some vector \\(\bar{x}\\):</p>
\\[ \bar{y} = A\bar{x} \\]
<p>If we expand this matrix-vector multiplication, we see that \\(\bar{y}\\) is a weighted sum of the columns of <b>A</b>, where the weights are the components of the vector \\(\bar{x}\\).</p>
\\[
\bar{y} =
\begin{bmatrix} | & | & & | \\ \bar{A_1} & \bar{A_2} & \cdots & \bar{A_n} \\ | & | & & | \end{bmatrix}
\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
= x_1\bar{A_1} + x_2\bar{A_2} + \cdots + x_n\bar{A_n}
\\]
<p>Therefore, the column space \\(C(A)\\) is the subspace spanned by the columns of <b>A</b>. It contains every possible linear combination of the matrix's column vectors.</p>

<b>3. Linear Independence</b>
<p>A crucial concept for understanding the rank is <b>linear independence</b>. A set of vectors \\(\{\bar{A_1}, \bar{A_2}, \dots, \bar{A_n}\}\\) is said to be <b>linearly independent</b> if the only way their linear combination can equal the zero vector is if all the coefficients (scalars) are zero.</p>
<p>Mathematically, the equation:</p>
\\[ x_1\bar{A_1} + x_2\bar{A_2} + \cdots + x_n\bar{A_n} = \bar{0} \\]
<p>is true <i>if and only if</i> \\(x_1 = x_2 = \cdots = x_n = 0\\).</p>
<p>Conversely, if there exists a set of scalars \\(x_1, x_2, \dots, x_n\\), where at least one is non-zero, that satisfies the equation above, the set of vectors is called <b>linearly dependent</b>. This means at least one vector in the set can be expressed as a linear combination of the others.</p>

<p><b>Example of Linear Dependence:</b></p>
<p>Consider the three vectors:</p>
\\[ \bar{A_1} = \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix}, \quad \bar{A_2} = \begin{pmatrix} 2 \\ 1 \\ -3 \end{pmatrix}, \quad \bar{A_3} = \begin{pmatrix} 7 \\ -1 \\ -3 \end{pmatrix} \\]
<p>The transcript shows that a linear combination of \\(\bar{A_1}\\) and \\(\bar{A_2}\\) can form \\(\bar{A_3}\\):</p>
\\[ 3\bar{A_1} + 2\bar{A_2} = 3\begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} + 2\begin{pmatrix} 2 \\ 1 \\ -3 \end{pmatrix} = \begin{pmatrix} 3 \\ -3 \\ 3 \end{pmatrix} + \begin{pmatrix} 4 \\ 2 \\ -6 \end{pmatrix} = \begin{pmatrix} 7 \\ -1 \\ -3 \end{pmatrix} = \bar{A_3} \\]
<p>By rearranging this equation, we get a linear combination that equals the zero vector, with non-zero coefficients:</p>
\\[ 3\bar{A_1} + 2\bar{A_2} - \bar{A_3} = \bar{0} \\]
<p>Since the coefficients \\(x_1=3, x_2=2, x_3=-1\\) are not all zero, the vectors \\(\bar{A_1}, \bar{A_2}, \bar{A_3}\\) are <b>linearly dependent</b>.</p>

<b>4. The Rank of a Matrix</b>
<p>The <b>rank</b> of a matrix <b>A</b> is a fundamental property that is defined in two equivalent ways:</p>
<ol>
    <li><b>Column Rank:</b> The maximum number of linearly independent columns in the matrix.</li>
    <li><b>Row Rank:</b> The maximum number of linearly independent rows in the matrix.</li>
</ol>
<p>A key theorem in linear algebra states that for any matrix, the column rank is equal to the row rank. This common value is simply called the rank of the matrix.</p>
\\[ \text{Column Rank} = \text{Row Rank} = \text{Rank}(A) \\]
<p>The rank of an \\(m \times n\\) matrix is always less than or equal to the smaller of its dimensions.</p>
\\[ \text{Rank}(A) \le \min(m, n) \\]

<b>5. Determining the Rank via Gaussian Elimination</b>
<p>A systematic procedure to find the rank of a matrix is <b>Gaussian elimination</b>, which uses a series of elementary row operations to transform the matrix into a simpler form called <b>Row Echelon Form</b>.</p>
<p>Let's use the matrix formed by the vectors from the previous example:</p>
\\[ A = \begin{pmatrix} 1 & 2 & 7 \\ -1 & 1 & -1 \\ 1 & -3 & -3 \end{pmatrix} \\]
<p>The goal of the row operations is to create zeros below the leading non-zero element of each row.</p>

<p><b>Step 1:</b> Replace Row 2 with (Row 2 + Row 1). Operation: \\(R_2 \to R_2 + R_1\\)</p>
\\[ \begin{pmatrix} 1 & 2 & 7 \\ 0 & 3 & 6 \\ 1 & -3 & -3 \end{pmatrix} \\]

<p><b>Step 2:</b> Simplify Row 2 by dividing by 3. Operation: \\(R_2 \to R_2 / 3\\)</p>
\\[ \begin{pmatrix} 1 & 2 & 7 \\ 0 & 1 & 2 \\ 1 & -3 & -3 \end{pmatrix} \\]

<p><b>Step 3:</b> Replace Row 3 with (Row 3 - Row 1). Operation: \\(R_3 \to R_3 - R_1\\)</p>
\\[ \begin{pmatrix} 1 & 2 & 7 \\ 0 & 1 & 2 \\ 0 & -5 & -10 \end{pmatrix} \\]

<p><b>Step 4:</b> Replace Row 3 with (Row 3 + 5 * Row 2). Operation: \\(R_3 \to R_3 + 5R_2\\)</p>
\\[ \begin{pmatrix} 1 & 2 & 7 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix} \\]

<p>This final matrix is in <b>Row Echelon Form</b>. The leading non-zero entries in each non-zero row are called <b>pivots</b>. In this case, the pivots are the '1' in the first row and the '1' in the second row.</p>
<p>The rank of the matrix is equal to the number of non-zero rows (or the number of pivots) in its row echelon form. Since there are two non-zero rows, the rank is 2.</p>
\\[ \text{Rank}(A) = 2 \\]
<p>This result confirms our earlier finding: the three column vectors are linearly dependent, but the maximum number of linearly independent vectors among them is two (e.g., \\(\bar{A_1}\\) and \\(\bar{A_2}\\) are linearly independent).</p>
</div></div><h2>Weekly Summary</h2><div>
<p>This week's lectures introduce the foundational concepts of linear algebra, starting with vectors and their applications, and then moving to matrices and their fundamental properties. The content is framed within the context of signal processing, data analytics, and machine learning.</p>

<b>1. Vectors: Representation and Basic Operations</b><br>
<p>
    <i>Main Topics:</i> The course begins by establishing vectors as the fundamental tool for representing data. An n-dimensional vector \\(\bar{u} = [u_1, u_2, \dots, u_n]\\) is defined as a point in an n-dimensional space, which can be real \\(\mathbb{R}^n\\) or complex \\(\mathbb{C}^n\\). Practical examples include representing signal samples, sensor readings, or image data. The lectures cover basic vector operations:
<ul>
<li><b>Vector Addition:</b> An element-wise operation, \\(\bar{u} + \bar{v} = [u_1+v_1, u_2+v_2, \dots, u_n+v_n]\\).</li>
<li><b>Scalar Multiplication:</b> Multiplying each component of a vector by a scalar \\(k\\), \\(k\bar{u} = [ku_1, ku_2, \dots, ku_n]\\).</li>
<li><b>Linear Combination:</b> A weighted sum of vectors, e.g., \\(k_1\bar{u}_1 + k_2\bar{u}_2 + \dots + k_m\bar{u}_m\\).</li>
</ul>
</p>
<p>
    <i>Key Takeaways:</i> Vectors are not just abstract mathematical objects but are essential for representing real-world data in a structured way. Basic arithmetic operations on vectors form the building blocks for more complex algorithms in data science and signal processing.
</p>

<b>2. Inner Product, Orthogonality, and Norm</b><br>
<p>
    <i>Main Topics:</i> The concept of the inner product is introduced as a way to measure the relationship between two vectors.
<ul>
<li>For real vectors, the inner product is \\(\bar{u}^T\bar{v} = \sum_{i=1}^{n} u_i v_i\\).</li>
<li>For complex vectors, it is \\(\bar{u}^H\bar{v} = \sum_{i=1}^{n} u_i^* v_i\\), where \\(u_i^*\\) is the complex conjugate.</li>
</ul>
Two vectors are <b>orthogonal</b> if their inner product is zero. A key example is the orthogonality of complex sinusoids at different frequencies, which is the foundation of Fourier Analysis. The <b>norm</b> (or L2-norm) of a vector, \\(\|\bar{u}\| = \sqrt{\bar{u}^H\bar{u}}\\), is defined as its length or magnitude. For a signal vector, the squared norm \\(\|\bar{u}\|^2\\) represents the signal's energy.
</p>
<p>
    <i>Key Takeaways:</i> The inner product is a powerful tool for measuring similarity or correlation between vectors. Orthogonality is a special case of zero similarity that is fundamental in many areas, including signal decomposition (e.g., Fourier Transform). The norm provides a measure of a vector's size or a signal's energy.
</p>

<b>3. Applications of the Inner Product and the Cauchy-Schwarz Inequality</b><br>
<p>
    <i>Main Topics:</i> The lectures highlight the <b>Cauchy-Schwarz inequality</b>, which states that the magnitude of the inner product is bounded by the product of the vector norms: \\(|\bar{u}^H \bar{v}|^2 \le \|\bar{u}\|^2 \|\bar{v}\|^2\\). Practical applications of the inner product (correlation) are discussed in detail:
<ul>
<li><b>Radar Detection:</b> To detect a target, a radar system correlates the received signal with the transmitted signal. A high correlation value suggests a target is present, forming a basis for hypothesis testing.</li>
<li><b>Wireless Beamforming:</b> In systems with multiple antennas (SIMO), a received signal vector \\(\bar{y}\\) is combined using a beamforming vector \\(\bar{w}\\). The operation is an inner product \\(\bar{w}^H\bar{y}\\). The optimal beamformer that maximizes signal quality is the <b>Maximal Ratio Combiner (MRC)</b>, which is a normalized version of the channel vector, \\(\bar{w} = \bar{h} / \|\bar{h}\|\\).</li>
</ul>
</p>
<p>
    <i>Key Takeaways:</i> The inner product is a core computational tool in engineering. It is used for signal detection in radar by measuring similarity and for optimizing signal reception in modern wireless communications (4G/5G) through beamforming.
</p>

<b>4. Introduction to Matrices and their Operations</b><br>
<p>
    <i>Main Topics:</i> Matrices are introduced as \\(m \times n\\) rectangular arrays of scalars, which can be viewed as collections of column vectors. Basic matrix operations are defined:
<ul>
<li><b>Addition:</b> Element-wise, defined only for matrices of the same dimensions.</li>
<li><b>Scalar Multiplication:</b> Multiplying every element of the matrix by a scalar.</li>
<li><b>Matrix Multiplication:</b> The product \\(C = AB\\) is defined if the number of columns in \\(A\\) equals the number of rows in \\(B\\). The resulting element \\(c_{ij}\\) is the inner product of the i-th row of \\(A\\) and the j-th column of \\(B\\).</li>
</ul>
</p>
<p>
    <i>Key Takeaways:</i> Matrices provide a way to organize data and represent linear transformations. Matrix multiplication is a key operation that combines information from two matrices based on a series of inner products.
</p>

<b>5. The Rank of a Matrix</b><br>
<p>
    <i>Main Topics:</i> The concept of <b>linear independence</b> is introduced to define the rank. A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. The <b>rank</b> of a matrix is the maximum number of linearly independent columns (or, equivalently, rows). The rank can be determined systematically using <b>Gaussian elimination</b> to transform the matrix into <b>row echelon form</b>.
</p>
<p>
    <i>Key Takeaways:</i> The rank is a fundamental property of a matrix that describes the dimension of the space spanned by its columns (or rows). A low rank implies redundancy in the data represented by the matrix. Gaussian elimination is the standard algorithm to compute the rank by identifying the number of non-zero rows (pivots) in the matrix's echelon form.
</p>
</div><h2>Assignment Explanation</h2><div>
<p>This document explains the questions and answers from the "Week 1 - Assignment-1" of the NPTEL course "Applied Linear Algebra for Signal Processing, Data Analytics and Machine Learning".</p>

<p><b>Question 1:</b> Consider the real vector \\(\bar{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\\). Its Euclidean norm is</p>
<p><b>Answer:</b> \\(\sqrt{x_1^2 + x_2^2 + \dots + x_n^2}\\)</p>
<p><b>Explanation:</b> The Euclidean norm (also known as the \\(L_2\\)-norm) of a real vector is a measure of its length or magnitude. It is calculated by taking the square root of the sum of the squares of its components. This is a direct application of the definition of the Euclidean norm.</p>
<br>

<p><b>Question 2:</b> Consider the complex sinusoidal vector \\(\bar{u}(f) = [1, e^{j2\pi f}, e^{j4\pi f}, \dots, e^{j2(N-1)\pi f}]^T\\). Then, \\(\bar{u}^H(\frac{k}{N}) \bar{u}(\frac{l}{N})\\), for \\(l \neq k\\), equals</p>
<p><b>Answer:</b> 0</p>
<p><b>Explanation:</b> This question is about the orthogonality of Discrete Fourier Transform (DFT) basis vectors. The vectors \\(\bar{u}(\frac{k}{N})\\) and \\(\bar{u}(\frac{l}{N})\\) are two distinct basis vectors for the DFT. The expression \\(\bar{u}^H(\frac{k}{N}) \bar{u}(\frac{l}{N})\\) represents the inner product of these two vectors. A fundamental property of DFT basis vectors is that they are orthogonal to each other. The inner product of any two distinct orthogonal vectors is, by definition, zero.</p>
<br>

<p><b>Question 3:</b> The vectors \\(\bar{u} = \begin{bmatrix} 1 \\ -2 \\ 3 \end{bmatrix}\\) and \\(\bar{v} = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}\\) are</p>
<p><b>Answer:</b> Both orthogonal and linearly independent</p>
<p><b>Explanation:</b>
<ul>
<li><b>Orthogonality:</b> Two vectors are orthogonal if their inner product (dot product for real vectors) is zero.
\\[\bar{u}^T\bar{v} = (1)(1) + (-2)(2) + (3)(1) = 1 - 4 + 3 = 0\\]
Since the inner product is 0, the vectors are orthogonal.</li>
<li><b>Linear Independence:</b> A set of non-zero orthogonal vectors is always linearly independent. Since both \\(\bar{u}\\) and \\(\bar{v}\\) are non-zero and we have shown they are orthogonal, they must also be linearly independent.</li>
</ul>
</p>
<br>

<p><b>Question 4:</b> Consider the real vectors \\(\bar{u}\\) and \\(\bar{v}\\) of size \\(n \times 1\\). The inner product of \\(\bar{u}\\) and \\(\bar{v}\\) is defined as</p>
<p><b>Answer:</b> \\(\bar{u}^T\bar{v}\\)</p>
<p><b>Explanation:</b> The standard inner product (or dot product) for two real column vectors is defined as the matrix product of the transpose of the first vector with the second vector. If \\(\bar{u}\\) is an \\(n \times 1\\) vector, its transpose \\(\bar{u}^T\\) is a \\(1 \times n\\) row vector. Multiplying a \\(1 \times n\\) vector by an \\(n \times 1\\) vector results in a \\(1 \times 1\\) scalar, which is the value of the inner product.</p>
<br>

<p><b>Question 5:</b> Orthogonal vectors \\(\bar{u} = [u_1, u_2, \dots, u_n]^T\\) and \\(\bar{v} = [v_1, v_2, \dots, v_n]^T\\) satisfy the property</p>
<p><b>Answer:</b> \\(\bar{u}^H\bar{v} = 0\\)</p>
<p><b>Explanation:</b> Two vectors are orthogonal if their inner product is zero. For complex vectors, the inner product is defined using the conjugate transpose (Hermitian transpose), denoted by \\(H\\). The inner product of \\(\bar{u}\\) and \\(\bar{v}\\) is \\(\langle \bar{u}, \bar{v} \rangle = \bar{u}^H\bar{v}\\). Therefore, the orthogonality condition is \\(\bar{u}^H\bar{v} = 0\\). This definition also holds for real vectors, as the conjugate transpose is the same as the regular transpose for real numbers.</p>
<br>

<p><b>Question 6:</b> The linear equation \\(3x_1 - 5x_2 = -1\\) has</p>
<p><b>Answer:</b> an infinite number of solutions</p>
<p><b>Explanation:</b> This is a single linear equation with two variables. Geometrically, it represents a line in a two-dimensional plane. A line consists of an infinite number of points. Algebraically, you can express one variable in terms of the other (e.g., \\(x_1 = \frac{5x_2 - 1}{3}\\)). For every real number you choose for \\(x_2\\), you can calculate a corresponding value for \\(x_1\\). Since there are infinite choices for \\(x_2\\), there are infinitely many solutions \\((x_1, x_2)\\) that satisfy the equation.</p>
<br>

<p><b>Question 7:</b> Consider the complex vector \\(\bar{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\\). Its Euclidean norm is</p>
<p><b>Answer:</b> \\(\sqrt{|x_1|^2 + |x_2|^2 + \dots + |x_n|^2}\\)</p>
<p><b>Explanation:</b> For a complex vector, the Euclidean norm is defined as the square root of the inner product of the vector with itself: \\(\Vert\bar{x}\Vert = \sqrt{\bar{x}^H \bar{x}}\\). The inner product \\(\bar{x}^H \bar{x}\\) is the sum of the products of the conjugate of each component with the component itself: \\(\sum_{i=1}^n x_i^* x_i\\). For any complex number \\(z\\), \\(z^*z = |z|^2\\) (the squared modulus). Thus, the norm is \\(\sqrt{\sum_{i=1}^n |x_i|^2}\\).</p>
<br>

<p><b>Question 8:</b> The Cauchy-Schwarz inequality for complex vectors \\(\bar{u}\\) and \\(\bar{v}\\) of size \\(n \times 1\\) is given as</p>
<p><b>Answer:</b> \\(|\bar{u}^H\bar{v}| \le \Vert\bar{u}\Vert \Vert\bar{v}\Vert\\)</p>
<p><b>Explanation:</b> The Cauchy-Schwarz inequality provides an upper bound on the magnitude of the inner product of two vectors. It states that the modulus (or absolute value) of the inner product of two vectors (\\(\bar{u}^H\bar{v}\\) for complex vectors) is less than or equal to the product of their norms (\\(\Vert\bar{u}\Vert\\) and \\(\Vert\bar{v}\Vert\\)).</p>
<br>

<p><b>Question 9:</b> Consider the matrix \\(\begin{bmatrix} 3 & -6 & 7 \\ 8 & -1 & 5 \\ 4 & -4 & -2 \end{bmatrix}\\). The transpose of the given matrix is</p>
<p><b>Answer:</b> \\(\begin{bmatrix} 3 & 8 & 4 \\ -6 & -1 & -4 \\ 7 & 5 & -2 \end{bmatrix}\\)</p>
<p><b>Explanation:</b> The transpose of a matrix, denoted \\(A^T\\), is obtained by interchanging its rows and columns. The first row of the original matrix becomes the first column of the transposed matrix, the second row becomes the second column, and so on.
<ul>
<li>Row 1: [3, -6, 7] becomes Column 1.</li>
<li>Row 2: [8, -1, 5] becomes Column 2.</li>
<li>Row 3: [4, -4, -2] becomes Column 3.</li>
</ul>
</p>
<br>

<p><b>Question 10:</b> Consider two vectors \\(\bar{u} = [u_1, u_2, \dots, u_n]^T\\) and \\(\bar{v} = [v_1, v_2, \dots, v_n]^T\\). The quantity \\(\bar{u}\bar{v}^T\\) has the size</p>
<p><b>Answer:</b> \\(n \times n\\)</p>
<p><b>Explanation:</b> This operation is known as the **outer product**. The vector \\(\bar{u}\\) is a column vector of size \\(n \times 1\\). The transpose of vector \\(\bar{v}\\), which is \\(\bar{v}^T\\), is a row vector of size \\(1 \times n\\). When you multiply a matrix of size \\(n \times 1\\) by a matrix of size \\(1 \times n\\), the resulting matrix has the size \\(n \times n\\).</p>
</div></div><div class="week" id="week_2"><h1 class="week-title">Week 2</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 6 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 6 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts presented in the transcript, focusing on the determinant and inverse of a matrix. The explanation clarifies the definitions, formulas, and step-by-step calculations discussed.</p>

<b><h3>1. Determinant of a Matrix</h3></b>
<p>The transcript begins by introducing the concept of a determinant, which is a fundamental property of square matrices and a prerequisite for calculating the matrix inverse.</p>
<p><b>Key Points:</b></p>
<ul>
    <li>The determinant is a scalar value that can be computed from the elements of a square matrix.</li>
    <li>It is defined only for square matrices, i.e., matrices with an equal number of rows and columns (\\(m \times n\\) where \\(m=n\\)).</li>
</ul>

<h4>a. Determinant of a 2x2 Matrix</h4>
<p>For a standard 2x2 matrix, the calculation is straightforward.</p>
<p>Given a matrix \\(\mathbf{A}\\):
\\[ \mathbf{A} = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \\]
The determinant of \\(\mathbf{A}\\), denoted as \\(\det(\mathbf{A})\\) or \\(|\mathbf{A}|\\), is calculated with the formula:
\\[ \det(\mathbf{A}) = ad - bc \\]
</p>
<p><b>Example from Transcript:</b></p>
<p>Let the matrix \\(\mathbf{A}\\) be:
\\[ \mathbf{A} = \begin{pmatrix} 2 & -3 \\ 1 & -2 \end{pmatrix} \\]
The determinant is calculated as:
\\[ \det(\mathbf{A}) = (2)(-2) - (1)(-3) = -4 - (-3) = -4 + 3 = -1 \\]
</p>

<h4>b. Determinant of an n x n Matrix (General Case)</h4>
<p>For larger square matrices, the determinant is typically calculated recursively using a method called <b>cofactor expansion</b> (or Laplace expansion). This can be done along any row or any column of the matrix.</p>
<p>Given an \\(m \times m\\) matrix \\(\mathbf{A}\\), the determinant can be found by expanding along the \\(i\\)-th row:
\\[ \det(\mathbf{A}) = \sum_{j=1}^{m} a_{ij} C_{ij} \\]
Or by expanding along the \\(j\\)-th column:
\\[ \det(\mathbf{A}) = \sum_{i=1}^{m} a_{ij} C_{ij} \\]
where:
<ul>
    <li>\\(a_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column.</li>
    <li>\\(C_{ij}\\) is the <b>cofactor</b> of the element \\(a_{ij}\\).</li>
</ul>
</p>
<p><b>Cofactors and Minors:</b></p>
<p>The cofactor \\(C_{ij}\\) is defined as:
\\[ C_{ij} = (-1)^{i+j} M_{ij} \\]
Here, \\(M_{ij}\\) is the <b>minor</b> of the element \\(a_{ij}\\). The minor \\(M_{ij}\\) is the determinant of the sub-matrix that remains after removing the \\(i\\)-th row and \\(j\\)-th column from the original matrix. This recursive definition means that the determinant of a 3x3 matrix is found using determinants of 2x2 matrices, a 4x4 using 3x3s, and so on.</p>

<p><b>Example from Transcript (3x3 Matrix):</b></p>
<p>Consider the matrix \\(\mathbf{A}\\):
\\[ \mathbf{A} = \begin{pmatrix} 7 & 2 & 1 \\ 0 & 3 & -1 \\ -3 & 4 & -2 \end{pmatrix} \\]
To find the determinant, we can expand along the first row (where \\(i=1\\)):
\\[ \det(\mathbf{A}) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} \\]
\\[ \det(\mathbf{A}) = 7 \cdot C_{11} + 2 \cdot C_{12} + 1 \cdot C_{13} \\]
Now, we calculate each cofactor:
<ul>
    <li><b>\\(C_{11}\\)</b>: Remove the 1st row and 1st column.
    \\[ C_{11} = (-1)^{1+1} \det \begin{pmatrix} 3 & -1 \\ 4 & -2 \end{pmatrix} = (1) \cdot ((3)(-2) - (4)(-1)) = -6 + 4 = -2 \\]
    </li>
    <li><b>\\(C_{12}\\)</b>: Remove the 1st row and 2nd column.
    \\[ C_{12} = (-1)^{1+2} \det \begin{pmatrix} 0 & -1 \\ -3 & -2 \end{pmatrix} = (-1) \cdot ((0)(-2) - (-3)(-1)) = (-1) \cdot (0 - 3) = 3 \\]
    </li>
    <li><b>\\(C_{13}\\)</b>: Remove the 1st row and 3rd column.
    \\[ C_{13} = (-1)^{1+3} \det \begin{pmatrix} 0 & 3 \\ -3 & 4 \end{pmatrix} = (1) \cdot ((0)(4) - (-3)(3)) = 0 + 9 = 9 \\]
    </li>
</ul>
Substituting these values back:
\\[ \det(\mathbf{A}) = 7(-2) + 2(3) + 1(9) = -14 + 6 + 9 = 1 \\]

<b><h3>2. Inverse of a Matrix</h3></b>
<p>The transcript then moves on to matrix inversion, which relies on the determinant.</p>
<p><b>Key Points:</b></p>
<ul>
    <li>The inverse of a square matrix \\(\mathbf{A}\\) is denoted by \\(\mathbf{A}^{-1}\\).</li>
    <li>It is defined only for square matrices.</li>
    <li>The inverse, if it exists, satisfies the property:
    \\[ \mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I} \\]
    where \\(\mathbf{I}\\) is the identity matrix of the same dimension.</li>
</ul>

<h4>a. Existence of the Inverse</h4>
<p>A matrix does not always have an inverse. The existence is determined by its determinant.</p>
<ul>
    <li>If \\(\det(\mathbf{A}) \neq 0\\), the matrix \\(\mathbf{A}\\) is called <b>invertible</b> (or non-singular), and its inverse \\(\mathbf{A}^{-1}\\) exists.</li>
    <li>If \\(\det(\mathbf{A}) = 0\\), the matrix \\(\mathbf{A}\\) is called <b>singular</b>, and it does not have an inverse.</li>
</ul>

<h4>b. Formula for the Matrix Inverse</h4>
<p>If a matrix \\(\mathbf{A}\\) is invertible, its inverse is calculated using the following formula:
\\[ \mathbf{A}^{-1} = \frac{1}{\det(\mathbf{A})} \text{adj}(\mathbf{A}) \\]
where \\(\text{adj}(\mathbf{A})\\) is the <b>adjoint</b> (or adjugate) of matrix \\(\mathbf{A}\\).</p>
<p>The <b>adjoint matrix</b> is the transpose of the matrix of cofactors:
\\[ \text{adj}(\mathbf{A}) = \mathbf{C}^T \\]
Here, \\(\mathbf{C}\\) is the matrix formed by replacing each element \\(a_{ij}\\) of \\(\mathbf{A}\\) with its corresponding cofactor \\(C_{ij}\\).</p>

<h4>c. Example from Transcript (3x3 Inverse)</h4>
<p>Using the same matrix \\(\mathbf{A}\\) as before:
\\[ \mathbf{A} = \begin{pmatrix} 7 & 2 & 1 \\ 0 & 3 & -1 \\ -3 & 4 & -2 \end{pmatrix} \\]
<b>Step 1: Calculate the determinant.</b><br>
We already found that \\(\det(\mathbf{A}) = 1\\). Since it is non-zero, the inverse exists.

<p><b>Step 2: Find the matrix of cofactors \\(\mathbf{C}\\).</b><br>
We need to calculate the cofactor for every element. We already have the first row: \\(C_{11}=-2, C_{12}=3, C_{13}=9\\). The transcript provides the full matrix of cofactors:
\\[ \mathbf{C} = \begin{pmatrix} -2 & 3 & 9 \\ 8 & -11 & -34 \\ -5 & 7 & 21 \end{pmatrix} \\]
</p>
<p><b>Step 3: Find the adjoint matrix \\(\text{adj}(\mathbf{A})\\).</b><br>
The adjoint is the transpose of the cofactor matrix \\(\mathbf{C}\\):
\\[ \text{adj}(\mathbf{A}) = \mathbf{C}^T = \begin{pmatrix} -2 & 8 & -5 \\ 3 & -11 & 7 \\ 9 & -34 & 21 \end{pmatrix} \\]
</p>
<p><b>Step 4: Calculate the inverse \\(\mathbf{A}^{-1}\\).</b><br>
Using the formula:
\\[ \mathbf{A}^{-1} = \frac{1}{\det(\mathbf{A})} \text{adj}(\mathbf{A}) = \frac{1}{1} \begin{pmatrix} -2 & 8 & -5 \\ 3 & -11 & 7 \\ 9 & -34 & 21 \end{pmatrix} \\]
\\[ \mathbf{A}^{-1} = \begin{pmatrix} -2 & 8 & -5 \\ 3 & -11 & 7 \\ 9 & -34 & 21 \end{pmatrix} \\]
</p>
<p>To verify the result, one can compute \\(\mathbf{A}\mathbf{A}^{-1}\\) or \\(\mathbf{A}^{-1}\mathbf{A}\\), both of which should result in the 3x3 identity matrix.</p>

<b><h3>3. Inverse vs. Pseudo-Inverse</h3></b>
<p>The transcript briefly distinguishes the matrix inverse from the <b>pseudo-inverse</b>. The key difference is that the inverse is only defined for invertible (square, non-singular) matrices. The pseudo-inverse is a more general concept that can be defined for any matrix, including non-square or singular matrices, but its properties are different. The transcript notes that if a matrix is invertible, its inverse and pseudo-inverse are the same.</p>
</div></div><div class="chapter" id="Lecture 7 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 7 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts from the transcript, focusing on the application of matrix inverses to solve systems of linear equations and its practical use in modern wireless communication systems.</p>

<b><h3>1. Solving a System of Linear Equations</h3></b>
<p>One of the most fundamental applications of matrices and their inverses is in solving systems of linear equations. A system of \\(n\\) linear equations with \\(n\\) unknowns can be represented in a general form.</p>

<b><h4>a. General Form</h4></b>
<p>A system with \\(n\\) equations and \\(n\\) variables (unknowns) \\(x_1, x_2, \dots, x_n\\) is given by:</p>
\\[ a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1 \\]
\\[ a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2 \\]
\\[ \vdots \\]
\\[ a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n \\]
<p>This is called a "linear" system because the equations are linear in the variables \\(x_1, x_2, \dots, x_n\\). This means the variables only appear to the first power (e.g., no \\(x_1^2\\) or \\(x_1x_2\\) terms).</p>

<b><h4>b. Matrix Representation</h4></b>
<p>This system of equations can be written more compactly using matrix notation. We can group the coefficients \\(a_{ij}\\), the variables \\(x_j\\), and the constants \\(b_i\\) into matrices and vectors:</p>
\\[
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
=
\begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{pmatrix}
\\]
<p>This can be represented in the simple form:</p>
\\[ A\mathbf{x} = \mathbf{b} \\]
<p>Where:</p>
<ul>
    <li>\\(A\\) is the \\(n \times n\\) square matrix of coefficients.</li>
    <li>\\(\mathbf{x}\\) is the \\(n \times 1\\) column vector of unknown variables.</li>
    <li>\\(\mathbf{b}\\) is the \\(n \times 1\\) column vector of constants.</li>
</ul>

<b><h4>c. Solution using the Matrix Inverse</h4></b>
<p>If the coefficient matrix \\(A\\) is square and <b>invertible</b> (meaning its inverse, \\(A^{-1}\\), exists), we can find a unique solution for the vector of unknowns \\(\mathbf{x}\\). By pre-multiplying both sides of the equation by \\(A^{-1}\\), we get:</p>
\\[ A^{-1}(A\mathbf{x}) = A^{-1}\mathbf{b} \\]
<p>Since \\(A^{-1}A = I\\) (the identity matrix) and \\(I\mathbf{x} = \mathbf{x}\\), the solution is:</p>
\\[ \mathbf{x} = A^{-1}\mathbf{b} \\]
<p>This formula provides a direct way to calculate the values of \\(x_1, x_2, \dots, x_n\\), provided that \\(A^{-1}\\) can be computed. The transcript notes that this simple solution applies only when \\(A\\) is a square, invertible matrix. The cases where \\(A\\) is not square or not invertible are more complex and will be discussed in later parts of the course.</p>

<b><h4>d. Numerical Example</h4></b>
<p>The transcript provides an example of a 3x3 system of equations:</p>
\\[ 7x_1 + 2x_2 + x_3 = 2 \\]
\\[ 3x_2 - x_3 = -1 \\]
\\[ -3x_1 + 4x_2 - 2x_3 = 3 \\]
<p>In matrix form \\(A\mathbf{x} = \mathbf{b}\\), this is:</p>
\\[
\underbrace{
\begin{pmatrix}
7 & 2 & 1 \\
0 & 3 & -1 \\
-3 & 4 & -2
\end{pmatrix}
}_{A}
\underbrace{
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix}
}_{\mathbf{x}}
=
\underbrace{
\begin{pmatrix}
2 \\
-1 \\
3
\end{pmatrix}
}_{\mathbf{b}}
\\]
<p>The solution is found using \\(\mathbf{x} = A^{-1}\mathbf{b}\\). The transcript states that the inverse matrix \\(A^{-1}\\) was calculated in a previous module and is given as:</p>
\\[
A^{-1} = \begin{pmatrix}
-2 & 8 & -5 \\
3 & -11 & 7 \\
9 & -34 & 21
\end{pmatrix}
\\]
<p><i>Note: There appears to be a typo in the transcript's final answer for the solution vector. The transcript shows `minus 27, thirty eight, one one five`, but the actual calculation with the provided `A_inverse` and `b` yields a different result. Let's re-evaluate based on the provided matrices. The transcript's inverse matrix seems to be incorrect based on the initial matrix A. However, following the calculation shown in the transcript:</i></p>
<p>The calculation shown is:</p>
\\[
\mathbf{x} = A^{-1}\mathbf{b} = 
\begin{pmatrix}
-2 & 8 & -5 \\
3 & -11 & 7 \\
9 & -34 & 21
\end{pmatrix}
\begin{pmatrix}
2 \\
-1 \\
3
\end{pmatrix}
=
\begin{pmatrix}
(-2)(2) + (8)(-1) + (-5)(3) \\
(3)(2) + (-11)(-1) + (7)(3) \\
(9)(2) + (-34)(-1) + (21)(3)
\end{pmatrix}
=
\begin{pmatrix}
-4 - 8 - 15 \\
6 + 11 + 21 \\
18 + 34 + 63
\end{pmatrix}
=
\begin{pmatrix}
-27 \\
38 \\
115
\end{pmatrix}
\\]
<p>Thus, the solution vector is \\(\mathbf{x} = \begin{pmatrix} -27 \\ 38 \\ 115 \end{pmatrix}\\).</p>

<hr>

<b><h3>2. Application in Wireless Communications: MIMO Systems</h3></b>
<p>The transcript introduces a powerful real-world application of linear algebra in modern wireless systems like 4G, 5G, and Wi-Fi, which use a technology called MIMO.</p>

<b><h4>a. MIMO (Multiple-Input Multiple-Output)</h4></b>
<p>MIMO technology uses multiple antennas at both the transmitter and the receiver. This setup creates multiple parallel paths for data to travel from the sender to the receiver, which dramatically increases data transmission rates and reliability.</p>

<b><h4>b. The MIMO Channel Matrix (H)</h4></b>
<p>The wireless path between each transmit antenna and each receive antenna is called a "channel." Each channel is characterized by a complex number called a <b>channel coefficient</b>, denoted by \\(h_{ij}\\), which represents the path between the \\(i\\)-th receive antenna and the \\(j\\)-th transmit antenna.</p>
<p>For a system with \\(r\\) receive antennas and \\(t\\) transmit antennas, all these coefficients can be arranged into an \\(r \times t\\) matrix called the <b>MIMO channel matrix</b>, \\(H\\):</p>
\\[
H = \begin{pmatrix}
h_{11} & h_{12} & \cdots & h_{1t} \\
h_{21} & h_{22} & \cdots & h_{2t} \\
\vdots & \vdots & \ddots & \vdots \\
h_{r1} & h_{r2} & \cdots & h_{rt}
\end{pmatrix}
\\]

<b><h4>c. Role of Matrix Rank and Spatial Multiplexing</h4></b>
<p>The <b>rank</b> of the channel matrix \\(H\\) is critically important. It determines the maximum number of independent data streams (symbols) that can be transmitted simultaneously over the same time and frequency resource. This capability is known as <b>spatial multiplexing</b>.</p>
<ul>
    <li><b>Rank and Data Rate:</b> The maximum number of symbols that can be transmitted is less than or equal to the rank of \\(H\\). Therefore, a larger rank allows for more parallel data streams, which translates to a higher data rate.</li>
    <li><b>Spatial Multiplexing:</b> This is the core benefit of MIMO. By transmitting multiple data streams "through space," the system can multiply its data capacity without needing more bandwidth.</li>
</ul>

<b><h4>d. MIMO System Model and Receiver Design</h4></b>
<p>The relationship between the transmitted signals, the channel, and the received signals can be modeled as a system of linear equations:</p>
\\[ \mathbf{y} = H\mathbf{x} + \mathbf{n} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is the \\(r \times 1\\) vector of received signals.</li>
    <li>\\(H\\) is the \\(r \times t\\) channel matrix.</li>
    <li>\\(\mathbf{x}\\) is the \\(t \times 1\\) vector of transmitted signals (symbols).</li>
    <li>\\(\mathbf{n}\\) is the \\(r \times 1\\) vector of random noise.</li>
</ul>
<p>The receiver's job is to recover the original transmitted signal vector \\(\mathbf{x}\\) from the received signal vector \\(\mathbf{y}\\). The transcript discusses a special, simplified case where the number of transmit and receive antennas are equal (\\(r = t\\)), making \\(H\\) a square matrix.</p>

<b><h4>e. The Zero-Forcing (ZF) Receiver</h4></b>
<p>For the special case where \\(r=t\\) and the channel matrix \\(H\\) is invertible, a simple receiver can be designed. Ignoring the noise for a moment, we have \\(\mathbf{y} \approx H\mathbf{x}\\). We can solve for \\(\mathbf{x}\\) just as we did for the general system of linear equations.</p>
<p>The estimated transmitted vector, denoted \\(\hat{\mathbf{x}}\\), is given by:</p>
\\[ \hat{\mathbf{x}} = H^{-1}\mathbf{y} \\]
<p>This type of receiver is called a <b>Zero-Forcing (ZF) receiver</b> because it uses the inverse of the channel matrix to completely "force out" or cancel the effects of the channel. However, its major limitation is that it only works when \\(H\\) is square and invertible. The more general cases where \\(r \neq t\\) require more advanced linear algebra techniques.</p>
</div></div><div class="chapter" id="Lecture 8 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 8 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains how linear algebra, particularly the use of matrices to solve systems of linear equations, can be applied to analyze two real-world problems: electrical circuits and traffic flows. The core idea is to model these physical systems with a set of linear equations and then use matrix algebra to find a solution.</p>

<b>1. Application in Circuit Analysis</b>

<p>The first example demonstrates how to find the unknown currents in an electrical circuit using a method called <b>Mesh Analysis</b>.</p>

<b>Key Concepts and Theories:</b>
<ul>
    <li><b>Mesh Analysis:</b> A technique used to solve planar circuits for the currents flowing in each "mesh" or closed loop. An independent current is assigned to each mesh (e.g., \\(i_1\\) for mesh 1, \\(i_2\\) for mesh 2). The actual current in a branch shared by two meshes is the algebraic sum (or difference) of the mesh currents. In the given circuit, the current flowing through the central 4Ω resistor is \\(i_2 - i_1\\) to the right.</li>
    <li><b>Kirchhoff's Voltage Law (KVL):</b> A fundamental law of circuit theory which states that the algebraic sum of the voltage drops around any closed loop in a circuit must be zero. This principle is used to create an equation for each mesh.</li>
    <li><b>Ohm's Law:</b> This law describes the relationship between voltage (V), current (I), and resistance (R) in a linear resistor. The voltage drop across a resistor is given by the formula:
    \\[ V = IR \\]
    <p>This formula signifies the voltage drop that occurs when moving across the resistor <i>in the same direction</i> as the current flow.</p>
    </li>
</ul>

<b>Derivation of the System of Equations:</b>
<p>By applying KVL and Ohm's Law to each of the two meshes in the circuit, we can derive a system of two linear equations with two unknowns (\\(i_1\\) and \\(i_2\\)). The analysis proceeds by "traveling" clockwise around each loop and summing the voltage drops.</p>

<p><b>For Loop 1:</b></p>
<ul>
    <li>Starting from the 8V battery, moving clockwise, we first encounter a voltage drop of 8V.</li>
    <li>Next is the 1Ω resistor with current \\(i_1\\), causing a voltage drop of \\(1 \cdot i_1\\).</li>
    <li>Finally, we cross the central 4Ω resistor. The defined current is \\(i_2 - i_1\\) to the right, but we are traveling to the left (opposite direction), so the voltage drop is \\(-4(i_2 - i_1)\\).</li>
</ul>
<p>Summing these according to KVL (total drop is zero):</p>
\\[ 8 + 1 \cdot i_1 - 4(i_2 - i_1) = 0 \\]
<p>Simplifying this equation gives:</p>
\\[ 5i_1 - 4i_2 = -8 \\]

<p><b>For Loop 2:</b></p>
<ul>
    <li>Starting at the central 4Ω resistor and moving clockwise, we are in the direction of the current \\(i_2 - i_1\\), so the drop is \\(4(i_2 - i_1)\\).</li>
    <li>The 2Ω resistor has current \\(i_2\\), causing a drop of \\(2i_2\\).</li>
    <li>The 4V battery causes a voltage drop of 4V.</li>
    <li>The final 4Ω resistor has current \\(i_2\\), causing a drop of \\(4i_2\\).</li>
</ul>
<p>Summing these according to KVL:</p>
\\[ 4(i_2 - i_1) + 2i_2 + 4 + 4i_2 = 0 \\]
<p>Simplifying this equation gives:</p>
\\[ -4i_1 + 10i_2 = -4 \\]

<b>Solving with Matrix Algebra:</b>
<p>The two simplified linear equations form a system that can be written in matrix form \\(A\mathbf{i} = \mathbf{b}\\):</p>
\\[ \begin{pmatrix} 5 & -4 \\ -4 & 10 \end{pmatrix} \begin{pmatrix} i_1 \\ i_2 \end{pmatrix} = \begin{pmatrix} -8 \\ -4 \end{pmatrix} \\]
<p>The solution for the current vector \\(\mathbf{i}\\) is found by multiplying the inverse of matrix A with the vector \\(\mathbf{b}\\):</p>
\\[ \mathbf{i} = A^{-1}\mathbf{b} \\]
<p>The inverse of a 2x2 matrix \\(\begin{pmatrix} a & b \\ c & d \end{pmatrix}\\) is \\(\frac{1}{ad-bc}\begin{pmatrix} d & -b \\ -c & a \end{pmatrix}\\). For our matrix A:</p>
<ul>
    <li>The determinant is \\(\det(A) = (5)(10) - (-4)(-4) = 50 - 16 = 34\\).</li>
    <li>The inverse is \\(A^{-1} = \frac{1}{34} \begin{pmatrix} 10 & 4 \\ 4 & 5 \end{pmatrix}\\).</li>
</ul>
<p>Now, we can solve for the currents:</p>
\\[ \begin{pmatrix} i_1 \\ i_2 \end{pmatrix} = \frac{1}{34} \begin{pmatrix} 10 & 4 \\ 4 & 5 \end{pmatrix} \begin{pmatrix} -8 \\ -4 \end{pmatrix} = \frac{1}{34} \begin{pmatrix} (10)(-8) + (4)(-4) \\ (4)(-8) + (5)(-4) \end{pmatrix} = \frac{1}{34} \begin{pmatrix} -96 \\ -52 \end{pmatrix} \\]
<p>The final mesh currents are:</p>
\\[ i_1 = -\frac{96}{34} = -\frac{48}{17} \text{ Amperes} \\]
\\[ i_2 = -\frac{52}{34} = -\frac{26}{17} \text{ Amperes} \\]
<p>The negative signs indicate that the actual direction of the mesh currents is counter-clockwise, opposite to the direction initially assumed.</p>

<br>

<b>2. Application in Traffic Flow Analysis</b>

<p>The second example shows how linear algebra can model and solve for unknown traffic flows at intersections in a city grid.</p>

<b>Key Concept:</b>
<ul>
    <li><b>Conservation of Flow:</b> The fundamental principle is that for any given intersection, the rate of traffic entering must equal the rate of traffic leaving. This ensures that there is no accumulation of vehicles at the intersection. This principle is analogous to Kirchhoff's Current Law (KCL) in circuits.</li>
</ul>

<b>Derivation of the System of Equations:</b>
<p>By applying the principle of conservation of flow to each of the four intersections (A, B, C, D), a system of linear equations with the unknown flows (\\(x_1, x_2, x_3, x_4\\)) is established.</p>
<ul>
    <li><b>Intersection A:</b> Incoming = \\(75 + x_4\\), Outgoing = \\(50 + x_1\\).
    <br>Equation: \\(75 + x_4 = 50 + x_1 \implies x_1 - x_4 = 25\\)</li>
    <li><b>Intersection B:</b> Incoming = \\(x_1 + 45\\), Outgoing = \\(x_2 + 80\\).
    <br>Equation: \\(x_1 + 45 = x_2 + 80 \implies x_1 - x_2 = 35\\)</li>
    <li><b>Intersection C:</b> Incoming = \\(x_2 + x_3\\), Outgoing = \\(35\\).
    <br>Equation: \\(x_2 + x_3 = 35\\)</li>
    <li><b>Intersection D:</b> Incoming = \\(35\\), Outgoing = \\(x_4 + 55\\).
    <br>Equation: \\(35 = x_4 + 55 \implies x_4 = -20\\)</li>
</ul>

<b>Solving with Matrix Algebra:</b>
<p>This system of four linear equations can be expressed in the matrix form \\(A\mathbf{x} = \mathbf{b}\\):</p>
\\[ \begin{pmatrix} 1 & 0 & 0 & -1 \\ 1 & -1 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} = \begin{pmatrix} 25 \\ 35 \\ 35 \\ -20 \end{pmatrix} \\]
<p>The solution vector \\(\mathbf{x}\\) is found by solving this system, for instance, by calculating \\(\mathbf{x} = A^{-1}\mathbf{b}\\). The transcript provides the final solution:</p>
\\[ \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} = \begin{pmatrix} 5 \\ -30 \\ 65 \\ -20 \end{pmatrix} \\]
<p>The individual traffic flows are:</p>
<ul>
    <li>\\(x_1 = 5\\)</li>
    <li>\\(x_2 = -30\\)</li>
    <li>\\(x_3 = 65\\)</li>
    <li>\\(x_4 = -20\\)</li>
</ul>
<p>A negative value for a flow (like \\(x_2\\) and \\(x_4\\)) indicates that the traffic is actually moving in the opposite direction to the arrow shown in the initial diagram.</p>
</div></div><div class="chapter" id="Lecture 9 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 9 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the application of linear algebra in graph theory, particularly for analyzing social networks.</p>

<h3>1. Graphs, Social Networks, and the Adjacency Matrix</h3>
<p>The core idea is to represent a network, such as a social network, as a mathematical object called a <b>graph</b>. This allows for powerful analysis using tools from linear algebra.</p>

<p><b>Key Concepts:</b></p>
<ul>
    <li><b>Graph:</b> A structure consisting of <b>nodes</b> (or vertices) and <b>edges</b> that connect pairs of nodes. In the context of a social network, nodes can represent people, and edges can represent relationships between them.</li>
    <li><b>Directed Graph:</b> A graph where each edge has a direction, indicated by an arrow. This is useful for representing asymmetric relationships. For example, in the transcript's social network analogy, an edge from person \\(P_i\\) to person \\(P_j\\) could mean "\\(P_i\\) influences \\(P_j\\)" or "\\(P_j\\) follows \\(P_i\\)" (like on Twitter).</li>
</ul>

<h4>The Adjacency Matrix (M)</h4>
<p>To analyze a graph using linear algebra, we first convert it into a matrix. The most common way to do this is by creating an <b>adjacency matrix</b>, denoted by \\(M\\).</p>
<ul>
    <li>For a graph with \\(n\\) nodes, the adjacency matrix \\(M\\) is an \\(n \times n\\) square matrix.</li>
    <li>The entry in the \\(i\\)-th row and \\(j\\)-th column, denoted \\(M_{ij}\\), is defined as:
    \\[ M_{ij} = \begin{cases} 1 & \text{if there is a directed edge from node } P_i \text{ to node } P_j \\ 0 & \text{otherwise} \end{cases} \\]
    </li>
</ul>

<p><b>Example 1:</b> For the first graph presented in the transcript, with 5 nodes (\\(P_1, P_2, P_3, P_4, P_5\\)), the adjacency matrix \\(M\\) is constructed as follows:</p>
<ul>
    <li><b>Row 1 (from \\(P_1\\)):</b> There are edges from \\(P_1\\) to \\(P_3\\) and \\(P_5\\). So, \\(M_{13} = 1\\) and \\(M_{15} = 1\\). All other entries in this row are 0.</li>
    <li><b>Row 2 (from \\(P_2\\)):</b> There is an edge from \\(P_2\\) to \\(P_4\\). So, \\(M_{24} = 1\\).</li>
    <li><b>Row 3 (from \\(P_3\\)):</b> There are edges from \\(P_3\\) to \\(P_2\\) and \\(P_4\\). So, \\(M_{32} = 1\\) and \\(M_{34} = 1\\).</li>
    <li><b>Row 4 (from \\(P_4\\)):</b> There is an edge from \\(P_4\\) to \\(P_5\\). So, \\(M_{45} = 1\\).</li>
    <li><b>Row 5 (from \\(P_5\\)):</b> There is an edge from \\(P_5\\) to \\(P_2\\). So, \\(M_{52} = 1\\).</li>
</ul>
<p>This results in the following 5x5 adjacency matrix:</p>
\\[ M = \begin{pmatrix} 0 & 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 0 \end{pmatrix} \\]

<h3>2. Paths and Powers of the Adjacency Matrix</h3>
<p>A powerful property of the adjacency matrix is revealed when we compute its powers (\\(M^2, M^3, \dots, M^r\\)).</p>

<h4>Key Theory: Paths of Length r</h4>
<p>The entry in the \\(i\\)-th row and \\(j\\)-th column of the matrix \\(M^r\\) (the matrix \\(M\\) multiplied by itself \\(r\\) times) gives the <b>number of distinct paths of length \\(r\\)</b> from node \\(P_i\\) to node \\(P_j\\).</p>
<ul>
    <li>A <b>path of length 1</b> is a direct connection (a single edge). These are counted by \\(M\\).</li>
    <li>A <b>path of length 2</b> is a two-step connection (e.g., from \\(P_i\\) to \\(P_k\\) and then from \\(P_k\\) to \\(P_j\\)). These are counted by \\(M^2\\).</li>
</ul>

<p><b>Example 2: Two-Step Connections (\\(M^2\\))</b></p>
<p>The transcript provides the matrix \\(M^2\\) for the first graph:</p>
\\[ M^2 = \begin{pmatrix} 0 & 2 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \end{pmatrix} \\]
<p>The entry \\( (M^2)_{12} = 2 \\). This means there are <b>two paths of length 2</b> from node \\(P_1\\) to node \\(P_2\\). By inspecting the graph, we can verify these paths:</p>
<ol>
    <li>\\(P_1 \to P_3 \to P_2\\)</li>
    <li>\\(P_1 \to P_5 \to P_2\\)</li>
</ol>

<p><b>Example 3: Three-Step Connections (\\(M^3\\))</b></p>
<p>Similarly, the matrix \\(M^3\\) is given as:</p>
\\[ M^3 = \begin{pmatrix} 0 & 1 & 0 & 2 & 1 \\ 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{pmatrix} \\]
<p>The entry \\( (M^3)_{14} = 2 \\) indicates there are <b>two paths of length 3</b> from node \\(P_1\\) to node \\(P_4\\). These paths are:</p>
<ol>
    <li>\\(P_1 \to P_3 \to P_2 \to P_4\\)</li>
    <li>\\(P_1 \to P_5 \to P_2 \to P_4\\)</li>
</ol>

<h3>3. Dominance Directed Graphs and Finding the Most Influential Node</h3>
<p>The analysis is extended to a special type of graph with applications in ranking and influence analysis.</p>

<h4>Dominance Directed Graph (or Tournament)</h4>
<p>This is a directed graph with a specific property: for any pair of distinct nodes \\(P_i\\) and \\(P_j\\), there is an edge in one and only one direction. That is, either there is an edge from \\(P_i \to P_j\\) or an edge from \\(P_j \to P_i\\), but <b>not both</b>. This is analogous to a round-robin tournament where every team plays every other team, and one must win (no draws are allowed).</p>
<p>In a social network context, this represents a scenario where for any two people, one person definitively influences the other.</p>

<h4>Finding the Most Influential Node</h4>
<p>The main question is: in such a network, who is the most influential person? The method described uses the adjacency matrix to quantify influence.</p>
<p>The influence of a node is determined not just by its direct connections (1-step paths) but also by its indirect connections (2-step paths). A person is influential if they influence many people directly, and also if the people they influence are themselves influential.</p>

<p><b>The Formula and Procedure:</b></p>
<ol>
    <li>For the dominance directed graph, construct its adjacency matrix \\(M\\).</li>
    <li>Calculate the matrix \\(S = M + M^2\\). This matrix combines the counts of 1-step paths (direct influence) and 2-step paths (indirect influence).</li>
    <li>For each row \\(i\\) in the matrix \\(S\\), calculate the <b>row sum</b> (the sum of all entries in that row). The sum of row \\(i\\) represents the total 1-step and 2-step influence exerted by person \\(P_i\\).</li>
    <li>The node \\(P_i\\) corresponding to the row with the <b>largest sum</b> is considered the most influential node in the network.</li>
</ol>

<p><b>Example 4: Calculating Influence</b></p>
<p>For the second graph (the dominance directed graph), the adjacency matrix \\(M\\) is:</p>
\\[ M = \begin{pmatrix} 0 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 1 & 1 & 0 & 0 \end{pmatrix} \\]
<p>The transcript provides the resulting matrix for \\(M + M^2\\):</p>
\\[ S = M + M^2 = \begin{pmatrix} 0 & 3 & 2 & 3 & 2 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 0 & 1 \\ 0 & 2 & 1 & 2 & 0 \end{pmatrix} \\]
<p>Next, we calculate the sum of each row:</p>
<ul>
    <li><b>Row 1 Sum:</b> \\(0+3+2+3+2 = 10\\)</li>
    <li><b>Row 2 Sum:</b> \\(0+0+0+1+1 = 2\\)</li>
    <li><b>Row 3 Sum:</b> \\(0+1+0+2+1 = 4\\)</li>
    <li><b>Row 4 Sum:</b> \\(0+1+1+0+1 = 3\\)</li>
    <li><b>Row 5 Sum:</b> \\(0+2+1+2+0 = 5\\)</li>
</ul>
<p>Since Row 1 has the maximum sum (10), we conclude that node <b>\\(P_1\\) is the most influential person</b> in this social network.</p>

</div></div><div class="chapter" id="Lecture 10 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 10 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts, theories, and formulas related to the <b>null space of a matrix</b> as presented in the transcript.</p>

<h3>1. Definition of the Null Space</h3>
<p>The null space of a matrix is a fundamental concept in linear algebra. For any given matrix \\(A\\) of size \\(m \times n\\), its null space is defined as the set of all vectors that, when multiplied by \\(A\\), result in the zero vector.</p>
<p>The null space of matrix \\(A\\) is denoted by \\(N(A)\\) and is formally defined as:</p>
\\[ N(A) = \{ \mathbf{x} \mid A\mathbf{x} = \mathbf{0} \} \\]
<p>Here, \\(\mathbf{x}\\) is a column vector of size \\(n \times 1\\), and \\(\mathbf{0}\\) is the \\(m \times 1\\) zero vector. In essence, the null space is the complete solution set to the homogeneous system of linear equations \\(A\mathbf{x} = \mathbf{0}\\).</p>

<h3>2. The Null Space as a Subspace</h3>
<p>The term "space" in "null space" is mathematically significant because the null space of any matrix is a <b>subspace</b> of \\(\mathbb{R}^n\\). A set is a subspace if it satisfies two key properties:</p>
<ol>
    <li>It contains the zero vector.</li>
    <li>It is closed under linear combinations (i.e., vector addition and scalar multiplication).</li>
</ol>
<p>The transcript demonstrates this property. Let's take any two vectors \\(\mathbf{x}_1\\) and \\(\mathbf{x}_2\\) from the null space \\(N(A)\\). By definition, this means:</p>
\\[ A\mathbf{x}_1 = \mathbf{0} \quad \text{and} \quad A\mathbf{x}_2 = \mathbf{0} \\]
<p>Now, consider an arbitrary linear combination of these two vectors, \\(\alpha\mathbf{x}_1 + \beta\mathbf{x}_2\\), where \\(\alpha\\) and \\(\beta\\) are scalars. To check if this new vector is also in the null space, we multiply it by \\(A\\):</p>
\\[ A(\alpha\mathbf{x}_1 + \beta\mathbf{x}_2) = A(\alpha\mathbf{x}_1) + A(\beta\mathbf{x}_2) \\]
<p>Using the properties of matrix multiplication, we can pull the scalars out:</p>
\\[ \alpha(A\mathbf{x}_1) + \beta(A\mathbf{x}_2) = \alpha(\mathbf{0}) + \beta(\mathbf{0}) = \mathbf{0} \\]
<p>Since \\(A(\alpha\mathbf{x}_1 + \beta\mathbf{x}_2) = \mathbf{0}\\), the linear combination \\(\alpha\mathbf{x}_1 + \beta\mathbf{x}_2\\) also belongs to \\(N(A)\\). This proves that the null space is indeed a subspace.</p>

<h3>3. Trivial and Non-Trivial Null Space</h3>
<p>For any matrix \\(A\\), the zero vector \\(\mathbf{0}\\) is always a solution to \\(A\mathbf{x} = \mathbf{0}\\) because \\(A\mathbf{0} = \mathbf{0}\\). This means the zero vector is always in the null space.</p>
<ul>
    <li><b>Trivial Null Space:</b> If the only vector in the null space is the zero vector (\\(N(A) = \{\mathbf{0}\}\\)), it is called the trivial null space. This is the "obvious" solution.</li>
    <li><b>Non-Trivial Null Space:</b> If there exists at least one non-zero vector \\(\mathbf{x}\\) (where \\(\mathbf{x} \neq \mathbf{0}\\)) such that \\(A\mathbf{x} = \mathbf{0}\\), the null space is called non-trivial.</li>
</ul>
<p>An important consequence of the subspace property is that if a null space is non-trivial, it must contain an infinite number of vectors. If \\(\mathbf{x} \neq \mathbf{0}\\) is in \\(N(A)\\), then any scalar multiple \\(\alpha\mathbf{x}\\) must also be in \\(N(A)\\).</p>

<h3>4. Null Space and Matrix Invertibility</h3>
<p>For a square matrix \\(A\\) (size \\(n \times n\\)), there is a direct connection between its null space and its invertibility.</p>
<p><b>Theorem:</b> A square matrix \\(A\\) is invertible if and only if its null space is trivial (i.e., \\(N(A) = \{\mathbf{0}\}\\)). Conversely, if a square matrix \\(A\\) has a non-trivial null space, it is <b>not invertible</b> (it is singular).</p>
<p>The transcript provides a proof by contradiction for this statement:</p>
<ol>
    <li>Assume \\(A\\) has a non-trivial null space. This means there is a vector \\(\mathbf{x} \neq \mathbf{0}\\) such that \\(A\mathbf{x} = \mathbf{0}\\).</li>
    <li>Now, assume \\(A\\) is invertible, meaning its inverse \\(A^{-1}\\) exists.</li>
    <li>Multiply both sides of the equation \\(A\mathbf{x} = \mathbf{0}\\) by \\(A^{-1}\\) from the left:
    \\[ A^{-1}(A\mathbf{x}) = A^{-1}\mathbf{0} \\]</li>
    <li>This simplifies to:
    \\[ (A^{-1}A)\mathbf{x} = \mathbf{0} \implies I\mathbf{x} = \mathbf{0} \implies \mathbf{x} = \mathbf{0} \\]
    where \\(I\\) is the identity matrix.</li>
    <li>This result, \\(\mathbf{x} = \mathbf{0}\\), contradicts our initial assumption that \\(\mathbf{x} \neq \mathbf{0}\\). Therefore, our assumption that \\(A^{-1}\\) exists must be false.</li>
</ol>
<p>This connection is part of a larger set of equivalent conditions for a square matrix to be singular, which also includes having a determinant of zero (\\(\det(A)=0\\)) and being rank-deficient (\\(\text{rank}(A) < n\\)).</p>

<h3>5. Finding a Basis for the Null Space: An Example</h3>
<p>To find the vectors that constitute the null space, we solve the system \\(A\mathbf{x} = \mathbf{0}\\). The standard method involves reducing the matrix \\(A\\) to its row-reduced echelon form (RREF).</p>
<p>Consider the example matrix:</p>
\\[ A = \begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{pmatrix} \\]
<p><b>Step 1: Row Reduction</b><br>
Perform row operations to find the RREF of A.
<br>1. \\(R_2 \leftarrow R_2 - R_1\\): \\( \begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\ 0 & 1 & 2 & 3 & 4 \end{pmatrix} \\)
<br>2. \\(R_1 \leftarrow R_1 - R_2\\): \\( \begin{pmatrix} 1 & 0 & -1 & -2 & -3 \\ 0 & 1 & 2 & 3 & 4 \end{pmatrix} \\)
This is the RREF.</p>
<p><b>Step 2: Express Pivot Variables in terms of Free Variables</b><br>
The system of equations corresponding to the RREF is:
\\[ x_1 - x_3 - 2x_4 - 3x_5 = 0 \\]
\\[ x_2 + 2x_3 + 3x_4 + 4x_5 = 0 \\]
The variables corresponding to the pivot columns (columns 1 and 2) are the pivot variables (\\(x_1, x_2\\)). The rest are free variables (\\(x_3, x_4, x_5\\)). We solve for the pivot variables:
\\[ x_1 = x_3 + 2x_4 + 3x_5 \\]
\\[ x_2 = -2x_3 - 3x_4 - 4x_5 \\]
<p><b>Step 3: Write the General Solution and Find the Basis</b><br>
The general solution vector \\(\mathbf{x}\\) can be written as:
\\[ \mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} = \begin{pmatrix} x_3 + 2x_4 + 3x_5 \\ -2x_3 - 3x_4 - 4x_5 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} \\]
We can separate this vector into a linear combination based on the free variables:
\\[ \mathbf{x} = x_3 \begin{pmatrix} 1 \\ -2 \\ 1 \\ 0 \\ 0 \end{pmatrix} + x_4 \begin{pmatrix} 2 \\ -3 \\ 0 \\ 1 \\ 0 \end{pmatrix} + x_5 \begin{pmatrix} 3 \\ -4 \\ 0 \\ 0 \\ 1 \end{pmatrix} \\]
The three vectors in this linear combination form a <b>basis</b> for the null space of \\(A\\). Any vector in \\(N(A)\\) can be written as a unique combination of these three basis vectors.</p>

<h3>6. The Rank-Nullity Theorem</h3>
<p>This example illustrates a fundamental theorem in linear algebra.</p>
<ul>
    <li>The <b>rank</b> of a matrix is the dimension of its column space (and row space), which equals the number of pivots in its RREF. For matrix \\(A\\) above, \\(\text{rank}(A) = 2\\).</li>
    <li>The <b>nullity</b> of a matrix is the dimension of its null space, which equals the number of vectors in its basis (or equivalently, the number of free variables). For matrix \\(A\\), \\(\text{nullity}(A) = 3\\).</li>
</ul>
<p>The <b>Rank-Nullity Theorem</b> states that for any \\(m \times n\\) matrix \\(A\\):</p>
\\[ \text{rank}(A) + \text{nullity}(A) = n \quad (\text{the number of columns}) \\]
<p>For our example, \\(2 + 3 = 5\\), which is the number of columns of \\(A\\), confirming the theorem.</p>

<h3>7. Application: Electrical Circuits and Kirchhoff's Current Law</h3>
<p>The null space has practical applications, such as in the analysis of electrical circuits. The transcript describes how the null space of a specific matrix formulation can represent Kirchhoff's Current Law (KCL).</p>
<p><b>Step 1: Construct the Adjacency Matrix (Incidence Matrix)</b><br>
For a circuit with \\(n\\) nodes and \\(m\\) edges (currents), we can define an \\(m \times n\\) matrix \\(A\\), often called an incidence matrix. The entry \\(A_{ij}\\) is defined as:
<ul>
    <li>\\(A_{ij} = -1\\) if current \\(i\\) leaves node \\(j\\).</li>
    <li>\\(A_{ij} = +1\\) if current \\(i\\) enters node \\(j\\).</li>
    <li>\\(A_{ij} = 0\\) otherwise.</li>
</ul>
For the circuit in the transcript, the \\(5 \times 4\\) matrix \\(A\\) is:
\\[ A = \begin{pmatrix} -1 & 1 & 0 & 0 \\ 0 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 \\ 1 & 0 & -1 & 0 \\ 0 & 1 & -1 & 0 \end{pmatrix} \\]
</p>
<p><b>Step 2: The Null Space of the Transpose</b><br>
A key insight is that any valid vector of currents \\(\mathbf{i} = (i_1, i_2, i_3, i_4, i_5)^T\\) in the circuit must belong to the null space of the transpose of this matrix, \\(A^T\\). That is:</p>
\\[ \mathbf{i} \in N(A^T) \quad \text{which means} \quad A^T\mathbf{i} = \mathbf{0} \\]
The transpose \\(A^T\\) is a \\(4 \times 5\\) matrix where each row now corresponds to a node and each column to a current.
\\[ A^T = \begin{pmatrix} -1 & 0 & 0 & 1 & 0 \\ 1 & -1 & 0 & 0 & 1 \\ 0 & 0 & 1 & -1 & -1 \\ 0 & 1 & -1 & 0 & 0 \end{pmatrix} \\]
<p><b>Step 3: Connecting to Kirchhoff's Current Law (KCL)</b><br>
The equation \\(A^T\mathbf{i} = \mathbf{0}\\) represents a system of four linear equations. Each equation corresponds to KCL for one of the nodes, which states that the sum of currents entering a node must equal the sum of currents leaving it.</p>
<ul>
    <li><b>Row 1 (Node 1):</b> \\(-i_1 + i_4 = 0 \implies i_4 = i_1\\) (Current entering = Current leaving)</li>
    <li><b>Row 2 (Node 2):</b> \\(i_1 - i_2 + i_5 = 0 \implies i_1 + i_5 = i_2\\) (Sum of currents entering = Current leaving)</li>
    <li><b>Row 3 (Node 3):</b> \\(i_3 - i_4 - i_5 = 0 \implies i_3 = i_4 + i_5\\) (Current entering = Sum of currents leaving)</li>
    <li><b>Row 4 (Node 4):</b> \\(i_2 - i_3 = 0 \implies i_2 = i_3\\) (Current entering = Current leaving)</li>
</ul>
<p>Thus, the condition that a current vector \\(\mathbf{i}\\) lies in the null space of \\(A^T\\) is a concise mathematical statement of Kirchhoff's Current Law for the entire circuit.</p>
</div></div><h2>Weekly Summary</h2><div>
<p>This week's lectures cover the concept of matrix inversion, its applications in solving various real-world problems, and the fundamental concept of a matrix's null space.</p>
<b>1. Matrix Inversion and Determinants</b>
<p>The lectures begin by introducing the necessary prerequisites for matrix inversion, starting with the determinant.</p>
<ul>
<li><b>Determinant:</b> The determinant is a scalar value defined for square matrices. For a 2x2 matrix, it is calculated as \\(ad-bc\\). For a general \\(n \times n\\) matrix, the determinant is calculated recursively by expanding along a row or column using cofactors and minors. A cofactor \\(C_{ij}\\) is defined as \\(C_{ij} = (-1)^{i+j} M_{ij}\\), where \\(M_{ij}\\) is the minor (the determinant of the submatrix formed by removing row \\(i\\) and column \\(j\\)).</li>
<li><b>Matrix Inverse:</b> The inverse of a square matrix \\(A\\), denoted \\(A^{-1}\\), is a matrix that satisfies the property \\(A A^{-1} = A^{-1} A = I\\), where \\(I\\) is the identity matrix.</li>
<li><b>Key Takeaway:</b> A square matrix is invertible if and only if its determinant is non-zero. If the determinant is zero, the matrix is called <b>singular</b>. The formula for the inverse is given by:
\\[ A^{-1} = \frac{1}{\det(A)} \text{adj}(A) \\]
where \\(\text{adj}(A)\\) is the adjoint of A, defined as the transpose of the matrix of cofactors.</li>
</ul>
<b>2. Applications of Matrices and Inversion</b>
<p>A significant portion of the lectures is dedicated to demonstrating the practical application of matrix concepts in various fields.</p>
<ul>
<li><b>Solving Systems of Linear Equations:</b> A system of \\(n\\) linear equations with \\(n\\) unknowns can be expressed in matrix form as \\(A\bar{x} = \bar{b}\\). If \\(A\\) is invertible, the system has a unique solution given by \\(\bar{x} = A^{-1}\bar{b}\\).</li>
<li><b>Wireless Communications (MIMO):</b> In modern 4G/5G wireless systems, Multiple-Input Multiple-Output (MIMO) technology is used. The wireless channel between multiple transmit and receive antennas is modeled by a channel matrix \\(H\\). The <b>rank</b> of this matrix determines the maximum number of data streams that can be sent simultaneously (a property called spatial multiplexing), thus dictating the maximum data rate. In a simplified case where \\(H\\) is square and invertible, a receiver can decode the transmitted signal \\(\bar{x}\\) from the received signal \\(\bar{y}\\) using the formula \\(\hat{x} = H^{-1}\bar{y}\\). This is known as a zero-forcing receiver.</li>
<li><b>Circuit Analysis:</b> Using mesh analysis and Kirchhoff's Voltage Law (KVL), the currents in an electrical circuit can be determined by setting up and solving a system of linear equations in matrix form.</li>
<li><b>Traffic Flow Analysis:</b> The flow of traffic through a network of roads and intersections can be modeled using the principle that the total traffic entering an intersection must equal the total traffic leaving it. This creates a system of linear equations that can be solved to find the unknown traffic flows.</li>
</ul>
<b>3. Graph Theory and Social Networks</b>
<p>Linear algebra provides powerful tools for analyzing graphs, which are used to model networks like social networks.</p>
<ul>
<li><b>Adjacency Matrix:</b> A directed graph can be represented by an adjacency matrix \\(M\\), where \\(M_{ij} = 1\\) indicates an edge from node \\(i\\) to node \\(j\\).</li>
<li><b>Key Takeaway:</b> The matrix power \\(M^r\\) contains information about connectivity in the graph. Specifically, the element \\((M^r)_{ij}\\) gives the number of distinct paths of length \\(r\\) from node \\(i\\) to node \\(j\\). This can be used to analyze relationships in a social network. For a special "dominance-directed graph," the most influential node can be found by calculating \\(M + M^2\\) and identifying the row with the largest sum.</li>
</ul>
<b>4. The Null Space</b>
<p>The concept of the null space of a matrix is introduced as another fundamental property.</p>
<ul>
<li><b>Definition:</b> The null space of a matrix \\(A\\), denoted \\(N(A)\\), is the set of all vectors \\(\bar{x}\\) such that \\(A\bar{x} = \bar{0}\\). The null space is always a subspace.</li>
<li><b>Properties & Rank-Nullity Theorem:</b> If a square matrix has a "non-trivial" null space (i.e., it contains non-zero vectors), the matrix is singular (not invertible). The dimension of the null space is called the <b>nullity</b>. The Rank-Nullity theorem states that for any matrix \\(A\\) with \\(n\\) columns, \\(\text{rank}(A) + \text{nullity}(A) = n\\).</li>
<li><b>Key Takeaway:</b> The null space has direct physical interpretations. For a circuit's adjacency matrix \\(A\\) (which describes how currents are connected to nodes), any valid current vector \\(\bar{i}\\) must be in the null space of \\(A^T\\). This mathematical condition is equivalent to satisfying Kirchhoff's Current Law (KCL) at every node in the circuit.</li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<p><b>Question 1: Rank of a matrix</b></p>
<p><b>Question:</b> Rank of a matrix equals the</p>
<p><b>Answer:</b> maximum number of linearly independent columns</p>
<p><b>Explanation:</b> The rank of a matrix is a fundamental concept that measures the "non-degeneracy" of the linear transformation described by the matrix. It has several equivalent definitions:</p>
<ul>
<li>The maximum number of linearly independent column vectors.</li>
<li>The maximum number of linearly independent row vectors.</li>
<li>The dimension of the column space of the matrix.</li>
<li>The number of non-zero rows (or pivots) in its row echelon form.</li>
</ul>
<p>The correct answer states one of these fundamental definitions. The other options are incorrect; for instance, the number of linearly dependent columns does not define the rank.</p>
<br>

<p><b>Question 2: Conjugate Transpose of a Matrix</b></p>
<p><b>Question:</b> Consider the matrix<br>
\\[ A = 
\begin{bmatrix}
1 + 2j & 5 - 2j & -6 + 2j \\
-1 - 3j & -1 - 5j & -3 - j \\
2 + 3j & -2 - 4j & 2 + j
\end{bmatrix}
\\]
The conjugate transpose or Hermitian transpose of the given matrix is</p>
<p><b>Answer:</b> 
\\[
\begin{bmatrix}
1 - 2j & -1 + 3j & 2 - 3j \\
5 + 2j & -1 + 5j & -2 + 4j \\
-6 - 2j & -3 + j & 2 - j
\end{bmatrix}
\\]
</p>
<p><b>Explanation:</b> The conjugate transpose (or Hermitian transpose) of a matrix, denoted as \\(A^H\\) or \\(A^*\\), is found in two steps:</p>
<p>1. <b>Transpose the matrix (\\(A^T\\)):</b> Swap the rows and columns.</p>
\\[ A^T = 
\begin{bmatrix}
1 + 2j & -1 - 3j & 2 + 3j \\
5 - 2j & -1 - 5j & -2 - 4j \\
-6 + 2j & -3 - j & 2 + j
\end{bmatrix}
\\]
<p>2. <b>Take the complex conjugate of each element:</b> For each element \\(a + bj\\), its conjugate is \\(a - bj\\). We apply this to every element of \\(A^T\\).</p>
\\[ A^H = (A^T)^* = 
\begin{bmatrix}
1 - 2j & -1 + 3j & 2 - 3j \\
5 + 2j & -1 + 5j & -2 + 4j \\
-6 - 2j & -3 + j & 2 - j
\end{bmatrix}
\\]
This result matches the selected answer.</p>
<br>

<p><b>Question 3: Norm of a Complex Vector</b></p>
<p><b>Question:</b> Consider the complex sinusoidal vector \\(\bar{u}(f) = [1, e^{j2\pi f}, e^{j4\pi f}, \dots, e^{j2(N-1)\pi f}]^T\\). Then, \\( ||\bar{u}(f)||^2 \\) equals</p>
<p><b>Answer:</b> \\(N\\)</p>
<p><b>Explanation:</b> The squared norm of a complex vector \\(\bar{u}\\) is calculated as the inner product of the vector with itself, which is \\(||\bar{u}||^2 = \bar{u}^H \bar{u}\\), where \\(\bar{u}^H\\) is the conjugate transpose of \\(\bar{u}\\).</p>
<p>1. First, find the conjugate transpose \\(\bar{u}^H\\):</p>
\\[ \bar{u}^H = [1, (e^{j2\pi f})^*, (e^{j4\pi f})^*, \dots, (e^{j2(N-1)\pi f})^*] \\]
\\[ \bar{u}^H = [1, e^{-j2\pi f}, e^{-j4\pi f}, \dots, e^{-j2(N-1)\pi f}] \\]
<p>2. Now, multiply \\(\bar{u}^H\\) by \\(\bar{u}\\):</p>
\\[ ||\bar{u}||^2 = \bar{u}^H \bar{u} = (1)(1) + (e^{-j2\pi f})(e^{j2\pi f}) + \dots + (e^{-j2(N-1)\pi f})(e^{j2(N-1)\pi f}) \\]
\\[ = 1 + e^0 + e^0 + \dots + e^0 \\]
\\[ = \underbrace{1 + 1 + 1 + \dots + 1}_{N \text{ times}} \\]
\\[ = N \\]
<p>The sum consists of \\(N\\) terms, each equal to 1, so the total sum is \\(N\\).</p>
<br>

<p><b>Question 4: Rank of a Matrix</b></p>
<p><b>Question:</b> The rank of the matrix \\(A\\) given below is<br>
\\[ A = 
\begin{bmatrix}
2 & 1 & -4 \\
-3 & -2 & 6 \\
1 & 5 & -7
\end{bmatrix}
\\]
</p>
<p><b>Answer:</b> 3</p>
<p><b>Explanation:</b> For a square matrix, the rank is equal to its size if and only if its determinant is non-zero. Let's calculate the determinant of the 3x3 matrix \\(A\\).</p>
\\[ \det(A) = 2 \begin{vmatrix} -2 & 6 \\ 5 & -7 \end{vmatrix} - 1 \begin{vmatrix} -3 & 6 \\ 1 & -7 \end{vmatrix} + (-4) \begin{vmatrix} -3 & -2 \\ 1 & 5 \end{vmatrix} \\]
\\[ = 2((-2)(-7) - (6)(5)) - 1((-3)(-7) - (6)(1)) - 4((-3)(5) - (-2)(1)) \\]
\\[ = 2(14 - 30) - 1(21 - 6) - 4(-15 + 2) \\]
\\[ = 2(-16) - 1(15) - 4(-13) \\]
\\[ = -32 - 15 + 52 = 5 \\]
<p>Since the determinant is 5 (which is not zero), the matrix is full rank. For a 3x3 matrix, full rank means the rank is 3.</p>
<br>

<p><b>Question 5: Adjoint of a Matrix</b></p>
<p><b>Question:</b> Let \\(C\\) denote the matrix of cofactors for \\(A\\). Then, the adjoint of \\(A\\) denoted by \\(\text{adj}(A)\\), is given as</p>
<p><b>Answer:</b> \\(C^T\\)</p>
<p><b>Explanation:</b> By definition, the <b>adjoint</b> (or <b>adjugate</b>) of a square matrix \\(A\\) is the transpose of its cofactor matrix, \\(C\\). The cofactor matrix \\(C\\) is a matrix of the same size as \\(A\\) where each element \\(C_{ij}\\) is the cofactor of the element \\(A_{ij}\\). Therefore, the correct relationship is \\(\text{adj}(A) = C^T\\).</p>
<br>

<p><b>Question 6: Linear Independence and Orthogonality</b></p>
<p><b>Question:</b> The vectors \\( \bar{u} = \begin{bmatrix} 2 \\ 4 \\ 2 \end{bmatrix} \\) and \\( \bar{v} = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \\) are</p>
<p><b>Answer:</b> Neither orthogonal nor linearly independent</p>
<p><b>Explanation:</b></p>
<p>1. <b>Linear Independence:</b> Two vectors are linearly dependent if one is a scalar multiple of the other. We can see that \\( \bar{u} = 2 \cdot \bar{v} \\), since \\( \begin{bmatrix} 2 \\ 4 \\ 2 \end{bmatrix} = 2 \cdot \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \\). Because they are scalar multiples, the vectors are <b>linearly dependent</b> (i.e., not linearly independent).</p>
<p>2. <b>Orthogonality:</b> Two vectors are orthogonal if their dot product is zero. The dot product is calculated as \\(\bar{u}^T \bar{v}\\).</p>
\\[ \bar{u}^T \bar{v} = (2)(1) + (4)(2) + (2)(1) = 2 + 8 + 2 = 12 \\]
<p>Since the dot product is 12 (not 0), the vectors are <b>not orthogonal</b>.</p>
<p>Therefore, the vectors are neither orthogonal nor linearly independent.</p>
<br>

<p><b>Question 7: Conditions for Invertibility</b></p>
<p><b>Question:</b> Which of the following conditions imply that matrix \\(A\\) of size \\(n \times n\\) is invertible?</p>
<p><b>Answer:</b> All of these</p>
<p><b>Explanation:</b> All the given options are equivalent conditions for an \\(n \times n\\) matrix \\(A\\) to be invertible (or non-singular). This is a core concept in linear algebra, often referred to as the Invertible Matrix Theorem.</p>
<ul>
<li><b>\\(\text{rank}(A) = n\\):</b> A square matrix having full rank means its columns (and rows) are linearly independent, which is a condition for invertibility.</li>
<li><b>\\(\det(A) \neq 0\\):</b> A non-zero determinant is the standard computational test for invertibility.</li>
<li><b>\\(A\bar{x} = 0 \iff \bar{x} = 0\\):</b> This means the homogeneous equation has only the trivial solution. This is equivalent to saying the null space of A contains only the zero vector, which is another condition for invertibility.</li>
</ul>
<p>Since all three conditions are equivalent ways of stating that \\(A\\) is invertible, the correct answer is "All of these".</p>
<br>

<p><b>Question 8: Inverse of a Matrix Formula</b></p>
<p><b>Question:</b> Let \\(C\\) denote the matrix of cofactors for \\(A\\). Then, the inverse of \\(A\\) is given as</p>
<p><b>Answer:</b> \\( \frac{C^T}{|A|} \\)</p>
<p><b>Explanation:</b> The formula for the inverse of a square matrix \\(A\\) is given by its adjoint divided by its determinant.
\\[ A^{-1} = \frac{1}{\det(A)} \text{adj}(A) \\]
As established in Question 5, the adjoint of \\(A\\) is the transpose of the cofactor matrix \\(C\\), so \\(\text{adj}(A) = C^T\\). Substituting this into the formula for the inverse gives:
\\[ A^{-1} = \frac{1}{\det(A)} C^T = \frac{C^T}{|A|} \\]
(Note: \\(\det(A)\\) is often written as \\(|A|\\)).</p>
<br>

<p><b>Question 9: Rank of a Matrix</b></p>
<p><b>Question:</b> The rank of the matrix \\(A\\) given below is<br>
\\[ A = 
\begin{bmatrix}
-2 & -1 & -3 \\
1 & 5 & -3 \\
-3 & 7 & -13
\end{bmatrix}
\\]
</p>
<p><b>Answer:</b> 2</p>
<p><b>Explanation:</b> To find the rank, we can use Gaussian elimination to reduce the matrix to its row echelon form and count the number of non-zero rows.</p>
<p>1. Start with matrix \\(A\\). Swap Row 1 and Row 2 to get a leading 1.</p>
\\[ \begin{bmatrix} 1 & 5 & -3 \\ -2 & -1 & -3 \\ -3 & 7 & -13 \end{bmatrix} \\]
<p>2. Perform row operations to create zeros below the first pivot: \\(R_2 \to R_2 + 2R_1\\) and \\(R_3 \to R_3 + 3R_1\\).</p>
\\[ \begin{bmatrix} 1 & 5 & -3 \\ 0 & 9 & -9 \\ 0 & 22 & -22 \end{bmatrix} \\]
<p>3. Simplify the rows: \\(R_2 \to R_2 / 9\\) and \\(R_3 \to R_3 / 22\\).</p>
\\[ \begin{bmatrix} 1 & 5 & -3 \\ 0 & 1 & -1 \\ 0 & 1 & -1 \end{bmatrix} \\]
<p>4. Create a zero below the second pivot: \\(R_3 \to R_3 - R_2\\).</p>
\\[ \begin{bmatrix} 1 & 5 & -3 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \end{bmatrix} \\]
<p>The resulting matrix is in row echelon form. It has two non-zero rows. Therefore, the rank of matrix \\(A\\) is 2.</p>
<br>

<p><b>Question 10: Hermitian Inner Product</b></p>
<p><b>Question:</b> Consider two vectors \\(\bar{u} = [u_1, u_2, \dots, u_n]^T\\) and \\(\bar{v} = [v_1, v_2, \dots, v_n]^T\\). The quantity \\(\bar{u}^H\bar{v}\\) equals</p>
<p><b>Answer:</b> \\( u_1^*v_1 + u_2^*v_2 + \dots + u_n^*v_n \\)</p>
<p><b>Explanation:</b> The expression \\(\bar{u}^H\bar{v}\\) represents the <b>Hermitian inner product</b> (or complex dot product) of vectors \\(\bar{u}\\) and \\(\bar{v}\\).</p>
<p>1. First, find the Hermitian transpose \\(\bar{u}^H\\). This involves transposing the column vector \\(\bar{u}\\) into a row vector and taking the complex conjugate of each element. The complex conjugate of \\(u_i\\) is denoted \\(u_i^*\\).</p>
\\[ \bar{u}^H = [u_1^*, u_2^*, \dots, u_n^*] \\]
<p>2. Next, perform the matrix multiplication of the row vector \\(\bar{u}^H\\) with the column vector \\(\bar{v}\\).</p>
\\[ \bar{u}^H\bar{v} = [u_1^*, u_2^*, \dots, u_n^*] 
\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = u_1^*v_1 + u_2^*v_2 + \dots + u_n^*v_n \\]
<p>This is the standard definition of the inner product in a complex vector space.</p>
</div></div><div class="week" id="week_3"><h1 class="week-title">Week 3</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 11 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 11 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the Gram-Schmidt procedure, a fundamental algorithm in linear algebra for creating an orthonormal set of vectors from an arbitrary set of basis vectors. The explanation follows the concepts, formulas, and examples presented in the transcript.</p>

<h3>1. The Goal of the Gram-Schmidt Procedure</h3>
<p>The Gram-Schmidt procedure is a method for <b>orthogonalization</b> or <b>orthonormalization</b>. Its primary purpose is to take any given basis for a vector subspace and convert it into a special type of basis called an <b>orthonormal basis</b>.</p>

<p>Let's say we start with an arbitrary basis for a subspace, represented by the set of vectors:
\\[ \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \\]
These vectors span the subspace, meaning any vector within that subspace can be written as a linear combination of them. However, these basis vectors are not necessarily perpendicular to each other, nor do they necessarily have a length of one.</p>

<p>The Gram-Schmidt procedure produces a new basis for the <i>same</i> subspace, represented by the vectors:
\\[ \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\} \\]
This new basis has two special properties that make it an <b>orthonormal basis</b>:</p>
<ol>
    <li><b>Unit Norm:</b> Each vector in the new basis has a length (or norm) of 1.
    \\[ ||\mathbf{v}_i|| = 1 \quad \text{for all } i \\]
    </li>
    <li><b>Orthogonality:</b> Each vector is orthogonal (perpendicular) to every other vector in the basis. The inner product (or dot product for real vectors) of any two distinct vectors is zero. For complex vectors, this is expressed using the Hermitian conjugate.
    \\[ \mathbf{v}_i^H \mathbf{v}_j = 0 \quad \text{for all } i \neq j \\]
    </li>
</ol>
<p>An orthonormal basis is highly desirable in many applications because it simplifies calculations involving projections, coordinate transformations, and numerical algorithms.</p>

<h3>2. The Step-by-Step Algorithm</h3>
<p>The procedure builds the orthonormal basis \\(\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}\\) one vector at a time. At each step, a new vector is made orthogonal to all previously constructed vectors, and then it is normalized to have a unit length.</p>

<h4>Step 1: Construct the first vector \\(\mathbf{v}_1\\)</h4>
<p>The first vector is the easiest. We simply take the first vector from the original basis, \\(\mathbf{x}_1\\), and normalize it by dividing it by its norm (length).</p>
\\[ \mathbf{v}_1 = \frac{\mathbf{x}_1}{||\mathbf{x}_1||} \\]

<h4>Step 2: Construct the second vector \\(\mathbf{v}_2\\)</h4>
<p>To find \\(\mathbf{v}_2\\), we first need to create a vector that is orthogonal to \\(\mathbf{v}_1\\). We do this by taking the second original vector, \\(\mathbf{x}_2\\), and subtracting its <b>projection</b> onto \\(\mathbf{v}_1\\). This process effectively removes the component of \\(\mathbf{x}_2\\) that lies in the direction of \\(\mathbf{v}_1\\), leaving only the part that is orthogonal to it.</p>

<p>First, we create an intermediate vector, which we'll call \\(\mathbf{v}_2^{\sim}\\):</p>
\\[ \mathbf{v}_2^{\sim} = \mathbf{x}_2 - \langle \mathbf{x}_2, \mathbf{v}_1 \rangle \mathbf{v}_1 \\]
<p>Here, \\(\langle \mathbf{x}_2, \mathbf{v}_1 \rangle\\) represents the inner product of \\(\mathbf{x}_2\\) and \\(\mathbf{v}_1\\). For complex vectors, this is written as \\(\mathbf{v}_1^H \mathbf{x}_2\\) or \\(\mathbf{x}_2^H \mathbf{v}_1\\) (depending on convention, the transcript uses the latter). The term \\(\langle \mathbf{x}_2, \mathbf{v}_1 \rangle \mathbf{v}_1\\) is the projection of \\(\mathbf{x}_2\\) onto \\(\mathbf{v}_1\\).</p>
<p>Now, \\(\mathbf{v}_2^{\sim}\\) is guaranteed to be orthogonal to \\(\mathbf{v}_1\\). To make it part of the orthonormal basis, we just need to normalize it:</p>
\\[ \mathbf{v}_2 = \frac{\mathbf{v}_2^{\sim}}{||\mathbf{v}_2^{\sim}||} \\]

<h4>General Step: Construct the i-th vector \\(\mathbf{v}_i\\)</h4>
<p>This process is generalized for any subsequent vector \\(\mathbf{v}_i\\). We take the original vector \\(\mathbf{x}_i\\) and subtract its projections onto <i>all</i> the previously constructed orthonormal vectors (\\(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_{i-1}\\)).</p>

<p>The intermediate orthogonal vector \\(\mathbf{v}_i^{\sim}\\) is calculated as:</p>
\\[ \mathbf{v}_i^{\sim} = \mathbf{x}_i - \sum_{j=1}^{i-1} \langle \mathbf{x}_i, \mathbf{v}_j \rangle \mathbf{v}_j \\]
<p>Using the Hermitian notation from the transcript for the inner product:</p>
\\[ \mathbf{v}_i^{\sim} = \mathbf{x}_i - \sum_{j=1}^{i-1} (\mathbf{x}_i^H \mathbf{v}_j) \mathbf{v}_j \\]
<p>This formula ensures that \\(\mathbf{v}_i^{\sim}\\) is orthogonal to all preceding vectors \\(\mathbf{v}_1, \dots, \mathbf{v}_{i-1}\\). Finally, we normalize this intermediate vector to get the next member of our orthonormal basis:</p>
\\[ \mathbf{v}_i = \frac{\mathbf{v}_i^{\sim}}{||\mathbf{v}_i^{\sim}||} \\]
<p>This process is repeated until all \\(n\\) vectors have been constructed.</p>

<h3>3. Worked Example</h3>
<p>Let's apply the Gram-Schmidt procedure to the basis vectors provided in the transcript:</p>
\\[ \mathbf{x}_1 = \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix}, \quad \mathbf{x}_2 = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}, \quad \mathbf{x}_3 = \begin{pmatrix} 1 \\ -2 \\ 3 \\ -1 \end{pmatrix} \\]

<h4>Step 1: Calculate \\(\mathbf{v}_1\\)</h4>
<p>First, we find the norm of \\(\mathbf{x}_1\\):</p>
\\[ ||\mathbf{x}_1|| = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = \sqrt{4} = 2 \\]
<p>Now, we normalize \\(\mathbf{x}_1\\) to get \\(\mathbf{v}_1\\):</p>
\\[ \mathbf{v}_1 = \frac{\mathbf{x}_1}{||\mathbf{x}_1||} = \frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} \\]

<h4>Step 2: Calculate \\(\mathbf{v}_2\\)</h4>
<p>We start by computing the intermediate vector \\(\mathbf{v}_2^{\sim}\\). Since the vectors are real, the Hermitian transpose \\((\cdot)^H\\) is just the regular transpose \\((\cdot)^T\\).</p>
\\[ \mathbf{v}_2^{\sim} = \mathbf{x}_2 - (\mathbf{x}_2^T \mathbf{v}_1) \mathbf{v}_1 \\]
<p>First, calculate the inner product (projection coefficient):</p>
\\[ \mathbf{x}_2^T \mathbf{v}_1 = \begin{pmatrix} 1 & 2 & 3 & 4 \end{pmatrix} \left( \frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} \right) = \frac{1}{2} (1+2+3+4) = \frac{10}{2} = 5 \\]
<p>Now, subtract the projection from \\(\mathbf{x}_2\\):</p>
\\[ \mathbf{v}_2^{\sim} = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix} - 5 \left( \frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} \right) = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix} - \begin{pmatrix} 2.5 \\ 2.5 \\ 2.5 \\ 2.5 \end{pmatrix} = \begin{pmatrix} -1.5 \\ -0.5 \\ 0.5 \\ 1.5 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \\]
<p>Next, we find the norm of \\(\mathbf{v}_2^{\sim}\\):</p>
\\[ ||\mathbf{v}_2^{\sim}|| = \left|\left| \frac{1}{2} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \right|\right| = \frac{1}{2}\sqrt{(-3)^2 + (-1)^2 + 1^2 + 3^2} = \frac{1}{2}\sqrt{9+1+1+9} = \frac{\sqrt{20}}{2} = \frac{2\sqrt{5}}{2} = \sqrt{5} \\]
<p>Finally, we normalize to get \\(\mathbf{v}_2\\):</p>
\\[ \mathbf{v}_2 = \frac{\mathbf{v}_2^{\sim}}{||\mathbf{v}_2^{\sim}||} = \frac{\frac{1}{2} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix}}{\sqrt{5}} = \frac{1}{2\sqrt{5}} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \\]

<h4>Step 3: Calculate \\(\mathbf{v}_3\\)</h4>
<p>We compute \\(\mathbf{v}_3^{\sim}\\) by subtracting the projections of \\(\mathbf{x}_3\\) onto both \\(\mathbf{v}_1\\) and \\(\mathbf{v}_2\\).</p>
\\[ \mathbf{v}_3^{\sim} = \mathbf{x}_3 - (\mathbf{x}_3^T \mathbf{v}_1) \mathbf{v}_1 - (\mathbf{x}_3^T \mathbf{v}_2) \mathbf{v}_2 \\]
<p>Calculate the inner products:</p>
\\[ \mathbf{x}_3^T \mathbf{v}_1 = \begin{pmatrix} 1 & -2 & 3 & -1 \end{pmatrix} \left( \frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} \right) = \frac{1}{2}(1-2+3-1) = \frac{1}{2} \\]
\\[ \mathbf{x}_3^T \mathbf{v}_2 = \begin{pmatrix} 1 & -2 & 3 & -1 \end{pmatrix} \left( \frac{1}{2\sqrt{5}} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \right) = \frac{1}{2\sqrt{5}}(-3+2+3-3) = \frac{-1}{2\sqrt{5}} \\]
<p>Now, assemble \\(\mathbf{v}_3^{\sim}\\):</p>
\\[ \mathbf{v}_3^{\sim} = \begin{pmatrix} 1 \\ -2 \\ 3 \\ -1 \end{pmatrix} - \left(\frac{1}{2}\right)\frac{1}{2}\begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} - \left(\frac{-1}{2\sqrt{5}}\right)\frac{1}{2\sqrt{5}}\begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \\]
\\[ \mathbf{v}_3^{\sim} = \begin{pmatrix} 1 \\ -2 \\ 3 \\ -1 \end{pmatrix} - \frac{1}{4}\begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} + \frac{1}{20}\begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \\]
<p>Finding a common denominator of 20:</p>
\\[ \mathbf{v}_3^{\sim} = \frac{1}{20} \left( 20\begin{pmatrix} 1 \\ -2 \\ 3 \\ -1 \end{pmatrix} - 5\begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} + \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix} \right) = \frac{1}{20} \begin{pmatrix} 20-5-3 \\ -40-5-1 \\ 60-5+1 \\ -20-5+3 \end{pmatrix} = \frac{1}{20}\begin{pmatrix} 12 \\ -46 \\ 56 \\ -22 \end{pmatrix} \\]
<p>The norm of the vector part is \\(\sqrt{12^2 + (-46)^2 + 56^2 + (-22)^2} = \sqrt{144 + 2116 + 3136 + 484} = \sqrt{5880} = 14\sqrt{30}\\). The norm of \\(\mathbf{v}_3^{\sim}\\) is therefore \\(\frac{14\sqrt{30}}{20}\\).</p>
<p>Finally, we normalize to get \\(\mathbf{v}_3\\):</p>
\\[ \mathbf{v}_3 = \frac{\mathbf{v}_3^{\sim}}{||\mathbf{v}_3^{\sim}||} = \frac{\frac{1}{20}\begin{pmatrix} 12 \\ -46 \\ 56 \\ -22 \end{pmatrix}}{\frac{\sqrt{5880}}{20}} = \frac{1}{\sqrt{5880}}\begin{pmatrix} 12 \\ -46 \\ 56 \\ -22 \end{pmatrix} = \frac{1}{14\sqrt{30}}\begin{pmatrix} 12 \\ -46 \\ 56 \\ -22 \end{pmatrix} \\]

<h3>4. Final Result</h3>
<p>The Gram-Schmidt procedure has transformed the original basis into the following orthonormal basis:</p>
\\[ \left\{ \frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix}, \quad \frac{1}{2\sqrt{5}} \begin{pmatrix} -3 \\ -1 \\ 1 \\ 3 \end{pmatrix}, \quad \frac{1}{14\sqrt{30}}\begin{pmatrix} 12 \\ -46 \\ 56 \\ -22 \end{pmatrix} \right\} \\]
<p>Each vector in this set has a norm of 1, and each vector is orthogonal to the others. This new basis spans the exact same subspace as the original set of vectors \\(\{\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\}\\).</p>
</div></div><div class="chapter" id="Lecture 12 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 12 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts surrounding Gaussian random variables and vectors, as presented in the transcript. These concepts are fundamental in linear system analysis, signal processing, and machine learning.</p>

<b>1. The Gaussian Random Variable</b>
<p>A Gaussian random variable, also known as a Normal random variable, is one of the most important continuous random variables in probability and statistics. Its probability distribution is described by a symmetric, bell-shaped curve.</p>
<p>The distribution is completely characterized by two parameters:</p>
<ul>
    <li><b>Mean (\\(\mu\\)):</b> This parameter represents the central tendency or the average value of the random variable. On the graph of the probability density function (PDF), the mean is the location of the peak of the bell curve. The expected value of the random variable \\(X\\) is its mean:
    \\[ E[X] = \mu \\]
    </li>
    <li><b>Variance (\\(\sigma^2\\)):</b> This parameter measures the spread or dispersion of the distribution. A larger variance results in a wider, flatter bell curve, indicating that the values are more spread out from the mean. A smaller variance results in a taller, narrower curve. The variance is defined as the expected value of the squared deviation from the mean:
    \\[ E[(X - \mu)^2] = \sigma^2 \\]
    The square root of the variance, \\(\sigma\\), is called the standard deviation.
    </li>
</ul>
<p>The <b>Probability Density Function (PDF)</b> of a Gaussian random variable \\(X\\) is given by the formula:</p>
\\[ f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\]
<p>where \\(x\\) can be any real number (\\(-\infty < x < \infty\\)). This function describes the likelihood of the random variable taking on a particular value \\(x\\).</p>

<b>2. The Gaussian Random Vector (Multivariate Gaussian)</b>
<p>In many applications, we deal with multiple random variables simultaneously. A <b>Gaussian random vector</b> is a collection of random variables, \\(\mathbf{x} = [x_1, x_2, \dots, x_n]^T\\), where any linear combination of these variables is also a Gaussian random variable. This property is known as being "jointly Gaussian."</p>
<p>Similar to the single-variable case, a multivariate Gaussian distribution is characterized by a mean vector and a covariance matrix.</p>
<ul>
    <li><b>Mean Vector (\\(\boldsymbol{\mu}\\)):</b> This is an n-dimensional vector where each element is the mean of the corresponding random variable.
    \\[ \boldsymbol{\mu} = E[\mathbf{x}] = \begin{pmatrix} E[x_1] \\ E[x_2] \\ \vdots \\ E[x_n] \end{pmatrix} \\]
    </li>
    <li><b>Covariance Matrix (\\(\mathbf{R}\\)):</b> This is an \\(n \times n\\) matrix that describes the variance of each individual component and the covariance between pairs of components. It is defined as:
    \\[ \mathbf{R} = E[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^T] \\]
    The diagonal elements \\(r_{ii}\\) of \\(\mathbf{R}\\) are the variances of the individual random variables \\(x_i\\), and the off-diagonal elements \\(r_{ij}\\) are the covariances between \\(x_i\\) and \\(x_j\\). The covariance matrix is always symmetric (\\(r_{ij} = r_{ji}\\)).
    </li>
</ul>
<p>The <b>PDF of a multivariate Gaussian</b> random vector \\(\mathbf{x}\\) is given by:</p>
\\[ f_{\mathbf{X}}(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n \det(\mathbf{R})}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \mathbf{R}^{-1}(\mathbf{x}-\boldsymbol{\mu})} \\]
<p>Here, \\(n\\) is the dimension of the vector, \\(\det(\mathbf{R})\\) is the determinant of the covariance matrix, and \\(\mathbf{R}^{-1}\\) is its inverse. The term in the exponent, \\((\mathbf{x}-\boldsymbol{\mu})^T \mathbf{R}^{-1}(\mathbf{x}-\boldsymbol{\mu})\\), is a quadratic form that generalizes the squared distance from the mean found in the single-variable case.</p>

<b>3. Analysis of the Covariance Matrix</b>
<p>The structure of the covariance matrix \\(\mathbf{R}\\) reveals the relationships between the components of the random vector. For simplicity, consider the case where the mean vector is zero (\\(\boldsymbol{\mu} = \mathbf{0}\\)). The covariance matrix then simplifies to:</p>
\\[ \mathbf{R} = E[\mathbf{x}\mathbf{x}^T] \\]
<p>The elements of this matrix are:</p>
<ul>
    <li><b>Diagonal Entries (\\(r_{ii}\\)):</b> These are the variances of the individual random variables.
    \\[ r_{ii} = E[x_i^2] = \text{Variance of } x_i \\]
    </li>
    <li><b>Off-Diagonal Entries (\\(r_{ij}\\) for \\(i \neq j\\)):</b> These are the correlations (covariances, in the zero-mean case) between pairs of random variables.
    \\[ r_{ij} = r_{ji} = E[x_i x_j] \\]
    </li>
</ul>

<b>4. Special Cases and Key Properties</b>
<p>The transcript highlights two important special cases related to the structure of the covariance matrix.</p>

<b>Case 1: Uncorrelated Components (Diagonal Covariance Matrix)</b>
<p>If the off-diagonal entries of the covariance matrix \\(\mathbf{R}\\) are all zero, the matrix is diagonal.
\\[ \mathbf{R} = \begin{pmatrix} \sigma_1^2 & 0 & \cdots & 0 \\ 0 & \sigma_2^2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma_n^2 \end{pmatrix} \\]
This means that for any pair of different random variables \\(x_i\\) and \\(x_j\\) (where \\(i \neq j\\)), their correlation is zero:
\\[ E[x_i x_j] = 0 \quad (\text{for } i \neq j) \\]
<p>Random variables with zero correlation are said to be <b>uncorrelated</b>. A crucial property of jointly Gaussian random variables is that if they are uncorrelated, they are also <b>independent</b>. This is a powerful result that does not hold for random variables in general.</p>

<b>Case 2: Independent and Identically Distributed (i.i.d.) Components</b>
<p>This is a further simplification where the covariance matrix is diagonal and all the diagonal elements (variances) are equal.
\\[ \mathbf{R} = \sigma^2 \mathbf{I} = \begin{pmatrix} \sigma^2 & 0 & \cdots & 0 \\ 0 & \sigma^2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma^2 \end{pmatrix} \\]
In this scenario, where \\(\mathbf{I}\\) is the identity matrix:</p>
<ul>
    <li>The components \\(x_1, x_2, \dots, x_n\\) are <b>independent</b> because the covariance matrix is diagonal.</li>
    <li>They are <b>identically distributed</b> because they share the same mean (assumed to be 0 here) and the same variance (\\(\sigma^2\\)).</li>
</ul>
<p>Such a collection of random variables is referred to as <b>i.i.d. (independent and identically distributed)</b>. This is a common assumption for modeling noise in many signal processing and communication systems, as it simplifies analysis significantly.</p>
</div></div><div class="chapter" id="Lecture 13 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 13 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<h3>Introduction: Linear Transformation of Gaussian Random Vectors</h3>
<p>This module explores a fundamental and powerful property of Gaussian (or Normal) random vectors: their behavior under linear transformations. This concept is crucial in many fields, especially in the analysis of linear systems, signal processing, and statistical modeling. A random vector is a collection of random variables, and a Gaussian random vector is one where these variables are jointly Gaussian.</p>
<p>The core idea presented is that if you take a Gaussian random vector and apply a linear transformation to it, the resulting vector is also a Gaussian random vector. This property, often summarized as <b>"Gaussian remains Gaussian,"</b> makes the analysis of such systems much more tractable.</p>

<h3>The General Linear Transformation</h3>
<p>Let's start with the general case. We have a Gaussian random vector, which the transcript denotes as \\(\bar{x}\\) (we will use the more common notation \\(\mathbf{x}\\)). This vector is characterized by its mean vector and its covariance matrix.</p>
<p><b>1. Initial Gaussian Random Vector</b></p>
<p>Let \\(\mathbf{x}\\) be an \\(n \times 1\\) Gaussian random vector with mean vector \\(\boldsymbol{\mu_x}\\) and covariance matrix \\(R\\). This is denoted as:</p>
\\[ \mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu_x}, R) \\]
<p>This notation implies two things:</p>
<ul>
    <li>The expected value (mean) of the vector is: \\( E[\mathbf{x}] = \boldsymbol{\mu_x} \\)</li>
    <li>The covariance matrix, which describes the variance of each component and the covariance between components, is: \\( R = E[(\mathbf{x} - \boldsymbol{\mu_x})(\mathbf{x} - \boldsymbol{\mu_x})^T] \\)</li>
</ul>

<p><b>2. The Linear Transformation</b></p>
<p>We now define a new random vector, \\(\mathbf{y}\\), by applying a linear transformation to \\(\mathbf{x}\\). The transformation is defined as:</p>
\\[ \mathbf{y} = A\mathbf{x} + \mathbf{b} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{x}\\) is the original \\(n \times 1\\) Gaussian random vector.</li>
    <li>\\(A\\) is a constant \\(m \times n\\) matrix.</li>
    <li>\\(\mathbf{b}\\) is a constant \\(m \times 1\\) vector.</li>
    <li>\\(\mathbf{y}\\) is the resulting \\(m \times 1\\) random vector.</li>
</ul>
<p><i>Note: This is technically an affine transformation, but it is commonly referred to as a linear transformation in this context.</i></p>

<p><b>3. The Resulting Distribution</b></p>
<p>The key property is that \\(\mathbf{y}\\) is also a Gaussian random vector. To fully describe its distribution, we need to find its mean \\(\boldsymbol{\mu_y}\\) and its covariance matrix \\(R_y\\).</p>

<p><b>Mean of \\(\mathbf{y}\\):</b></p>
<p>The mean of \\(\mathbf{y}\\) is found by taking the expectation of the transformation. Since the expectation operator \\(E[\cdot]\\) is linear, we can distribute it through the expression:</p>
\\[ \boldsymbol{\mu_y} = E[\mathbf{y}] = E[A\mathbf{x} + \mathbf{b}] \\]
\\[ \boldsymbol{\mu_y} = E[A\mathbf{x}] + E[\mathbf{b}] \\]
<p>Since \\(A\\) and \\(\mathbf{b}\\) are constants, this simplifies to:</p>
\\[ \boldsymbol{\mu_y} = A E[\mathbf{x}] + \mathbf{b} \\]
<p>Substituting \\(E[\mathbf{x}] = \boldsymbol{\mu_x}\\), we get the final result for the mean of \\(\mathbf{y}\\):</p>
\\[ \boldsymbol{\mu_y} = A\boldsymbol{\mu_x} + \mathbf{b} \\]

<p><b>Covariance of \\(\mathbf{y}\\):</b></p>
<p>The covariance matrix of \\(\mathbf{y}\\) is defined as \\(R_y = E[(\mathbf{y} - \boldsymbol{\mu_y})(\mathbf{y} - \boldsymbol{\mu_y})^T]\\). To derive this, we first express the term \\(\mathbf{y} - \boldsymbol{\mu_y}\\):</p>
\\[ \mathbf{y} - \boldsymbol{\mu_y} = (A\mathbf{x} + \mathbf{b}) - (A\boldsymbol{\mu_x} + \mathbf{b}) = A\mathbf{x} - A\boldsymbol{\mu_x} = A(\mathbf{x} - \boldsymbol{\mu_x}) \\]
<p>Now, we substitute this back into the covariance formula:</p>
\\[ R_y = E\left[ \left( A(\mathbf{x} - \boldsymbol{\mu_x}) \right) \left( A(\mathbf{x} - \boldsymbol{\mu_x}) \right)^T \right] \\]
<p>Using the property of transpose \\((BC)^T = C^T B^T\\), the second term becomes \\((\mathbf{x} - \boldsymbol{\mu_x})^T A^T\\). The expression is now:</p>
\\[ R_y = E\left[ A (\mathbf{x} - \boldsymbol{\mu_x}) (\mathbf{x} - \boldsymbol{\mu_x})^T A^T \right] \\]
<p>Since \\(A\\) and \\(A^T\\) are constant matrices, we can pull them outside the expectation operator:</p>
\\[ R_y = A \cdot E\left[ (\mathbf{x} - \boldsymbol{\mu_x}) (\mathbf{x} - \boldsymbol{\mu_x})^T \right] \cdot A^T \\]
<p>The term inside the expectation is, by definition, the covariance matrix of \\(\mathbf{x}\\), which is \\(R\\). Therefore, the covariance matrix of \\(\mathbf{y}\\) is:</p>
\\[ R_y = A R A^T \\]

<p><b>Summary of the General Result</b></p>
<p>If \\(\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu_x}, R)\\) and \\(\mathbf{y} = A\mathbf{x} + \mathbf{b}\\), then the transformed vector \\(\mathbf{y}\\) is also Gaussian with the following distribution:</p>
\\[ \mathbf{y} \sim \mathcal{N}(A\boldsymbol{\mu_x} + \mathbf{b}, A R A^T) \\]

<h3>Special Case: Weighted Sum of IID Gaussian Variables</h3>
<p>The transcript examines a very common and important special case: forming a weighted sum of independent and identically distributed (IID) Gaussian random variables.</p>

<p><b>1. The IID Gaussian Vector</b></p>
<p>Consider a vector \\(\mathbf{x}\\) composed of \\(n\\) random variables \\(x_1, x_2, \dots, x_n\\) that are:</p>
<ul>
    <li><b>Independent:</b> The value of one variable gives no information about the others.</li>
    <li><b>Identically Distributed:</b> They all follow the same probability distribution.</li>
    <li><b>Zero-Mean Gaussian:</b> Each \\(x_i\\) has a mean of 0 and a variance of \\(\sigma^2\\). That is, \\(E[x_i] = 0\\) and \\(E[x_i^2] = \sigma^2\\).</li>
</ul>
<p>For this vector \\(\mathbf{x}\\):</p>
<ul>
    <li>The mean vector is the zero vector: \\(\boldsymbol{\mu_x} = \mathbf{0}\\).</li>
    <li>The covariance matrix is \\(R = \sigma^2 I\\), where \\(I\\) is the identity matrix. The off-diagonal elements are zero due to independence, and the diagonal elements are all \\(\sigma^2\\) because the variables are identically distributed.</li>
</ul>
\\[ R = E[\mathbf{x}\mathbf{x}^T] = \begin{pmatrix} \sigma^2 & 0 & \cdots & 0 \\ 0 & \sigma^2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma^2 \end{pmatrix} = \sigma^2 I \\]

<p><b>2. The Weighted Sum Transformation</b></p>
<p>We now create a new scalar random variable \\(y\\) which is a weighted sum of the components of \\(\mathbf{x}\\):</p>
\\[ y = a_1 x_1 + a_2 x_2 + \dots + a_n x_n = \sum_{i=1}^{n} a_i x_i \\]
<p>This can be written in vector form as a dot product:</p>
\\[ y = \begin{pmatrix} a_1 & a_2 & \cdots & a_n \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} = \mathbf{a}^T \mathbf{x} \\]
<p>This is a specific instance of the linear transformation \\(\mathbf{y} = A\mathbf{x} + \mathbf{b}\\), where \\(A = \mathbf{a}^T\\) (an \\(1 \times n\\) matrix) and \\(\mathbf{b} = 0\\).</p>

<p><b>3. Mean and Variance of the Weighted Sum</b></p>
<p>Since \\(y\\) is a linear transformation of a Gaussian vector, \\(y\\) itself is a Gaussian random variable (a scalar). We can find its mean and variance using the general formulas.</p>
<p><b>Mean of \\(y\\):</b></p>
\\[ E[y] = E[\mathbf{a}^T \mathbf{x}] = \mathbf{a}^T E[\mathbf{x}] = \mathbf{a}^T \mathbf{0} = 0 \\]
<p>The resulting weighted sum is also a zero-mean random variable.</p>

<p><b>Variance of \\(y\\):</b></p>
<p>The variance is \\(\text{Var}(y) = E[(y - E[y])^2] = E[y^2]\\) since the mean is zero. Using the formula for the transformed covariance matrix \\(A R A^T\\):</p>
\\[ \text{Var}(y) = (\mathbf{a}^T) (\sigma^2 I) (\mathbf{a}^T)^T = \sigma^2 (\mathbf{a}^T I \mathbf{a}) = \sigma^2 \mathbf{a}^T \mathbf{a} \\]
<p>The term \\(\mathbf{a}^T \mathbf{a}\\) is the dot product of the vector of weights with itself, which is the squared Euclidean norm of the vector, denoted \\(||\mathbf{a}||^2\\).</p>
\\[ \mathbf{a}^T \mathbf{a} = ||\mathbf{a}||^2 = \sum_{i=1}^{n} a_i^2 \\]
<p>So, the variance is:</p>
\\[ \text{Var}(y) = \sigma^2 ||\mathbf{a}||^2 = \sigma^2 \sum_{i=1}^{n} a_i^2 \\]

<p>The final result is that the weighted sum \\(y\\) is a Gaussian random variable with mean 0 and variance \\(\sigma^2 ||\mathbf{a}||^2\\):</p>
\\[ y \sim \mathcal{N}(0, \sigma^2 ||\mathbf{a}||^2) \\]

<h3>Application: Beamforming</h3>
<p>The transcript mentions that this special case has a direct and important application in wireless communications, known as <b>beamforming</b>. In a system with an antenna array (multiple antennas), the receiver can combine the signals \\((x_i)\\) from each antenna element using a set of complex weights \\((a_i)\\). By carefully choosing these weights (the "beamforming vector" \\(\mathbf{a}\\)), the receiver can amplify signals coming from a desired direction and suppress interference from other directions, effectively "forming a beam" of sensitivity in space.</p>

</div></div><div class="chapter" id="Lecture 14 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 14 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on a machine learning application of Gaussian random vectors: <b>classification with Gaussian classes</b>.</p>

<h3>1. The Classification Problem</h3>
<p>The core problem is to classify a new data observation into one of two predefined classes. It is assumed that the data points within each class are distributed according to a multivariate Gaussian distribution.</p>
<p><b>Problem Setup:</b></p>
<ul>
    <li>There are two classes, which we'll call Class 1 (\\(C_1\\)) and Class 2 (\\(C_2\\)).</li>
    <li>Data points in each class are represented by random vectors.</li>
    <li><b>Class 1</b> is modeled by a Gaussian distribution with mean vector \\(\bar{\mu}_1\\) and covariance matrix \\(\Sigma\\).</li>
    <li><b>Class 2</b> is modeled by a Gaussian distribution with mean vector \\(\bar{\mu}_2\\) and covariance matrix \\(\Sigma\\).</li>
</ul>
<p>A key simplifying assumption here is that both classes share the <b>same covariance matrix \\(\Sigma\\)</b>, but they have different means (\\(\bar{\mu}_1 \neq \bar{\mu}_2\\)). The central question is: given a new observation vector \\(\bar{x}\\), does it belong to Class 1 or Class 2?</p>
<p>A practical example is image segmentation, where pixels belonging to a foreground object (Class 1) might have a different average color (mean) than pixels in the background (Class 2), while their color variations (covariance) might be similar.</p>

<h3>2. The Likelihood-Based Classifier</h3>
<p>To decide which class \\(\bar{x}\\) belongs to, we use a principle based on likelihood. The <b>likelihood</b> of a class for a given observation \\(\bar{x}\\) is the value of that class's probability density function (PDF) evaluated at \\(\bar{x}\\).</p>

<p><b>Likelihood of Class 1:</b><br>
This is the multivariate Gaussian PDF for Class 1, evaluated at \\(\bar{x}\\).
\\[ p(\bar{x} | \bar{\mu}_1, \Sigma) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2}(\bar{x} - \bar{\mu}_1)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_1)\right) \\]
where \\(n\\) is the dimension of the vector \\(\bar{x}\\) and \\(\det(\Sigma)\\) is the determinant of the covariance matrix.</p>

<p><b>Likelihood of Class 2:</b><br>
Similarly, this is the PDF for Class 2, evaluated at \\(\bar{x}\\).
\\[ p(\bar{x} | \bar{\mu}_2, \Sigma) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp\left(-\frac{1}{2}(\bar{x} - \bar{\mu}_2)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_2)\right) \\]
</p>

<p><b>The Classification Rule:</b><br>
The decision rule is intuitive: assign \\(\bar{x}\\) to the class with the higher likelihood.
</p>
<ul>
    <li>Choose <b>Class 1</b> if: \\( p(\bar{x} | \bar{\mu}_1, \Sigma) \ge p(\bar{x} | \bar{\mu}_2, \Sigma) \\)</li>
    <li>Choose <b>Class 2</b> if: \\( p(\bar{x} | \bar{\mu}_1, \Sigma) < p(\bar{x} | \bar{\mu}_2, \Sigma) \\)</li>
</ul>
<p>This can be written compactly as:
\\[ p(\bar{x} | \bar{\mu}_1, \Sigma) \underset{C_2}{\overset{C_1}{\gtrless}} p(\bar{x} | \bar{\mu}_2, \Sigma) \\]
</p>

<h3>3. Derivation and Simplification of the Rule</h3>
<p>We can simplify the classification rule by substituting the PDF formulas into the inequality.</p>
<p><b>Step 1: Cancel the constant term</b><br>
The term \\( \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \\) is common to both sides and positive, so it can be cancelled. The comparison becomes:
\\[ \exp\left(-\frac{1}{2}(\bar{x} - \bar{\mu}_1)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_1)\right) \ge \exp\left(-\frac{1}{2}(\bar{x} - \bar{\mu}_2)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_2)\right) \\]
</p>
<p><b>Step 2: Remove the exponential</b><br>
Since the exponential function \\(e^z\\) is monotonically increasing, we can compare its arguments directly. However, because of the negative sign in the exponent, the direction of the inequality is reversed:
\\[ -\frac{1}{2}(\bar{x} - \bar{\mu}_1)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_1) \ge -\frac{1}{2}(\bar{x} - \bar{\mu}_2)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_2) \\]
Multiplying by -2 (and flipping the inequality again) gives:
\\[ (\bar{x} - \bar{\mu}_1)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_1) \le (\bar{x} - \bar{\mu}_2)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_2) \\]
The term \\((\bar{x} - \bar{\mu})^T \Sigma^{-1} (\bar{x} - \bar{\mu})\\) is the squared Mahalanobis distance from \\(\bar{x}\\) to the mean \\(\bar{\mu}\\). So, this rule classifies \\(\bar{x}\\) to the class with the smaller Mahalanobis distance.</p>

<p><b>Step 3: Expand and simplify the quadratic forms</b><br>
Expanding both sides gives:
\\[ \bar{x}^T\Sigma^{-1}\bar{x} - 2\bar{\mu}_1^T\Sigma^{-1}\bar{x} + \bar{\mu}_1^T\Sigma^{-1}\bar{\mu}_1 \le \bar{x}^T\Sigma^{-1}\bar{x} - 2\bar{\mu}_2^T\Sigma^{-1}\bar{x} + \bar{\mu}_2^T\Sigma^{-1}\bar{\mu}_2 \\]
The term \\(\bar{x}^T\Sigma^{-1}\bar{x}\\) cancels from both sides. Rearranging the remaining terms to group \\(\bar{x}\\) yields:
\\[ 2(\bar{\mu}_2 - \bar{\mu}_1)^T\Sigma^{-1}\bar{x} \le \bar{\mu}_2^T\Sigma^{-1}\bar{\mu}_2 - \bar{\mu}_1^T\Sigma^{-1}\bar{\mu}_1 \\]
Multiplying by -1 and swapping terms gives:
\\[ 2(\bar{\mu}_1 - \bar{\mu}_2)^T\Sigma^{-1}\bar{x} \ge \bar{\mu}_1^T\Sigma^{-1}\bar{\mu}_1 - \bar{\mu}_2^T\Sigma^{-1}\bar{\mu}_2 \\]
This leads to the final form shown in the transcript after some manipulation:
\\[ (\bar{\mu}_1 - \bar{\mu}_2)^T \Sigma^{-1} \left(\bar{x} - \frac{\bar{\mu}_1 + \bar{\mu}_2}{2}\right) \ge 0 \\]
This elegant result defines a <b>linear classifier</b>. The expression \\(\bar{w}^T(\bar{x}-\bar{x}_0)=0\\) defines a hyperplane, and the classification depends on which side of this plane the point \\(\bar{x}\\) falls.
</p>

<h3>4. Special Case: Independent and Identically Distributed (IID) Components</h3>
<p>A very intuitive result emerges when we consider a simpler covariance structure where the components of the Gaussian vectors are independent and have the same variance \\(\sigma^2\\). In this case, the covariance matrix is a scaled identity matrix:</p>
\\[ \Sigma = \sigma^2 I \\]
Its inverse is simply:
\\[ \Sigma^{-1} = \frac{1}{\sigma^2} I \\]
Substituting this into our decision rule:
\\[ (\bar{\mu}_1 - \bar{\mu}_2)^T \left(\frac{1}{\sigma^2} I\right) \left(\bar{x} - \frac{\bar{\mu}_1 + \bar{\mu}_2}{2}\right) \ge 0 \\]
Since \\(\frac{1}{\sigma^2}\\) is a positive scalar, we can remove it without changing the inequality:
\\[ (\bar{\mu}_1 - \bar{\mu}_2)^T \left(\bar{x} - \frac{\bar{\mu}_1 + \bar{\mu}_2}{2}\right) \ge 0 \\]
<p>We choose Class 1 if the inequality holds, and Class 2 otherwise.</p>

<h3>5. Geometric Interpretation: The Minimum Distance Classifier</h3>
<p>This simplified rule has a clear and powerful geometric meaning.</p>
<ul>
    <li>The vector \\( \bar{\mu}_1 - \bar{\mu}_2 \\) points from the mean of Class 2 to the mean of Class 1.</li>
    <li>The point \\( \bar{x}_0 = \frac{\bar{\mu}_1 + \bar{\mu}_2}{2} \\) is the midpoint of the line segment connecting the two means.</li>
    <li>The vector \\( \bar{x} - \bar{x}_0 \\) points from this midpoint to our new observation \\(\bar{x}\\).</li>
</ul>
<p>The expression \\((\bar{\mu}_1 - \bar{\mu}_2)^T (\bar{x} - \bar{x}_0)\\) is the dot product of these two vectors. The dot product is positive if and only if the angle between the vectors is less than 90°. This means \\(\bar{x}\\) lies on the same side of the perpendicular bisector of the means as \\(\bar{\mu}_1\\).</p>
<p>The equation for the decision boundary is:
\\[ (\bar{\mu}_1 - \bar{\mu}_2)^T \left(\bar{x} - \frac{\bar{\mu}_1 + \bar{\mu}_2}{2}\right) = 0 \\]
This equation defines the hyperplane that is the <b>perpendicular bisector</b> of the line segment joining \\(\bar{\mu}_1\\) and \\(\bar{\mu}_2\\).</p>

<p>Therefore, the classification rule simplifies to:</p>
<ul>
    <li>If \\(\bar{x}\\) lies on the side of the perpendicular bisector closer to \\(\bar{\mu}_1\\), classify it as <b>Class 1</b>.</li>
    <li>If it lies on the side closer to \\(\bar{\mu}_2\\), classify it as <b>Class 2</b>.</li>
</ul>
<p>This is equivalent to classifying \\(\bar{x}\\) based on which mean it is closer to in Euclidean distance. This is why it is called a <b>Minimum Distance Classifier</b>. The condition is: choose Class 1 if
\\[ ||\bar{x} - \bar{\mu}_1|| \le ||\bar{x} - \bar{\mu}_2|| \\]
This demonstrates that for the special case of Gaussian classes with identical and spherical covariances (\\(\Sigma = \sigma^2 I\\)), the sophisticated likelihood-based approach reduces to a simple and intuitive geometric rule.</p>
</div></div><div class="chapter" id="Lecture 15 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 15 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts of eigenvalues and eigenvalue decomposition (EVD) as presented in the transcript. This is one of the most fundamental and impactful topics in linear algebra, with widespread applications in numerous fields.</p>

<b>1. The Core Concept: Eigenvectors and Eigenvalues</b>
<p>Eigenvalue analysis is defined for <b>square matrices</b>. Given an \\( n \times n \\) square matrix \\( A \\), we are interested in finding special vectors that, when transformed by \\( A \\), do not change their direction.</p>

<p>A non-zero vector \\( \bar{u} \\) is called an <b>eigenvector</b> of matrix \\( A \\) if the matrix-vector product \\( A\bar{u} \\) is simply a scaled version of the original vector \\( \bar{u} \\). The scaling factor is a scalar value \\( \lambda \\), known as the corresponding <b>eigenvalue</b>.</p>

<p>This relationship is captured by the fundamental equation:</p>
\\[ A\bar{u} = \lambda \bar{u} \\]
<p>Here:</p>
<ul>
    <li>\\( A \\) is an \\( n \times n \\) square matrix.</li>
    <li>\\( \bar{u} \\) is an \\( n \times 1 \\) non-zero vector (the eigenvector).</li>
    <li>\\( \lambda \\) is a scalar (the eigenvalue).</li>
</ul>
<p>Conceptually, if you consider the vector \\( \bar{u} \\) as an input to a system represented by matrix \\( A \\), the output \\( A\bar{u} \\) is perfectly aligned with the input, just stretched or shrunk by the factor \\( \lambda \\).</p>

<b>2. Finding Eigenvalues: The Characteristic Equation</b>
<p>To find the eigenvalues of a matrix \\( A \\), we can rearrange the core equation. The process is as follows:</p>
<ol>
    <li>Start with the definition: \\( A\bar{u} = \lambda \bar{u} \\).</li>
    <li>Bring all terms to one side: \\( A\bar{u} - \lambda \bar{u} = \bar{0} \\).</li>
    <li>To factor out \\( \bar{u} \\), we introduce the identity matrix \\( I \\) of the same size as \\( A \\): \\( A\bar{u} - \lambda I \bar{u} = \bar{0} \\).</li>
    <li>Factor out the vector \\( \bar{u} \\): \\( (A - \lambda I)\bar{u} = \bar{0} \\).</li>
</ol>
<p>This equation, \\( (A - \lambda I)\bar{u} = \bar{0} \\), signifies that the eigenvector \\( \bar{u} \\) lies in the <b>null space</b> of the matrix \\( (A - \lambda I) \\). Since eigenvectors are defined to be non-zero, this means the null space must be non-trivial. A matrix has a non-trivial null space if and only if it is <b>singular</b>, which in turn means its determinant is zero.</p>
<p>This leads to the <b>characteristic equation</b>, which allows us to solve for the eigenvalues \\( \lambda \\):</p>
\\[ \det(A - \lambda I) = 0 \\]
<p>Solving this equation, which will be a polynomial in \\( \lambda \\) of degree \\( n \\), yields the \\( n \\) eigenvalues of the matrix \\( A \\).</p>

<b>3. Finding Eigenvectors</b>
<p>Once an eigenvalue \\( \lambda \\) has been found by solving the characteristic equation, its corresponding eigenvector(s) \\( \bar{u} \\) can be found by substituting the value of \\( \lambda \\) back into the equation:</p>
\\[ (A - \lambda I)\bar{u} = \bar{0} \\]
<p>This is equivalent to finding the basis vectors for the null space of the matrix \\( A - \lambda I \\).</p>
<p>An important property mentioned in the transcript is that if \\( \bar{u} \\) is an eigenvector, then any scaled version \\( \alpha\bar{u} \\) (where \\( \alpha \\) is a non-zero scalar) is also an eigenvector for the same eigenvalue \\( \lambda \\). This is because:</p>
\\[ A(\alpha\bar{u}) = \alpha(A\bar{u}) = \alpha(\lambda\bar{u}) = \lambda(\alpha\bar{u}) \\]
<p>This is why solving for an eigenvector will always involve a free variable, allowing for infinite solutions that all lie along the same line (or in the same subspace).</p>

<b>4. Worked Example</b>
<p>Let's use the example from the transcript to find the eigenvalues and eigenvectors.</p>
<p>Consider the matrix:
\\[ A = \begin{pmatrix} 2 & 2 \\ 3 & 1 \end{pmatrix} \\]
</p>
<p><b>Step 1: Find the Eigenvalues</b></p>
<p>First, we set up the characteristic equation \\( \det(A - \lambda I) = 0 \\).</p>
\\[ A - \lambda I = \begin{pmatrix} 2 & 2 \\ 3 & 1 \end{pmatrix} - \lambda \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 2-\lambda & 2 \\ 3 & 1-\lambda \end{pmatrix} \\]
<p>Now, we compute the determinant and set it to zero:</p>
\\[ \det(A - \lambda I) = (2-\lambda)(1-\lambda) - (3)(2) = 0 \\]
\\[ 2 - 2\lambda - \lambda + \lambda^2 - 6 = 0 \\]
\\[ \lambda^2 - 3\lambda - 4 = 0 \\]
<p>Factoring the quadratic equation:</p>
\\[ (\lambda - 4)(\lambda + 1) = 0 \\]
<p>The eigenvalues are \\( \lambda_1 = 4 \\) and \\( \lambda_2 = -1 \\).</p>

<p><b>Step 2: Find the Eigenvectors</b></p>
<p><b>For \\( \lambda_1 = 4 \\)</b>: We solve \\( (A - 4I)\bar{u} = \bar{0} \\).</p>
\\[ \begin{pmatrix} 2-4 & 2 \\ 3 & 1-4 \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \implies \begin{pmatrix} -2 & 2 \\ 3 & -3 \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\]
<p>This gives the system of equations: \\( -2u_1 + 2u_2 = 0 \\) and \\( 3u_1 - 3u_2 = 0 \\). Both equations simplify to \\( u_1 = u_2 \\). We can choose any value for the free variable. If we set \\( u_2 = 1 \\), then \\( u_1 = 1 \\). So, the eigenvector is:</p>
\\[ \bar{u}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \\]

<p><b>For \\( \lambda_2 = -1 \\)</b>: We solve \\( (A - (-1)I)\bar{u} = \bar{0} \\).</p>
\\[ \begin{pmatrix} 2-(-1) & 2 \\ 3 & 1-(-1) \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \implies \begin{pmatrix} 3 & 2 \\ 3 & 2 \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\]
<p>This gives the equation \\( 3u_1 + 2u_2 = 0 \\). If we set \\( u_1 = 2 \\), then \\( 3(2) + 2u_2 = 0 \implies 2u_2 = -6 \implies u_2 = -3 \\). So, the eigenvector is:</p>
\\[ \bar{u}_2 = \begin{pmatrix} 2 \\ -3 \end{pmatrix} \\]

<b>5. Eigenvalue Decomposition (EVD)</b>
<p>Eigenvalue Decomposition is a way of factorizing a matrix into a product of its eigenvectors and eigenvalues. For an \\( n \times n \\) matrix \\( A \\) with \\( n \\) linearly independent eigenvectors, we can write the \\( n \\) eigenvector equations:</p>
\\[ A\bar{u}_1 = \lambda_1\bar{u}_1 \\]
\\[ A\bar{u}_2 = \lambda_2\bar{u}_2 \\]
\\[ \vdots \\]
\\[ A\bar{u}_n = \lambda_n\bar{u}_n \\]

<p>We can combine these into a single matrix equation. Let \\( U \\) be the matrix whose columns are the eigenvectors \\( \bar{u}_i \\), and let \\( \Lambda \\) be a diagonal matrix with the corresponding eigenvalues \\( \lambda_i \\) on its diagonal.</p>
\\[ A \begin{pmatrix} | & | & & | \\ \bar{u}_1 & \bar{u}_2 & \cdots & \bar{u}_n \\ | & | & & | \end{pmatrix} = \begin{pmatrix} | & | & & | \\ \lambda_1\bar{u}_1 & \lambda_2\bar{u}_2 & \cdots & \lambda_n\bar{u}_n \\ | & | & & | \end{pmatrix} \\]
<p>The right side can be rewritten as:</p>
\\[ \begin{pmatrix} | & | & & | \\ \bar{u}_1 & \bar{u}_2 & \cdots & \bar{u}_n \\ | & | & & | \end{pmatrix} \begin{pmatrix} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n \end{pmatrix} \\]
<p>This gives us the compact form \\( AU = U\Lambda \\). If the eigenvectors are linearly independent, the matrix \\( U \\) is invertible. We can right-multiply by \\( U^{-1} \\) to get the final EVD formula:</p>
\\[ A = U \Lambda U^{-1} \\]
<p>This is the <b>Eigenvalue Decomposition</b> of \\( A \\). It expresses the matrix \\( A \\) in terms of its fundamental components: its eigenvectors (in \\( U \\)) and its eigenvalues (in \\( \Lambda \\)). It is a convention, and often useful in applications, to arrange the eigenvalues in \\( \Lambda \\) in decreasing order of magnitude, with the corresponding eigenvectors arranged in the same order in \\( U \\).</p>

<b>6. EVD Example (Continued)</b>
<p>Using our previous results for \\( A = \begin{pmatrix} 2 & 2 \\ 3 & 1 \end{pmatrix} \\):</p>
<ul>
    <li>Eigenvalues: \\( \lambda_1 = 4, \lambda_2 = -1 \\)</li>
    <li>Eigenvectors: \\( \bar{u}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \bar{u}_2 = \begin{pmatrix} 2 \\ -3 \end{pmatrix} \\) (using a different scaling for demonstration).</li>
</ul>
<p>The matrices for the decomposition are:</p>
<p><b>Matrix of Eigenvectors, \\( U \\)</b>:</p>
\\[ U = \begin{pmatrix} 1 & 2 \\ 1 & -3 \end{pmatrix} \\]
<p><b>Diagonal Matrix of Eigenvalues, \\( \Lambda \\)</b>:</p>
\\[ \Lambda = \begin{pmatrix} 4 & 0 \\ 0 & -1 \end{pmatrix} \\]
<p><b>Inverse of Eigenvector Matrix, \\( U^{-1} \\)</b>:</p>
\\[ U^{-1} = \frac{1}{(1)(-3) - (2)(1)} \begin{pmatrix} -3 & -2 \\ -1 & 1 \end{pmatrix} = -\frac{1}{5} \begin{pmatrix} -3 & -2 \\ -1 & 1 \end{pmatrix} = \frac{1}{5} \begin{pmatrix} 3 & 2 \\ 1 & -1 \end{pmatrix} \\]
<p>Thus, the eigenvalue decomposition of \\( A \\) is:</p>
\\[ A = U \Lambda U^{-1} = \begin{pmatrix} 1 & 2 \\ 1 & -3 \end{pmatrix} \begin{pmatrix} 4 & 0 \\ 0 & -1 \end{pmatrix} \left( \frac{1}{5} \begin{pmatrix} 3 & 2 \\ 1 & -1 \end{pmatrix} \right) \\]
<p>Multiplying these matrices will verify that the result is the original matrix \\( A \\).</p>
</div></div><div class="chapter" id="Lecture 16| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 16| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on two special classes of matrices: <b>Rotation matrices</b> and <b>Unitary matrices</b>. These matrices have significant applications in various fields, including linear algebra, physics, and modern wireless communications.</p>

<h3>1. Rotation Matrix</h3>
<p>A rotation matrix is a real-valued matrix that performs a rotation in Euclidean space. The transcript introduces the standard 2x2 rotation matrix, which rotates a coordinate system in a two-dimensional plane.</p>

<b>Definition and Formula:</b>
<p>The 2x2 rotation matrix, denoted as \\(R(\theta)\\), is parameterized by a single variable \\(\theta\\), which represents the angle of rotation. Its standard form is given by:</p>
\\[ R(\theta) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \\]
<p>This matrix corresponds to a rotation of the coordinate system <b>anti-clockwise</b> by an angle \\(\theta\\). If you have a point with coordinates \\(\bar{x}\\) in the original system, its new coordinates \\(\bar{x}'\\) after the rotation are given by \\(\bar{x}' = R(\theta) \bar{x}\\).</p>

<b>Key Property: Orthogonality</b>
<p>Rotation matrices have a crucial property: their transpose is also their inverse. This is expressed by the formula:</p>
\\[ R^T R = R R^T = I \\]
<p>where \\(R^T\\) is the transpose of \\(R\\) and \\(I\\) is the identity matrix. This relationship implies that \\(R^T = R^{-1}\\). Matrices with this property are known as <b>orthogonal matrices</b>. A rotation matrix is a specific type of orthogonal matrix.</p>

<b>Verification of the Property:</b>
<p>We can verify this property by direct matrix multiplication. Let's compute \\(R^T R\\):</p>
<p>First, the transpose of \\(R(\theta)\\) is:</p>
\\[ R^T = \begin{pmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{pmatrix} \\]
<p>Now, multiplying \\(R^T\\) by \\(R\\):</p>
\\[ R^T R = \begin{pmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{pmatrix} \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \\]
\\[ = \begin{pmatrix} (\cos\theta)(\cos\theta) + (\sin\theta)(\sin\theta) & (\cos\theta)(-\sin\theta) + (\sin\theta)(\cos\theta) \\ (-\sin\theta)(\cos\theta) + (\cos\theta)(\sin\theta) & (-\sin\theta)(-\sin\theta) + (\cos\theta)(\cos\theta) \end{pmatrix} \\]
\\[ = \begin{pmatrix} \cos^2\theta + \sin^2\theta & -\cos\theta\sin\theta + \sin\theta\cos\theta \\ -\sin\theta\cos\theta + \cos\theta\sin\theta & \sin^2\theta + \cos^2\theta \end{pmatrix} \\]
<p>Using the fundamental trigonometric identity \\(\cos^2\theta + \sin^2\theta = 1\\), the matrix simplifies to:</p>
\\[ R^T R = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = I \\]
<p>This confirms that \\(R^T\\) is the inverse of \\(R\\).</p>

<h3>2. Unitary Matrix</h3>
<p>A unitary matrix is the complex analog of a rotation (or more generally, an orthogonal) matrix. It can be loosely thought of as a "complex rotation" matrix.</p>

<b>Definition and Key Property:</b>
<p>A square complex matrix \\(U\\) is unitary if its conjugate transpose is also its inverse. The conjugate transpose, or <b>Hermitian transpose</b>, is denoted by \\(U^H\\). The defining property is:</p>
\\[ UU^H = U^H U = I \\]
<p>This implies \\(U^H = U^{-1}\\).</p>

<b>Example of a 2x2 Unitary Matrix:</b>
<p>The transcript provides an example of a 2x2 unitary matrix that depends on three parameters: \\(\phi_1, \phi_2, \theta\\). As noted in the transcript, the example presented contains typos that make the verification difficult. The matrix intended was likely a corrected form. Below is the verification based on the corrected version for clarity.</p>
<p>A correct form for a general 2x2 unitary matrix (belonging to the group SU(2), up to a determinant factor) is:</p>
\\[ U = \begin{pmatrix} e^{j\phi_1}\cos\theta & e^{j\phi_2}\sin\theta \\ -e^{-j\phi_2}\sin\theta & e^{-j\phi_1}\cos\theta \end{pmatrix} \\]
<p>Its Hermitian transpose is:</p>
\\[ U^H = \begin{pmatrix} e^{-j\phi_1}\cos\theta & -e^{j\phi_2}\sin\theta \\ e^{-j\phi_2}\sin\theta & e^{j\phi_1}\cos\theta \end{pmatrix} \\]
<p>Multiplying \\(U U^H\\) gives:</p>
<ul>
    <li><b>(1,1) element:</b> \\( (e^{j\phi_1}\cos\theta)(e^{-j\phi_1}\cos\theta) + (e^{j\phi_2}\sin\theta)(e^{-j\phi_2}\sin\theta) = \cos^2\theta + \sin^2\theta = 1 \\)</li>
    <li><b>(1,2) element:</b> \\( (e^{j\phi_1}\cos\theta)(-e^{j\phi_2}\sin\theta) + (e^{j\phi_2}\sin\theta)(e^{j\phi_1}\cos\theta) = -e^{j(\phi_1+\phi_2)}\cos\theta\sin\theta + e^{j(\phi_1+\phi_2)}\cos\theta\sin\theta = 0 \\)</li>
    <li><b>(2,1) element:</b> \\( (-e^{-j\phi_2}\sin\theta)(e^{-j\phi_1}\cos\theta) + (e^{-j\phi_1}\cos\theta)(e^{-j\phi_2}\sin\theta) = -e^{-j(\phi_1+\phi_2)}\sin\theta\cos\theta + e^{-j(\phi_1+\phi_2)}\sin\theta\cos\theta = 0 \\)</li>
    <li><b>(2,2) element:</b> \\( (-e^{-j\phi_2}\sin\theta)(-e^{j\phi_2}\sin\theta) + (e^{-j\phi_1}\cos\theta)(e^{j\phi_1}\cos\theta) = \sin^2\theta + \cos^2\theta = 1 \\)</li>
</ul>
<p>The result is indeed the identity matrix \\(I\\).</p>

<h3>3. Practical Application: The Alamouti Code</h3>
<p>A practical example where unitary matrices arise is in wireless communications, specifically in the <b>Alamouti code</b>. This code is used in systems with multiple antennas to improve reliability.</p>
<p>For a system with 2 transmit antennas and 1 receive antenna, the channel is described by coefficients \\(h_1\\) and \\(h_2\\). The effective channel matrix for the Alamouti code is:</p>
\\[ H_{eff} = \begin{pmatrix} h_1 & h_2 \\ h_2^* & -h_1^* \end{pmatrix} \\]
<p>where \\(h^*\\) denotes the complex conjugate.</p>
<p>This matrix is not unitary by itself. However, it can be normalized to create a unitary matrix \\(\tilde{H}\\):</p>
\\[ \tilde{H} = \frac{1}{||\bar{h}||} H_{eff} = \frac{1}{\sqrt{|h_1|^2 + |h_2|^2}} \begin{pmatrix} h_1 & h_2 \\ h_2^* & -h_1^* \end{pmatrix} \\]
<p>Here, \\(||\bar{h}|| = \sqrt{|h_1|^2 + |h_2|^2}\\) is the norm of the channel vector \\(\bar{h} = [h_1, h_2]^T\\). This normalized matrix \\(\tilde{H}\\) is unitary, satisfying \\(\tilde{H}^H \tilde{H} = I\\).</p>

<h3>4. Properties of Unitary Matrices</h3>
<p>Unitary matrices have several important and useful properties.</p>

<b>1. Determinant has Unit Magnitude:</b>
<p>For any unitary matrix \\(U\\), the magnitude of its determinant is 1.</p>
\\[ |\det(U)| = 1 \\]
<i>Proof:</i>
<p>Starting from \\(UU^H = I\\), we take the determinant of both sides:
\\(\det(UU^H) = \det(I) = 1\\).<br>
Using the property \\(\det(AB) = \det(A)\det(B)\\), we get \\(\det(U)\det(U^H) = 1\\).<br>
Using the property \\(\det(U^H) = \overline{\det(U)}\\) (the conjugate of the determinant), we have \\(\det(U)\overline{\det(U)} = 1\\).<br>
This is equivalent to \\(|\det(U)|^2 = 1\\), which implies \\(|\det(U)| = 1\\).</p>

<b>2. Eigenvalues have Unit Magnitude:</b>
<p>All eigenvalues \\(\lambda\\) of a unitary matrix have a magnitude of 1.</p>
\\[ |\lambda| = 1 \\]
<i>Proof:</i>
<p>Let \\(\lambda\\) be an eigenvalue of \\(U\\) with corresponding eigenvector \\(\bar{z}\\). The eigenvalue equation is \\(U\bar{z} = \lambda\bar{z}\\).<br>
Taking the Hermitian transpose of both sides gives \\(\bar{z}^H U^H = \bar{\lambda}\bar{z}^H\\).<br>
Multiplying the left-hand side of the second equation by the left-hand side of the first, and similarly for the right-hand sides, we get:
\\( (\bar{z}^H U^H)(U\bar{z}) = (\bar{\lambda}\bar{z}^H)(\lambda\bar{z}) \\)<br>
\\( \bar{z}^H (U^H U) \bar{z} = \bar{\lambda}\lambda(\bar{z}^H \bar{z}) \\)<br>
Since \\(U^H U = I\\) and \\(\bar{z}^H\bar{z} = ||\bar{z}||^2\\), this simplifies to:
\\( \bar{z}^H I \bar{z} = |\lambda|^2 ||\bar{z}||^2 \\)<br>
\\( ||\bar{z}||^2 = |\lambda|^2 ||\bar{z}||^2 \\)<br>
Since an eigenvector \\(\bar{z}\\) must be non-zero (\\(||\bar{z}||^2 \neq 0\\)), we can divide by \\(||\bar{z}||^2\\) to get \\(|\lambda|^2 = 1\\), which means \\(|\lambda| = 1\\).</p>

<b>3. Orthonormal Columns and Rows:</b>
<p>The columns (and rows) of a unitary matrix form an orthonormal set. This means each column has a unit norm, and any two distinct columns are orthogonal to each other.</p>
<p>If \\(\bar{u}_i\\) and \\(\bar{u}_j\\) are columns of \\(U\\), then:</p>
<ul>
    <li>\\( \bar{u}_i^H \bar{u}_i = ||\bar{u}_i||^2 = 1 \\) (Unit Norm)</li>
    <li>\\( \bar{u}_i^H \bar{u}_j = 0 \\) for \\( i \neq j \\) (Orthogonality)</li>
</ul>
<p>This comes directly from the condition \\(U^H U = I\\), where the \\((i, j)\\)-th entry of the resulting identity matrix is the inner product \\(\bar{u}_i^H \bar{u}_j\\).</p>

<b>4. Norm Preservation (Isometry):</b>
<p>Multiplying a vector by a unitary matrix preserves the vector's norm (length).</p>
<p>If \\(\tilde{x} = U\bar{x}\\), then \\( ||\tilde{x}|| = ||\bar{x}|| \\).</p>
<i>Proof:</i>
<p>\\( ||\tilde{x}||^2 = \tilde{x}^H \tilde{x} = (U\bar{x})^H (U\bar{x}) = (\bar{x}^H U^H)(U\bar{x}) \\)<br>
\\( = \bar{x}^H (U^H U) \bar{x} = \bar{x}^H I \bar{x} = \bar{x}^H \bar{x} = ||\bar{x}||^2 \\)<br>
Therefore, \\(||\tilde{x}|| = ||\bar{x}||\\).</p>

<b>5. Preservation of IID Gaussian Distribution:</b>
<p>If a random vector \\(\bar{x}\\) has components that are independent and identically distributed (i.i.d.) Gaussian random variables with mean 0 and variance \\(\sigma^2\\), its covariance matrix is \\(\sigma^2 I\\). When this vector is transformed by a unitary matrix \\(U\\) to get a new vector \\(\tilde{x} = U\bar{x}\\), the new vector \\(\tilde{x}\\) also has i.i.d. Gaussian components with the same mean and variance.</p>
<p><i>Proof Sketch:</i></p>
<ul>
    <li><b>Mean:</b> \\( E[\tilde{x}] = E[U\bar{x}] = U E[\bar{x}] = U \cdot 0 = 0 \\).</li>
    <li><b>Covariance:</b> \\( E[\tilde{x}\tilde{x}^H] = E[(U\bar{x})(U\bar{x})^H] = E[U\bar{x}\bar{x}^H U^H] \\)
    \\( = U E[\bar{x}\bar{x}^H] U^H = U(\sigma^2 I)U^H = \sigma^2(UU^H) = \sigma^2 I \\).</li>
</ul>
<p>Since the resulting covariance matrix is still a scaled identity matrix, the components of \\(\tilde{x}\\) are also uncorrelated (and thus independent, since they are Gaussian) and have the same variance \\(\sigma^2\\).</p>
</div></div><h2>Weekly Summary</h2><p>This week's lectures cover the Gram-Schmidt procedure for creating orthonormal bases, the properties and transformations of Gaussian random vectors, an application of these concepts in machine learning classification, the fundamental theory of eigenvalue decomposition, and special classes of matrices like rotation and unitary matrices.</p>

<p><b>Gram-Schmidt Orthonormalization</b></p>
<p>
The Gram-Schmidt procedure is a systematic method to convert any given basis \\(\\{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \\}\\) for a subspace into an <i>orthonormal basis</i> \\(\\{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \\}\\) for the same subspace. An orthonormal basis consists of vectors that are mutually orthogonal (\\(\mathbf{v}_i^H \mathbf{v}_j = 0\\) for \\(i \neq j\\)) and have a unit norm (\\(\\| \mathbf{v}_i \\| = 1\\)). The process is iterative:
</p>
<p>
1. The first orthonormal vector is found by normalizing the first original vector: \\(\mathbf{v}_1 = \frac{\mathbf{x}_1}{\\| \mathbf{x}_1 \\|}\\).<br>
2. For each subsequent vector \\(\mathbf{x}_i\\), its projections onto the previously found orthonormal vectors (\\(\mathbf{v}_1, \dots, \mathbf{v}_{i-1}\\)) are subtracted. This creates an intermediate vector \\(\tilde{\mathbf{v}}_i = \mathbf{x}_i - \sum_{j=1}^{i-1} (\mathbf{x}_i^H \mathbf{v}_j) \mathbf{v}_j\\) which is orthogonal to all preceding basis vectors.<br>
3. This intermediate vector is then normalized to have unit length: \\(\mathbf{v}_i = \frac{\tilde{\mathbf{v}}_i}{\\| \tilde{\mathbf{v}}_i \\|}\\).
</p>

<p><b>Gaussian Random Vectors</b></p>
<p>
A <i>multivariate Gaussian random vector</i> is a collection of jointly Gaussian random variables, fundamental for modeling phenomena like noise in signal processing and data in machine learning.
</p>
<p>
A Gaussian random vector \\(\mathbf{x}\\) is completely characterized by its mean vector \\(\boldsymbol{\mu} = E[\mathbf{x}]\\) and its covariance matrix \\(\mathbf{R} = E[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^H]\\). A key takeaway is the interpretation of the covariance matrix structure:
</p>
<p>
    • If \\(\mathbf{R}\\) is a diagonal matrix, the components of \\(\mathbf{x}\\) are uncorrelated. For Gaussian vectors, this also implies they are <i>independent</i>.<br>
    • If \\(\mathbf{R}\\) is proportional to the identity matrix (\\(\mathbf{R} = \sigma^2 \mathbf{I}\\)), the components are independent and identically distributed (i.i.d.), each with the same variance \\(\sigma^2\\).
</p>

<p><b>Linear Transformation of Gaussian Vectors</b></p>
<p>
A crucial property of Gaussian random vectors is that they remain Gaussian after a linear transformation. If \\(\mathbf{x}\\) is a Gaussian random vector with mean \\(\boldsymbol{\mu}_x\\) and covariance \\(\mathbf{R}_x\\), and it undergoes the transformation \\(\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{b}\\), then \\(\mathbf{y}\\) is also a Gaussian random vector with:
</p>
<p>
    • New mean: \\(\boldsymbol{\mu}_y = \mathbf{A}\boldsymbol{\mu}_x + \mathbf{b}\\)<br>
    • New covariance: \\(\mathbf{R}_y = \mathbf{A} \mathbf{R}_x \mathbf{A}^T\\)
</p>
<p>This property makes the analysis of linear systems with Gaussian inputs mathematically tractable.</p>

<p><b>Application: Classification with Gaussian Classes</b></p>
<p>
This topic demonstrates a practical machine learning application. In a binary classification problem where data from two classes are modeled by Gaussian distributions with different means (\\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2\\)) but an identical covariance matrix (\\(\boldsymbol{\Sigma}\\)), the optimal decision rule is a <i>Maximum Likelihood classifier</i>. This involves assigning a new data point \\(\mathbf{x}\\) to the class whose probability density function yields a higher value at \\(\mathbf{x}\\).
</p>
<p>
<b>Key Takeaways:</b><br>
    • The decision boundary for this classifier simplifies to a linear hyperplane.<br>
    • In the special case where the covariance matrix is proportional to identity (\\(\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\\)), the rule further simplifies to a <i>minimum distance classifier</i>. The data point is assigned to the class corresponding to the closer mean. The decision boundary becomes the perpendicular bisector of the line segment connecting the two means.
</p>

<p><b>Eigenvalues and Eigenvalue Decomposition (EVD)</b></p>
<p>
Eigenvalues and eigenvectors are fundamental properties of a square matrix \\(\mathbf{A}\\). An eigenvector \\(\mathbf{u}\\) is a non-zero vector whose direction is unchanged when multiplied by \\(\mathbf{A}\\); it is only scaled by a factor \\(\lambda\\), the corresponding eigenvalue. This relationship is defined by the equation:</p>
\\[ \mathbf{A}\mathbf{u} = \lambda\mathbf{u} \\]
<p>Eigenvalues are found by solving the <i>characteristic equation</i>, \\(\det(\mathbf{A} - \lambda\mathbf{I}) = 0\\). For each eigenvalue, the corresponding eigenvectors are found by determining the null space of \\((\mathbf{A} - \lambda\mathbf{I})\\).</p>
<p>The <i>Eigenvalue Decomposition (EVD)</i> expresses a matrix \\(\mathbf{A}\\) (if it has a full set of linearly independent eigenvectors) as:</p>
\\[ \mathbf{A} = \mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^{-1} \\]
<p>Here, \\(\mathbf{U}\\) is the matrix whose columns are the eigenvectors of \\(\mathbf{A}\\), and \\(\boldsymbol{\Lambda}\\) is a diagonal matrix containing the corresponding eigenvalues. EVD is a powerful tool that reveals the intrinsic structure of a linear transformation.</p>

<p><b>Rotation and Unitary Matrices</b></p>
<p>
These are special classes of square matrices representing transformations that preserve length and angles.
</p>
<p>
    • A real <i>rotation matrix</i> \\(\mathbf{R}\\) is an orthogonal matrix, satisfying \\(\mathbf{R}^T \mathbf{R} = \mathbf{I}\\), which implies \\(\mathbf{R}^{-1} = \mathbf{R}^T\\).<br>
    • A complex <i>unitary matrix</i> \\(\mathbf{U}\\) is the complex analog, satisfying \\(\mathbf{U}^H \mathbf{U} = \mathbf{I}\\), which implies \\(\mathbf{U}^{-1} = \mathbf{U}^H\\).
</p>
<p>
<b>Key Properties of Unitary Matrices:</b><br>
    • Their columns (and rows) form an orthonormal set.<br>
    • They preserve the norm of any vector: \\(\\| \mathbf{U}\mathbf{x} \\| = \\| \mathbf{x} \\|\\).<br>
    • The magnitude of their determinant is 1 (\\(|\det(\mathbf{U})|=1\\)).<br>
    • All their eigenvalues have a magnitude of 1 (\\(|\lambda|=1\\)).<br>
    • They arise frequently in signal processing (e.g., Alamouti codes) and advanced matrix decompositions.
</p><h2>Assignment Explanation</h2><div>
<p>Here is an explanation of the questions and answers from the assignment.</p>

<p><b>Question 1: Vectors \\(\bar{a}_1, \bar{a}_2, \dots, \bar{a}_n\\) are linearly independent when...</b></p>
<p><b>Explanation:</b> This question asks for the formal definition of linear independence. A set of vectors is called linearly independent if the only way to make their linear combination equal to the zero vector is by choosing all scalar coefficients to be zero. The linear combination is given by \\(\alpha_1\bar{a}_1 + \alpha_2\bar{a}_2 + \dots + \alpha_n\bar{a}_n\\). The condition for this sum to be zero \\(\textit{if and only if}\\) all coefficients (\\(\alpha_1, \alpha_2, \dots, \alpha_n\\)) are zero is the definition of linear independence. The symbol \\(\Leftrightarrow\\) means "if and only if".</p>
<p>The correct answer is: \\(\alpha_1\bar{a}_1 + \alpha_2\bar{a}_2 + \dots + \alpha_n\bar{a}_n = 0 \Leftrightarrow \alpha_1 = \alpha_2 = \dots = 0\\).</p>

<p><b>Question 2: Consider the matrix \\(A\\) given below. The cofactor \\(C_{2,1}\\) is given as...</b></p>
<p><b>Explanation:</b> This question asks to calculate a specific cofactor of the given matrix. The cofactor \\(C_{i,j}\\) is defined as \\(C_{i,j} = (-1)^{i+j} M_{i,j}\\), where \\(M_{i,j}\\) is the minor. The minor is the determinant of the submatrix formed by removing the \\(i\\)-th row and \\(j\\)-th column.</p>
<p>Given the matrix:
\\[ A = \begin{bmatrix} 3 & 6 & -2 \\ -2 & -1 & 4 \\ 1 & -5 & -3 \end{bmatrix} \\]
To find \\(C_{2,1}\\), we have \\(i=2\\) and \\(j=1\\).
<ol>
    <li>First, find the minor \\(M_{2,1}\\) by removing the 2nd row and 1st column:
    \\[ M_{2,1} = \det \begin{pmatrix} 6 & -2 \\ -5 & -3 \end{pmatrix} = (6)(-3) - (-2)(-5) = -18 - 10 = -28 \\]
    </li>
    <li>Next, calculate the sign: \\((-1)^{2+1} = (-1)^3 = -1\\).</li>
    <li>Finally, the cofactor is \\(C_{2,1} = (-1) \times M_{2,1} = (-1) \times (-28) = 28\\).</li>
</ol>
<b>Note:</b> There appears to be an error in the question's options and accepted answer. The correct calculated value for the cofactor is 28, but this is not listed as an option. The system has an incorrect answer marked as correct.</p>

<p><b>Question 3: Consider the directed graph below. The adjacency matrix for this graph is given as...</b></p>
<p><b>Explanation:</b> This question asks for the adjacency matrix of a given directed graph. For a graph with \\(n\\) nodes, the adjacency matrix \\(A\\) is an \\(n \times n\\) matrix where the element \\(A_{ij}\\) is 1 if there is a directed edge from node \\(i\\) to node \\(j\\), and 0 otherwise. Based on the graph (and confirmed by the correct answer), the connections are:
<ul>
    <li>From Node 1: No outgoing edges. (Row 1 is all zeros)</li>
    <li>From Node 2: Edges to Node 3 and Node 5. (Row 2 has 1s in columns 3 and 5)</li>
    <li>From Node 3: Edge to Node 1. (Row 3 has a 1 in column 1)</li>
    <li>From Node 4: Edges to Node 2 and Node 3. (Row 4 has 1s in columns 2 and 3)</li>
    <li>From Node 5: Edges to Node 1 and Node 4. (Row 5 has 1s in columns 1 and 4)</li>
</ul>
Translating this into a matrix gives the accepted answer:
\\[ \begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 & 0 \\ 1 & 0 & 0 & 1 & 0 \end{bmatrix} \\]</p>

<p><b>Question 4: Consider the matrix \\(A\\) given below. The minor \\(M_{1,3}\\) is given as...</b></p>
<p><b>Explanation:</b> This question asks to calculate a specific minor of the matrix. The minor \\(M_{i,j}\\) is the determinant of the submatrix formed by deleting the \\(i\\)-th row and \\(j\\)-th column.</p>
<p>Given the matrix:
\\[ A = \begin{bmatrix} 1 & -4 & -5 \\ -2 & 2 & 3 \\ 5 & -3 & -2 \end{bmatrix} \\]
To find \\(M_{1,3}\\), we remove the 1st row and 3rd column:
\\[ M_{1,3} = \det \begin{pmatrix} -2 & 2 \\ 5 & -3 \end{pmatrix} = (-2)(-3) - (2)(5) = 6 - 10 = -4 \\]
<b>Note:</b> Similar to Question 2, there seems to be an error in the provided options and accepted answer. The correct calculation for the minor \\(M_{1,3}\\) is -4. The system incorrectly marks -2 as the correct answer.</p>

<p><b>Question 5: Vectors \\(\bar{a}_1, \bar{a}_2, \dots, \bar{a}_n\\) are linearly dependent when...</b></p>
<p><b>Explanation:</b> This question asks for the definition of linear dependence. A set of vectors is linearly dependent if their linear combination can be made equal to the zero vector using scalar coefficients that are <b>not all zero</b>. This means there is a non-trivial way to combine the vectors to get zero, implying that at least one vector can be expressed as a linear combination of the others.</p>
<p>The correct answer is: There exist \\(\alpha_1, \alpha_2, \dots, \alpha_n\\), not all zero, such that \\(\alpha_1\bar{a}_1 + \alpha_2\bar{a}_2 + \dots + \alpha_n\bar{a}_n = 0\\).</p>

<p><b>Question 6: Consider the traffic flow problem given below. The values of the traffic flows \\(x_1\\) and \\(x_2\\) are...</b></p>
<p><b>Explanation:</b> This question requires setting up and solving a system of linear equations based on the principle of conservation of flow, which states that the total flow into an intersection must equal the total flow out of it.
<ul>
    <li><b>Top Intersection:</b> Flow in = \\(50 + x_2\\). Flow out = \\(20 + x_1\\).
    <br>Equation: \\(50 + x_2 = 20 + x_1 \Rightarrow x_1 - x_2 = 30\\).</li>
    <li><b>Bottom Intersection:</b> Flow in = \\(x_1 + 40\\). Flow out = \\(x_2 + 80\\).
    <br>Equation: \\(x_1 + 40 = x_2 + 80 \Rightarrow x_1 - x_2 = 40\\).</li>
</ul>
We have a system of two equations:
\\[ x_1 - x_2 = 30 \\]
\\[ x_1 - x_2 = 40 \\]
This system is inconsistent because \\(x_1 - x_2\\) cannot be equal to both 30 and 40 simultaneously.
<br><b>Note:</b> The problem as stated in the diagram has no solution. There is an error in the problem's values, making it impossible to solve.</p>

<p><b>Question 7: Consider the \\(r \times t\\) MIMO channel matrix \\(H\\). The \\((i, j)\\) element of \\(H\\), represented by \\(h_{i,j}\\), denotes the channel between...</b></p>
<p><b>Explanation:</b> MIMO stands for Multiple-Input Multiple-Output, a technology used in wireless communication. The channel matrix \\(H\\) models the physical paths between the transmitter and receiver antennas. By convention, a matrix of size \\(r \times t\\) models a system with \\(t\\) transmit antennas and \\(r\\) receive antennas. The element \\(h_{i,j}\\) (in the \\(i\\)-th row and \\(j\\)-th column) represents the gain and phase shift of the channel from the \\(j\\)-th transmit antenna to the \\(i\\)-th receive antenna.</p>
<p>The correct answer is: Receive antenna \\(i\\) and transmit antenna \\(j\\).</p>

<p><b>Question 8: Consider the circuit shown below. The equation for loop 1 is given as...</b></p>
<p><b>Explanation:</b> This question asks to apply Kirchhoff's Voltage Law (KVL) to the first loop of the circuit. KVL states that the sum of voltage rises (from sources) in a closed loop equals the sum of voltage drops (across resistors).
<ul>
    <li>Loop 1 contains the 8V source, a 5Ω resistor, and a 4Ω resistor.</li>
    <li>The voltage source provides a rise of 8V.</li>
    <li>The voltage drop across the 5Ω resistor is \\(5 \times i_1\\).</li>
    <li>The 4Ω resistor is shared between loop 1 and loop 2. The net current flowing down through it (in the direction of \\(i_1\\)) is \\(i_1 - i_2\\). The voltage drop is \\(4 \times (i_1 - i_2)\\).</li>
</ul>
Applying KVL:
\\[ \text{Voltage Rises} = \text{Voltage Drops} \\]
\\[ 8 = 5i_1 + 4(i_1 - i_2) \\]
\\[ 8 = 5i_1 + 4i_1 - 4i_2 \\]
\\[ 8 = 9i_1 - 4i_2 \\]
<b>Note:</b> The equation derived using KVL is \\(9i_1 - 4i_2 = 8\\). None of the options match this correct equation. The provided accepted answer of \\(5i_1 - 4i_2 = -8\\) is incorrect, suggesting an error in the question itself.</p>

<p><b>Question 9: Matrix that is NOT invertible is termed a...</b></p>
<p><b>Explanation:</b> This is a vocabulary question from linear algebra. A square matrix that does not have a multiplicative inverse is called a <b>singular matrix</b>. A non-singular matrix is another term for an invertible matrix. A key property of a singular matrix is that its determinant is zero.</p>
<p>The correct answer is: Singular matrix.</p>

<p><b>Question 10: MIMO technology in 4G/ 5G systems relies on which of the principles below for high data rate transmission?</b></p>
<p><b>Explanation:</b> The primary principle that allows MIMO systems to achieve very high data rates is <b>spatial multiplexing</b>. This technique uses multiple antennas at both the transmitter and receiver to send multiple, independent data streams simultaneously over the same frequency band. By exploiting the different spatial paths between the antennas, the receiver can separate these streams, effectively multiplying the data rate without requiring more bandwidth or transmission power.</p>
<p>The correct answer is: Spatial multiplexing.</p>
</div></div><div class="week" id="week_4"><h1 class="week-title">Week 4</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 17 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 17 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts surrounding Positive Semi-Definite (PSD) and Positive Definite (PD) matrices as presented in the transcript.</p>

<b>1. Definition of Positive Semi-Definite (PSD) and Positive Definite (PD) Matrices</b>

<p>The transcript introduces a special class of square, symmetric matrices known as PSD and PD matrices. The core idea is to define a matrix's "positivity" through a specific quadratic form.</p>

<p>A matrix \\( \mathbf{A} \\) is required to be <b>symmetric</b> (for real matrices) or <b>Hermitian</b> (for complex matrices). This means:</p>
<ul>
    <li>For a real matrix: \\( \mathbf{A} = \mathbf{A}^T \\) (where \\( T \\) denotes the transpose).</li>
    <li>For a complex matrix: \\( \mathbf{A} = \mathbf{A}^H \\) (where \\( H \\) denotes the Hermitian transpose, i.e., conjugate transpose). This property is also called Hermitian symmetry.</li>
</ul>

<p>With this prerequisite, the definitions are as follows:</p>

<p><b>For Real Matrices:</b></p>
<ul>
    <li>A matrix \\( \mathbf{A} \\) is <b>Positive Semi-Definite (PSD)</b> if the quadratic form \\( \mathbf{x}^T \mathbf{A} \mathbf{x} \\) is non-negative for any non-zero real vector \\( \mathbf{x} \\).
    \\[ \mathbf{x}^T \mathbf{A} \mathbf{x} \ge 0 \quad \text{for all } \mathbf{x} \\]
    </li>
    <li>A matrix \\( \mathbf{A} \\) is <b>Positive Definite (PD)</b> if the quadratic form is strictly positive for any non-zero real vector \\( \mathbf{x} \\).
    \\[ \mathbf{x}^T \mathbf{A} \mathbf{x} > 0 \quad \text{for all } \mathbf{x} \neq \mathbf{0} \\]
    </li>
</ul>

<p><b>For Complex Matrices:</b></p>
<ul>
    <li>A matrix \\( \mathbf{A} \\) is <b>Positive Semi-Definite (PSD)</b> if:
    \\[ \mathbf{x}^H \mathbf{A} \mathbf{x} \ge 0 \quad \text{for all } \mathbf{x} \\]
    </li>
    <li>A matrix \\( \mathbf{A} \\) is <b>Positive Definite (PD)</b> if:
    \\[ \mathbf{x}^H \mathbf{A} \mathbf{x} > 0 \quad \text{for all } \mathbf{x} \neq \mathbf{0} \\]
    </li>
</ul>

<br>
<b>2. Properties of PSD Matrices</b>

<p>The transcript details several fundamental properties that arise from the definition of PSD matrices. The proofs are shown for the more general complex case (using the Hermitian transpose), which also covers the real case.</p>

<b>Property 1: The Eigenvalues of a PSD Matrix are Real.</b>
<p>To prove this, consider an eigenvector \\(\mathbf{u}\\) and its corresponding eigenvalue \\(\lambda\\) for a PSD matrix \\(\mathbf{A}\\). The eigenvalue equation is:</p>
\\[ \mathbf{A}\mathbf{u} = \lambda\mathbf{u} \\]
<p><b>Step 1:</b> Pre-multiply by \\(\mathbf{u}^H\\):</p>
\\[ \mathbf{u}^H \mathbf{A}\mathbf{u} = \mathbf{u}^H (\lambda\mathbf{u}) = \lambda \mathbf{u}^H \mathbf{u} = \lambda \|\mathbf{u}\|^2 \\]
<p>This gives us our first key equation:</p>
\\[ \mathbf{u}^H \mathbf{A}\mathbf{u} = \lambda \|\mathbf{u}\|^2 \quad \text{(Equation 1)} \\]
<p><b>Step 2:</b> Take the Hermitian (conjugate transpose) of Equation 1. Since the expression is a scalar, the Hermitian is simply its complex conjugate.</p>
\\[ (\mathbf{u}^H \mathbf{A}\mathbf{u})^H = (\lambda \|\mathbf{u}\|^2)^H \\]
\\[ \mathbf{u}^H \mathbf{A}^H (\mathbf{u}^H)^H = \lambda^* (\|\mathbf{u}\|^2)^H \\]
<p>Using the properties \\((\mathbf{u}^H)^H = \mathbf{u}\\) and that the squared norm \\(\|\mathbf{u}\|^2\\) is a real scalar, this simplifies to:</p>
\\[ \mathbf{u}^H \mathbf{A}^H \mathbf{u} = \lambda^* \|\mathbf{u}\|^2 \\]
<p><b>Step 3:</b> Since \\(\mathbf{A}\\) is a PSD matrix, it is Hermitian by definition (\\(\mathbf{A} = \mathbf{A}^H\\)).</p>
\\[ \mathbf{u}^H \mathbf{A}\mathbf{u} = \lambda^* \|\mathbf{u}\|^2 \quad \text{(Equation 2)} \\]
<p><b>Step 4:</b> Comparing Equation 1 and Equation 2, we see that:</p>
\\[ \lambda \|\mathbf{u}\|^2 = \lambda^* \|\mathbf{u}\|^2 \\]
<p>Since \\(\mathbf{u}\\) is an eigenvector, it is non-zero, so \\(\|\mathbf{u}\|^2 > 0\\). We can divide by it to get:</p>
\\[ \lambda = \lambda^* \\]
<p>A number that equals its own complex conjugate must be a real number. Thus, the eigenvalues of a PSD matrix are real.</p>

<b>Property 2: The Eigenvalues of a PSD Matrix are Non-negative.</b>
<p>This property follows directly from the definition of a PSD matrix and the result from Property 1.</p>
<p>From the eigenvalue equation, we derived:</p>
\\[ \mathbf{u}^H \mathbf{A}\mathbf{u} = \lambda \|\mathbf{u}\|^2 \\]
<p>By the definition of a PSD matrix, the left-hand side is non-negative:</p>
\\[ \mathbf{u}^H \mathbf{A}\mathbf{u} \ge 0 \\]
<p>Therefore, the right-hand side must also be non-negative:</p>
\\[ \lambda \|\mathbf{u}\|^2 \ge 0 \\]
<p>Since \\(\|\mathbf{u}\|^2\\) is strictly positive for an eigenvector, the eigenvalue \\(\lambda\\) must be non-negative:</p>
\\[ \lambda \ge 0 \\]
<p><b>Conclusion:</b> The eigenvalues of a PSD matrix are real and greater than or equal to zero. For a <b>PD matrix</b>, the inequality is strict (\\( \mathbf{u}^H \mathbf{A}\mathbf{u} > 0 \\)), which means its eigenvalues are strictly positive (\\(\lambda > 0\\)).</p>

<b>Property 3: Eigenvectors Corresponding to Distinct Eigenvalues are Orthogonal.</b>
<p>Let \\((\lambda_1, \mathbf{u}_1)\\) and \\((\lambda_2, \mathbf{u}_2)\\) be two eigenpairs of a PSD matrix \\(\mathbf{A}\\), with \\(\lambda_1 \neq \lambda_2\\).</p>
\\[ \mathbf{A}\mathbf{u}_1 = \lambda_1\mathbf{u}_1 \quad \text{(Eq. a)} \\]
\\[ \mathbf{A}\mathbf{u}_2 = \lambda_2\mathbf{u}_2 \quad \text{(Eq. b)} \\]
<p><b>Step 1:</b> Pre-multiply Eq. (a) by \\(\mathbf{u}_2^H\\):</p>
\\[ \mathbf{u}_2^H \mathbf{A}\mathbf{u}_1 = \mathbf{u}_2^H (\lambda_1\mathbf{u}_1) = \lambda_1 (\mathbf{u}_2^H \mathbf{u}_1) \quad \text{(Eq. 3)} \\]
<p><b>Step 2:</b> Pre-multiply Eq. (b) by \\(\mathbf{u}_1^H\\) and take the Hermitian of the entire equation:</p>
\\[ (\mathbf{u}_1^H \mathbf{A}\mathbf{u}_2)^H = (\lambda_2 \mathbf{u}_1^H \mathbf{u}_2)^H \\]
\\[ \mathbf{u}_2^H \mathbf{A}^H \mathbf{u}_1 = \lambda_2^* (\mathbf{u}_2^H \mathbf{u}_1) \\]
<p>Since \\(\mathbf{A} = \mathbf{A}^H\\) (Hermitian) and \\(\lambda_2 = \lambda_2^*\\) (eigenvalues are real), this becomes:</p>
\\[ \mathbf{u}_2^H \mathbf{A}\mathbf{u}_1 = \lambda_2 (\mathbf{u}_2^H \mathbf{u}_1) \quad \text{(Eq. 4)} \\]
<p><b>Step 3:</b> The left-hand sides of Eq. 3 and Eq. 4 are identical. Equating their right-hand sides gives:</p>
\\[ \lambda_1 (\mathbf{u}_2^H \mathbf{u}_1) = \lambda_2 (\mathbf{u}_2^H \mathbf{u}_1) \\]
<p>Rearranging the terms:</p>
\\[ (\lambda_1 - \lambda_2) (\mathbf{u}_2^H \mathbf{u}_1) = 0 \\]
<p>Since we assumed the eigenvalues are distinct (\\(\lambda_1 \neq \lambda_2\\)), the term \\((\lambda_1 - \lambda_2)\\) is non-zero. Therefore, the other term must be zero:</p>
\\[ \mathbf{u}_2^H \mathbf{u}_1 = 0 \\]
<p>This proves that the eigenvectors \\(\mathbf{u}_1\\) and \\(\mathbf{u}_2\\) are orthogonal.</p>

<br>
<b>3. Eigenvalue Decomposition (EVD) for PSD Matrices</b>

<p>The properties above lead to a special form of the Eigenvalue Decomposition (EVD) for PSD matrices.</p>
<ol>
    <li>The eigenvectors corresponding to distinct eigenvalues are orthogonal. Even if eigenvalues are repeated, a full set of orthogonal eigenvectors can be constructed.</li>
    <li>Each eigenvector can be normalized to have a unit norm (\\(\|\mathbf{u}_i\|=1\\)).</li>
</ol>
<p>Combining these two points, we can form an <b>orthonormal set of eigenvectors</b> \\(\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}\\) for an \\(n \times n\\) PSD matrix \\(\mathbf{A}\\).</p>
<p>Let \\(\mathbf{U}\\) be the matrix whose columns are these orthonormal eigenvectors, and \\(\mathbf{\Lambda}\\) be the diagonal matrix of the corresponding real, non-negative eigenvalues.</p>
\\[ \mathbf{U} = [\mathbf{u}_1 | \mathbf{u}_2 | \dots | \mathbf{u}_n] \quad , \quad \mathbf{\Lambda} = \begin{pmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix} \\]
<p>Because the columns of \\(\mathbf{U}\\) are orthonormal, \\(\mathbf{U}\\) is a <b>unitary matrix</b>. A key property of a unitary matrix is that its inverse is equal to its Hermitian transpose:</p>
\\[ \mathbf{U}^{-1} = \mathbf{U}^H \\]
<p>The general EVD for any diagonalizable matrix is \\(\mathbf{A} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^{-1}\\). For a PSD matrix, this simplifies to:</p>
\\[ \mathbf{A} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^H \\]
<p>This is the special form of the EVD for PSD (and PD) matrices.</p>

<br>
<b>4. Cholesky Factorization</b>

<p>The transcript presents a decomposition derived from the EVD, which is related to the Cholesky factorization. This decomposition expresses a PSD matrix as the product of another matrix and its Hermitian transpose.</p>

<p><b>Step 1:</b> Start with the EVD of the PSD matrix \\(\mathbf{A}\\):</p>
\\[ \mathbf{A} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^H \\]
<p><b>Step 2:</b> Since all eigenvalues \\(\lambda_i\\) are non-negative, we can define a matrix \\(\mathbf{\Lambda}^{1/2}\\) as the diagonal matrix containing the square roots of the eigenvalues:</p>
\\[ \mathbf{\Lambda}^{1/2} = \begin{pmatrix} \sqrt{\lambda_1} & & 0 \\ & \ddots & \\ 0 & & \sqrt{\lambda_n} \end{pmatrix} \\]
<p>The matrix \\(\mathbf{\Lambda}\\) can be written as:</p>
\\[ \mathbf{\Lambda} = \mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2} \\]
<p><b>Step 3:</b> Substitute this back into the EVD equation:</p>
\\[ \mathbf{A} = \mathbf{U} (\mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2}) \mathbf{U}^H \\]
<p><b>Step 4:</b> Group the terms as follows. Since \\(\mathbf{\Lambda}^{1/2}\\) is a real diagonal matrix, it is its own Hermitian (\\((\mathbf{\Lambda}^{1/2})^H = \mathbf{\Lambda}^{1/2}\\)).</p>
\\[ \mathbf{A} = (\mathbf{U} \mathbf{\Lambda}^{1/2}) (\mathbf{\Lambda}^{1/2} \mathbf{U}^H) = (\mathbf{U} \mathbf{\Lambda}^{1/2}) (\mathbf{\Lambda}^{1/2})^H \mathbf{U}^H \\]
<p>Using the property \\((\mathbf{B}\mathbf{C})^H = \mathbf{C}^H \mathbf{B}^H\\), we can write this as:</p>
\\[ \mathbf{A} = (\mathbf{U} \mathbf{\Lambda}^{1/2}) (\mathbf{U} \mathbf{\Lambda}^{1/2})^H \\]
<p>If we define a new matrix \\(\tilde{\mathbf{A}} = \mathbf{U} \mathbf{\Lambda}^{1/2}\\), then any PSD matrix \\(\mathbf{A}\\) can be decomposed as:</p>
\\[ \mathbf{A} = \tilde{\mathbf{A}} \tilde{\mathbf{A}}^H \\]
<p>This result is called the <b>Cholesky factorization</b>. It is conceptually similar to writing a non-negative real number \\(x\\) as the product of its square root with itself (\\(x = \sqrt{x} \cdot \sqrt{x}\\)). For matrices, this becomes the product of a matrix and its Hermitian transpose. This factorization is widely used in numerical linear algebra, statistics, and machine learning.</p>
</div></div><div class="chapter" id="Lecture 18 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 18 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts from the transcript, focusing on Positive Semi-Definite (PSD) and Positive Definite (PD) matrices, their properties, and their application in the context of covariance matrices for random vectors.</p>

<h3>1. Positive Semi-Definite (PSD) and Positive Definite (PD) Matrices</h3>
<p>The transcript begins by defining the terms PSD (Positive Semi-Definite) and PD (Positive Definite) and then tests a specific matrix to see which category it falls into.</p>
<p>A symmetric matrix \\(\mathbf{A}\\) is:</p>
<ul>
    <li><b>Positive Semi-Definite (PSD)</b> if the quadratic form \\(\mathbf{x}^T \mathbf{A} \mathbf{x} \ge 0\\) for every non-zero vector \\(\mathbf{x}\\). The equality \\(\mathbf{x}^T \mathbf{A} \mathbf{x} = 0\\) is allowed for some non-zero \\(\mathbf{x}\\).</li>
    <li><b>Positive Definite (PD)</b> if the quadratic form \\(\mathbf{x}^T \mathbf{A} \mathbf{x} > 0\\) for every non-zero vector \\(\mathbf{x}\\).</li>
</ul>

<h4>Example: Testing a 2x2 Matrix</h4>
<p>The transcript analyzes the matrix:</p>
\\[ \mathbf{A} = \begin{pmatrix} 1 & 3 \\ 3 & 9 \end{pmatrix} \\]
<p>To determine if \\(\mathbf{A}\\) is PSD or PD, we compute the quadratic form \\(\mathbf{x}^T \mathbf{A} \mathbf{x}\\) for an arbitrary 2-dimensional vector \\(\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\\).</p>
\\[ \mathbf{x}^T \mathbf{A} \mathbf{x} = \begin{pmatrix} x_1 & x_2 \end{pmatrix} \begin{pmatrix} 1 & 3 \\ 3 & 9 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \\]
<p>First, multiplying the row vector by the matrix:</p>
\\[ \begin{pmatrix} x_1 & x_2 \end{pmatrix} \begin{pmatrix} 1 & 3 \\ 3 & 9 \end{pmatrix} = \begin{pmatrix} (x_1 + 3x_2) & (3x_1 + 9x_2) \end{pmatrix} \\]
<p>Next, multiplying this result by the column vector:</p>
\\[ \begin{pmatrix} (x_1 + 3x_2) & (3x_1 + 9x_2) \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = x_1(x_1 + 3x_2) + x_2(3x_1 + 9x_2) \\]
<p>This expands to:</p>
\\[ x_1^2 + 3x_1x_2 + 3x_1x_2 + 9x_2^2 = x_1^2 + 6x_1x_2 + 9x_2^2 \\]
<p>This expression can be factored into a perfect square:</p>
\\[ \mathbf{x}^T \mathbf{A} \mathbf{x} = (x_1 + 3x_2)^2 \\]
<p>Since the result is a square, it is always greater than or equal to zero (\\(\ge 0\\)). Importantly, the expression can be equal to zero for non-zero vectors. For instance, if \\(x_1 = -3x_2\\), then \\(\mathbf{x}^T \mathbf{A} \mathbf{x} = 0\\). A non-zero vector satisfying this is \\(\mathbf{x} = \begin{pmatrix} -3 \\ 1 \end{pmatrix}\\). Because the quadratic form can be zero for non-zero vectors, the matrix \\(\mathbf{A}\\) is <b>Positive Semi-Definite (PSD) but not Positive Definite (PD)</b>.</p>

<h3>2. Eigenvalues and Eigenvectors of the PSD Matrix</h3>

<h4>Finding the Eigenvalues</h4>
<p>A key property of PSD and PD matrices is related to their eigenvalues. The eigenvalues (\\(\lambda\\)) are found by solving the characteristic equation \\(\det(\mathbf{A} - \lambda\mathbf{I}) = 0\\).</p>
\\[ \det\left(\begin{pmatrix} 1 & 3 \\ 3 & 9 \end{pmatrix} - \lambda \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\right) = 0 \\]
\\[ \det\begin{pmatrix} 1-\lambda & 3 \\ 3 & 9-\lambda \end{pmatrix} = 0 \\]
<p>Expanding the determinant:</p>
\\[ (1-\lambda)(9-\lambda) - (3)(3) = 0 \\]
\\[ 9 - \lambda - 9\lambda + \lambda^2 - 9 = 0 \\]
\\[ \lambda^2 - 10\lambda = 0 \\]
\\[ \lambda(\lambda - 10) = 0 \\]
<p>The eigenvalues are \\(\lambda_1 = 10\\) and \\(\lambda_2 = 0\\).</p>
<p><b>Key Property Connection:</b> The eigenvalues of a symmetric matrix are always real. For a PSD matrix, all eigenvalues must be non-negative (\\(\lambda \ge 0\\)). For a PD matrix, all eigenvalues must be strictly positive (\\(\lambda > 0\\)). Since one of the eigenvalues is 0, this confirms that the matrix \\(\mathbf{A}\\) is PSD but not PD.</p>

<h4>Finding the Eigenvectors</h4>
<p>Eigenvectors are found by finding the null space of \\(\mathbf{A} - \lambda\mathbf{I}\\) for each eigenvalue.</p>
<p><b>For \\(\lambda_1 = 10\\):</b></p>
<p>We solve \\((\mathbf{A} - 10\mathbf{I})\mathbf{x} = \mathbf{0}\\):</p>
\\[ \begin{pmatrix} 1-10 & 3 \\ 3 & 9-10 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} -9 & 3 \\ 3 & -1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\]
<p>Both rows give the same equation: \\(3x_1 - x_2 = 0\\), or \\(x_2 = 3x_1\\). An eigenvector is of the form \\(\begin{pmatrix} k \\ 3k \end{pmatrix}\\). Choosing \\(k=1\\), we get the eigenvector \\(\mathbf{x_1} = \begin{pmatrix} 1 \\ 3 \end{pmatrix}\\). The corresponding unit-norm eigenvector \\(\mathbf{u_1}\\) is:</p>
\\[ \mathbf{u_1} = \frac{\mathbf{x_1}}{||\mathbf{x_1}||} = \frac{1}{\sqrt{1^2 + 3^2}} \begin{pmatrix} 1 \\ 3 \end{pmatrix} = \frac{1}{\sqrt{10}} \begin{pmatrix} 1 \\ 3 \end{pmatrix} \\]

<p><b>For \\(\lambda_2 = 0\\):</b></p>
<p>We solve \\((\mathbf{A} - 0\mathbf{I})\mathbf{x} = \mathbf{A}\mathbf{x} = \mathbf{0}\\). This means the eigenvector corresponding to \\(\lambda=0\\) lies in the null space of \\(\mathbf{A}\\).</p>
\\[ \begin{pmatrix} 1 & 3 \\ 3 & 9 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\]
<p>This gives the equation \\(x_1 + 3x_2 = 0\\), or \\(x_1 = -3x_2\\). An eigenvector is of the form \\(\begin{pmatrix} -3k \\ k \end{pmatrix}\\). Choosing \\(k=1\\), we get \\(\mathbf{x_2} = \begin{pmatrix} -3 \\ 1 \end{pmatrix}\\). The corresponding unit-norm eigenvector \\(\mathbf{u_2}\\) is:</p>
\\[ \mathbf{u_2} = \frac{\mathbf{x_2}}{||\mathbf{x_2}||} = \frac{1}{\sqrt{(-3)^2 + 1^2}} \begin{pmatrix} -3 \\ 1 \end{pmatrix} = \frac{1}{\sqrt{10}} \begin{pmatrix} -3 \\ 1 \end{pmatrix} \\]

<h4>Orthogonality of Eigenvectors and Eigenvalue Decomposition</h4>
<p>For a symmetric matrix (including PSD matrices), eigenvectors corresponding to distinct eigenvalues are orthogonal. Let's verify this:</p>
\\[ \mathbf{u_1}^T \mathbf{u_2} = \left(\frac{1}{\sqrt{10}} \begin{pmatrix} 1 & 3 \end{pmatrix}\right) \left(\frac{1}{\sqrt{10}} \begin{pmatrix} -3 \\ 1 \end{pmatrix}\right) = \frac{1}{10} (1 \cdot (-3) + 3 \cdot 1) = \frac{1}{10} (-3+3) = 0 \\]
<p>The eigenvalue decomposition of a symmetric matrix is \\(\mathbf{A} = \mathbf{U\Lambda U}^T\\), where \\(\mathbf{U}\\) is the matrix whose columns are the orthonormal eigenvectors and \\(\mathbf{\Lambda}\\) is the diagonal matrix of eigenvalues.</p>
\\[ \mathbf{U} = \begin{pmatrix} \mathbf{u_1} & \mathbf{u_2} \end{pmatrix} = \frac{1}{\sqrt{10}} \begin{pmatrix} 1 & -3 \\ 3 & 1 \end{pmatrix} \\]
\\[ \mathbf{\Lambda} = \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} = \begin{pmatrix} 10 & 0 \\ 0 & 0 \end{pmatrix} \\]
<p>The matrix \\(\mathbf{U}\\) is unitary (or orthogonal for real matrices), meaning \\(\mathbf{U}^T\mathbf{U} = \mathbf{I}\\) and \\(\mathbf{U}^T = \mathbf{U}^{-1}\\).</p>

<h3>3. Application: Covariance Matrices and Whitening</h3>

<h4>Covariance Matrices are PSD</h4>
<p>A very important application of PSD matrices is in the study of random vectors. The covariance matrix \\(\mathbf{R}\\) of a zero-mean random vector \\(\mathbf{z}\\) is defined as:</p>
\\[ \mathbf{R} = E[\mathbf{z}\mathbf{z}^T] \\]
<p>The transcript proves that any covariance matrix \\(\mathbf{R}\\) is always PSD. The proof involves checking the quadratic form \\(\mathbf{x}^T \mathbf{R} \mathbf{x}\\) for an arbitrary non-zero vector \\(\mathbf{x}\\):</p>
<p>\\(\mathbf{x}^T \mathbf{R} \mathbf{x} = \mathbf{x}^T E[\mathbf{z}\mathbf{z}^T] \mathbf{x}\\)</p>
<p>Since \\(\mathbf{x}\\) is a constant vector, we can move it inside the expectation:</p>
<p>\\(\mathbf{x}^T \mathbf{R} \mathbf{x} = E[\mathbf{x}^T \mathbf{z}\mathbf{z}^T \mathbf{x}]\\)</p>
<p>Note that \\(\mathbf{x}^T\mathbf{z}\\) is a scalar. The term \\(\mathbf{z}^T\mathbf{x}\\) is its transpose, which is the same scalar. Therefore, \\((\mathbf{x}^T\mathbf{z})(\mathbf{z}^T\mathbf{x}) = (\mathbf{x}^T\mathbf{z})^2\\).</p>
<p>\\(\mathbf{x}^T \mathbf{R} \mathbf{x} = E[(\mathbf{x}^T\mathbf{z})^2]\\)</p>
<p>The quantity \\((\mathbf{x}^T\mathbf{z})^2\\) is a squared real number, so it is always non-negative. The expected value of a non-negative random variable is also non-negative. Thus:</p>
\\[ \mathbf{x}^T \mathbf{R} \mathbf{x} \ge 0 \\]
<p>This proves by definition that \\(\mathbf{R}\\) is a PSD matrix.</p>

<h4>Cholesky Decomposition, Whitening, and Coloring</h4>
<p>Since \\(\mathbf{R}\\) is PSD, it has a Cholesky-like decomposition (or "square root"):</p>
\\[ \mathbf{R} = \mathbf{R}_{\text{tilde}} \mathbf{R}_{\text{tilde}}^T \\]
<p>This property is used for two important processes in signal processing and statistics:</p>

<p><b>1. Whitening: From Correlated (Colored) to Uncorrelated (White) Noise</b></p>
<p>Given a random vector \\(\mathbf{z}\\) with covariance \\(\mathbf{R}\\), we can create a new vector \\(\tilde{\mathbf{z}}\\) whose components are uncorrelated and have unit variance. This new vector is called "white noise," and the process is called <b>whitening</b>.</p>
\\[ \tilde{\mathbf{z}} = \mathbf{R}_{\text{tilde}}^{-1} \mathbf{z} \\]
<p>The covariance of \\(\tilde{\mathbf{z}}\\) is the identity matrix:</p>
\\[ E[\tilde{\mathbf{z}}\tilde{\mathbf{z}}^T] = E[(\mathbf{R}_{\text{tilde}}^{-1} \mathbf{z})(\mathbf{R}_{\text{tilde}}^{-1} \mathbf{z})^T] = \mathbf{R}_{\text{tilde}}^{-1} E[\mathbf{z}\mathbf{z}^T] (\mathbf{R}_{\text{tilde}}^{-1})^T \\]
\\[ = \mathbf{R}_{\text{tilde}}^{-1} (\mathbf{R}) (\mathbf{R}_{\text{tilde}}^T)^{-1} = \mathbf{R}_{\text{tilde}}^{-1} (\mathbf{R}_{\text{tilde}} \mathbf{R}_{\text{tilde}}^T) (\mathbf{R}_{\text{tilde}}^T)^{-1} = \mathbf{I} \\]
<p>An identity covariance matrix means the off-diagonal elements are zero (uncorrelated components) and the diagonal elements are one (unit variance). If the original vector \\(\mathbf{z}\\) is Gaussian, the components of the whitened vector \\(\tilde{\mathbf{z}}\\) are not just uncorrelated but also independent.</p>

<p><b>2. Coloring: From White Noise to Colored Noise</b></p>
<p>The inverse process is used to generate a random vector with a specific, arbitrary covariance matrix \\(\mathbf{R}\\). This is useful in simulations. We start with a "white" random vector \\(\tilde{\mathbf{z}}\\) (components are IID, mean 0, variance 1), which is easy to generate, and "color" it.</p>
\\[ \mathbf{z} = \mathbf{R}_{\text{tilde}} \tilde{\mathbf{z}} \\]
<p>The covariance of this new vector \\(\mathbf{z}\\) is the desired matrix \\(\mathbf{R}\\):</p>
\\[ E[\mathbf{z}\mathbf{z}^T] = E[(\mathbf{R}_{\text{tilde}} \tilde{\mathbf{z}})(\mathbf{R}_{\text{tilde}} \tilde{\mathbf{z}})^T] = \mathbf{R}_{\text{tilde}} E[\tilde{\mathbf{z}}\tilde{\mathbf{z}}^T] \mathbf{R}_{\text{tilde}}^T \\]
\\[ = \mathbf{R}_{\text{tilde}} (\mathbf{I}) \mathbf{R}_{\text{tilde}}^T = \mathbf{R}_{\text{tilde}} \mathbf{R}_{\text{tilde}}^T = \mathbf{R} \\]
<p>This demonstrates how the properties of PSD matrices provide a powerful tool for manipulating and generating random vectors with specific statistical properties.</p>

</div></div><div class="chapter" id="Lecture 19 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 19 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of Principal Component Analysis (PCA), a fundamental technique in machine learning for dimensionality reduction and feature extraction. The explanation follows the concepts, theories, and formulas presented in the transcript.</p>

<h3>1. Introduction to Principal Component Analysis (PCA)</h3>
<p><b>Principal Component Analysis (PCA)</b> is an important application of linear algebra concepts, particularly those related to positive semi-definite matrices, eigenvalues, and eigenvectors. It is one of the most widely used techniques in machine learning for addressing the challenges posed by high-dimensional data.</p>

<p>
    <b>The Problem of High Dimensionality:</b>
    <br>
    In many real-world applications, such as facial image recognition, data vectors can have an extremely high number of dimensions. For instance, a 256x256 pixel image, when flattened into a vector, has \\(256^2 = 65,536\\) dimensions. Analyzing and processing such high-dimensional data is computationally expensive and complex. This is often referred to as the "curse of dimensionality."
</p>
<p>
    <b>The Goal of PCA:</b>
    <br>
    PCA aims to simplify this complexity by reducing the number of dimensions in the data. This process is known as <b>dimensionality reduction</b> or <b>feature extraction</b>. The core idea is to transform the data into a new, lower-dimensional space while retaining as much of the original information as possible. "Information" in this context is quantified by the variance in the data. PCA identifies the directions (principal components) along which the data has the largest variance or "spread."
</p>

<h3>2. The Intuition Behind PCA</h3>
<p>
    Imagine a two-dimensional dataset that is spread out like an ellipse.
    <ul>
        <li>There is one direction along which the data has the <b>most spread</b> (largest variance). This is the major axis of the ellipse.</li>
        <li>Perpendicular to this, there is another direction along which the data has a <b>smaller spread</b> (smaller variance). This is the minor axis.</li>
    </ul>
    PCA identifies these directions of varying spread. The direction with the largest variance is the most "important" because it captures the most significant structure in the data. By projecting the data onto this single direction, we can reduce the dimensionality from two to one while preserving the most crucial information.
</p>
<p>
    The key properties of these directions, called <b>principal directions</b>, are:
    <ol>
        <li>They are ordered by the amount of variance they capture, from largest to smallest.</li>
        <li>They are orthogonal (perpendicular) to each other. In a statistical sense, this means the resulting principal components are <b>uncorrelated</b>.</li>
    </ol>
</p>
<p>The projection of the original data onto these principal directions yields the <b>principal components</b>.</p>

<h3>3. The Mathematical Procedure of PCA</h3>
<p>The process of performing PCA involves several key steps, starting from a raw dataset and ending with a lower-dimensional representation.</p>

<p><b>Step 1: Data Preparation (Mean Centering)</b></p>
<p>
    Let the dataset consist of \\(n\\) data points \\(\{\tilde{\mathbf{x}}_1, \tilde{\mathbf{x}}_2, \dots, \tilde{\mathbf{x}}_n\}\\\), where each data point \\(\tilde{\mathbf{x}}_i\\) is a vector of size \\(M \times 1\\).
</p>
<ol>
    <li>
        <b>Calculate the Sample Mean:</b> First, compute the mean vector \\(\bar{\mu}\\) of the dataset.
        \\[ \bar{\mu} = \frac{1}{n} \sum_{i=1}^{n} \tilde{\mathbf{x}}_i \\]
    </li>
    <li>
        <b>Center the Data:</b> Subtract the mean vector from each data point to create a new dataset \\(\{\bar{\mathbf{x}}_1, \bar{\mathbf{x}}_2, \dots, \bar{\mathbf{x}}_n\}\\\) with a zero mean.
        \\[ \bar{\mathbf{x}}_i = \tilde{\mathbf{x}}_i - \bar{\mu} \\]
    </li>
</ol>
<p>This step is crucial because PCA is designed to find the directions of maximum variance, and variance is measured relative to the mean.</p>

<p><b>Step 2: Compute the Covariance Matrix</b></p>
<p>
    The next step is to estimate the sample covariance matrix, denoted by \\(\mathbf{R}\\), from the mean-centered data. The covariance matrix describes the relationships between the different dimensions in the data.
    \\[ \mathbf{R} = \frac{1}{n-1} \sum_{i=1}^{n} \bar{\mathbf{x}}_i \bar{\mathbf{x}}_i^T \\]
</p>
<p>The covariance matrix \\(\mathbf{R}\\) has important properties:</p>
<ul>
    <li>It is a <b>positive semi-definite (PSD)</b> matrix.</li>
    <li>This implies all its eigenvalues \\(\lambda_i\\) are non-negative (\\(\lambda_i \ge 0\\)).</li>
    <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal.</li>
</ul>

<p><b>Step 3: Finding the Principal Directions via Optimization</b></p>
<p>The first principal direction, \\(\bar{\mathbf{v}}_1\\), is the direction in space that maximizes the variance of the projected data. Let's formalize this.</p>
<ol>
    <li>
        <b>Projection and Variance:</b> The projection of a data point \\(\bar{\mathbf{x}}_i\\) onto a direction vector \\(\bar{\mathbf{v}}_1\\) is given by the inner product \\(\bar{\mathbf{v}}_1^T \bar{\mathbf{x}}_i\\). The variance of all these projections is:
        \\[ \text{Variance} = \frac{1}{n-1} \sum_{i=1}^{n} (\bar{\mathbf{v}}_1^T \bar{\mathbf{x}}_i)^2 \\]
        This expression can be simplified as follows:
        \\[ \text{Variance} = \frac{1}{n-1} \sum_{i=1}^{n} (\bar{\mathbf{v}}_1^T \bar{\mathbf{x}}_i)(\bar{\mathbf{x}}_i^T \bar{\mathbf{v}}_1) = \bar{\mathbf{v}}_1^T \left( \frac{1}{n-1} \sum_{i=1}^{n} \bar{\mathbf{x}}_i \bar{\mathbf{x}}_i^T \right) \bar{\mathbf{v}}_1 = \bar{\mathbf{v}}_1^T \mathbf{R} \bar{\mathbf{v}}_1 \\]
    </li>
    <li>
        <b>The Optimization Problem:</b> We want to find the vector \\(\bar{\mathbf{v}}_1\\) that maximizes this variance, with the constraint that \\(\bar{\mathbf{v}}_1\\) must be a unit vector (i.e., its norm is 1). This constraint is necessary because otherwise, we could make the variance arbitrarily large just by increasing the length of \\(\bar{\mathbf{v}}_1\\).
        <p>
            Maximize: \\(\bar{\mathbf{v}}_1^T \mathbf{R} \bar{\mathbf{v}}_1\\)
            <br>
            Subject to: \\(\Vert\bar{\mathbf{v}}_1\Vert^2 = \bar{\mathbf{v}}_1^T \bar{\mathbf{v}}_1 = 1\\)
        </p>
    </li>
    <li>
        <b>Solving with Lagrange Multipliers:</b> This constrained optimization problem can be solved using the method of Lagrange multipliers. The Lagrangian function \\(\mathcal{L}\\) is:
        \\[ \mathcal{L}(\bar{\mathbf{v}}_1, \lambda) = \bar{\mathbf{v}}_1^T \mathbf{R} \bar{\mathbf{v}}_1 + \lambda (1 - \bar{\mathbf{v}}_1^T \bar{\mathbf{v}}_1) \\]
        To find the maximum, we take the gradient of \\(\mathcal{L}\\) with respect to \\(\bar{\mathbf{v}}_1\\) and set it to zero:
        \\[ \nabla_{\bar{\mathbf{v}}_1} \mathcal{L} = 2\mathbf{R}\bar{\mathbf{v}}_1 - 2\lambda\bar{\mathbf{v}}_1 = 0 \\]
        This simplifies to the fundamental eigenvalue equation:
        \\[ \mathbf{R}\bar{\mathbf{v}}_1 = \lambda\bar{\mathbf{v}}_1 \\]
    </li>
    <li>
        <b>The Solution:</b> This equation shows that the direction \\(\bar{\mathbf{v}}_1\\) must be an <b>eigenvector</b> of the covariance matrix \\(\mathbf{R}\\), and \\(\lambda\\) is its corresponding <b>eigenvalue</b>.
        <p>
            To determine which eigenvector to choose, we substitute this result back into the variance formula:
            \\[ \text{Variance} = \bar{\mathbf{v}}_1^T \mathbf{R} \bar{\mathbf{v}}_1 = \bar{\mathbf{v}}_1^T (\lambda \bar{\mathbf{v}}_1) = \lambda (\bar{\mathbf{v}}_1^T \bar{\mathbf{v}}_1) = \lambda(1) = \lambda \\]
            This reveals that the variance along an eigenvector is equal to its corresponding eigenvalue. Therefore, to maximize the variance, we must choose the eigenvector \\(\bar{\mathbf{v}}_1\\) corresponding to the <b>largest eigenvalue</b> of \\(\mathbf{R}\\).
        </p>
    </li>
</ol>
<p>
    This logic extends to finding subsequent principal directions. The second principal direction, \\(\bar{\mathbf{v}}_2\\), is the eigenvector corresponding to the second-largest eigenvalue, and so on. If we want to reduce our data to \\(P\\) dimensions, we select the \\(P\\) eigenvectors \\(\{\bar{\mathbf{v}}_1, \bar{\mathbf{v}}_2, \dots, \bar{\mathbf{v}}_P\}\\) that correspond to the \\(P\\) largest eigenvalues of \\(\mathbf{R}\\).
</p>

<h3>4. Dimensionality Reduction with Principal Components</h3>
<p>Once the principal directions (eigenvectors) are identified, we can project the original data onto this new, lower-dimensional basis.</p>

<p>
    <b>Transforming a Single Data Point:</b>
    <br>
    To obtain the principal components for a single mean-centered data point \\(\bar{\mathbf{x}}_i\\), we project it onto each of the chosen principal directions.
    \\[
    \begin{pmatrix}
    \text{1st principal component} \\
    \text{2nd principal component} \\
    \vdots \\
    \text{P-th principal component}
    \end{pmatrix}
    =
    \begin{pmatrix}
    \bar{\mathbf{v}}_1^T \bar{\mathbf{x}}_i \\
    \bar{\mathbf{v}}_2^T \bar{\mathbf{x}}_i \\
    \vdots \\
    \bar{\mathbf{v}}_P^T \bar{\mathbf{x}}_i
    \end{pmatrix}
    \\]
    This can be written compactly in matrix form. Let \\(\mathbf{V}\\) be a matrix whose columns are the \\(P\\) chosen eigenvectors: \\(\mathbf{V} = [\bar{\mathbf{v}}_1, \bar{\mathbf{v}}_2, \dots, \bar{\mathbf{v}}_P]\\\). The new, reduced-dimension vector \\(\check{\mathbf{x}}_i\\) is:
    \\[ \check{\mathbf{x}}_i = \mathbf{V}^T \bar{\mathbf{x}}_i \\]
    If the original vector \\(\bar{\mathbf{x}}_i\\) was \\(M \times 1\\) and \\(\mathbf{V}\\) is \\(M \times P\\), the new vector \\(\check{\mathbf{x}}_i\\) is \\(P \times 1\\). Since we choose \\(P \ll M\\), we have achieved dimensionality reduction.
</p>

<p>
    <b>Transforming the Entire Dataset:</b>
    <br>
    We can apply this transformation to the entire dataset at once. Let \\(\mathbf{X} = [\bar{\mathbf{x}}_1, \bar{\mathbf{x}}_2, \dots, \bar{\mathbf{x}}_n]\\) be the \\(M \times n\\) matrix of mean-centered data. The transformed dataset \\(\check{\mathbf{X}}\\) is:
    \\[ \check{\mathbf{X}} = \mathbf{V}^T \mathbf{X} \\]
    The resulting matrix \\(\check{\mathbf{X}}\\) has dimensions \\(P \times n\\), containing the principal components for all data points. This is the final, lower-dimensional representation of the dataset.
</p>
</div></div><div class="chapter" id="Lecture 20 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 20 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the <b>Eigenfaces algorithm</b>, a significant application of linear algebra, specifically Principal Component Analysis (PCA), in the field of machine learning for face recognition.</p>

<h3>1. Introduction to Eigenfaces</h3>
<p>The Eigenfaces algorithm is a foundational method for computer-based face recognition. It treats face recognition as a dimensionality reduction problem. Instead of working with high-dimensional image data directly, it identifies a lower-dimensional subspace, often called "face space," that captures the most significant variations among a set of training face images. The core idea is that any face image can be approximated by a combination of a set of basis images, known as "eigenfaces," which are the principal components of the training set of faces.</p>

<h3>2. Representing Face Images Mathematically</h3>
<p>Before applying the algorithm, each face image must be converted into a suitable mathematical format.</p>
<p>
  <b>Step 1: Image as a Matrix</b><br>
  A grayscale digital image is a grid of pixels, where each pixel has an intensity value. This can be represented as a matrix. Let's say a facial image has \\(m_r\\) rows and \\(m_c\\) columns of pixels. The total number of pixels in the image is:
  \\[ m = m_r \times m_c \\]
  The \\(i^{th}\\) image in a dataset can be represented by a matrix \\(X_i\\) of size \\(m_r \times m_c\\).
  \\[ X_i = \begin{bmatrix} x_{i,1} & x_{i,2} & \cdots & x_{i,m_c} \end{bmatrix} \\]
  Here, \\(x_{i,j}\\) represents the \\(j^{th}\\) column of the \\(i^{th}\\) image matrix.
</p>
<p>
  <b>Step 2: Vectorization (Image as a Vector)</b><br>
  To apply linear algebra techniques like PCA, the 2D image matrix is converted into a 1D column vector. This is achieved by stacking the columns of the image matrix one after another. This process is also known as the <b>vec</b> operation.
  \\[ \tilde{x}_i = \text{vec}(X_i) = \begin{bmatrix} x_{i,1} \\ x_{i,2} \\ \vdots \\ x_{i,m_c} \end{bmatrix} \\]
  The resulting vector \\(\tilde{x}_i\\) has a dimension of \\(m \times 1\\). For example, a 256x256 pixel image becomes a single vector with \\(256 \times 256 = 65,536\\) elements. This vector represents the \\(i^{th}\\) face in the high-dimensional image space.
</p>

<h3>3. The Eigenfaces Algorithm: Procedure</h3>
<p>The algorithm can be divided into two main phases: a training phase where the "face space" is created from a database of known faces, and a recognition phase where a new face is identified.</p>

<h4>Phase 1: Training - Creating the Face Space</h4>
<p>Assume we have a training database of \\(N\\) face images, represented by the vectors \\(\tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_N\\).</p>
<p>
  <b>1. Calculate the Mean Face:</b><br>
  The first step is to compute the average face vector, \\(\bar{\mu}\\), which represents a "typical" face from the training set.
  \\[ \bar{\mu} = \frac{1}{N} \sum_{i=1}^{N} \tilde{x}_i \\]
</p>
<p>
  <b>2. Center the Face Vectors:</b><br>
  Subtract the mean face from each individual face vector. This centers the data around the origin, highlighting the unique variations of each face.
  \\[ \bar{x}_i = \tilde{x}_i - \bar{\mu} \\]
</p>
<p>
  <b>3. Compute the Covariance Matrix:</b><br>
  Next, compute the sample covariance matrix \\(R\\) of the centered face vectors. This \\(m \times m\\) matrix describes how the pixel values vary together across the dataset.
  \\[ R = \frac{1}{N-1} \sum_{i=1}^{N} \bar{x}_i \bar{x}_i^T \\]
  (Note: The transcript mentions \\(\bar{x}_i^H\\) or Hermitian transpose, which is equivalent to the standard transpose \\(\bar{x}_i^T\\) for real-valued pixel data.)
</p>
<p>
  <b>4. Eigenvalue Decomposition of the Covariance Matrix:</b><br>
  The core of the method is to find the eigenvectors and eigenvalues of the large covariance matrix \\(R\\).
  \\[ R = V \Lambda V^T \\]
  <ul>
    <li><b>Eigenvectors (\\(V\\)):</b> The columns of the matrix \\(V\\) are the eigenvectors of \\(R\\). When reshaped back into an \\(m_r \times m_c\\) grid, these eigenvectors look like ghostly face-like images, which is why they are called <b>"eigenfaces"</b>. They form an orthonormal basis for the "face space."</li>
    <li><b>Eigenvalues (\\(\Lambda\\)):</b> The diagonal matrix \\(\Lambda\\) contains the eigenvalues. Each eigenvalue indicates the amount of variance in the data captured by its corresponding eigenvector (eigenface).</li>
  </ul>
</p>
<p>
  <b>5. Select the Principal Components:</b><br>
  The eigenfaces are sorted in descending order based on their corresponding eigenvalues. The eigenfaces with the largest eigenvalues capture the most significant variations in the set of face images. We select the top \\(p\\) eigenfaces (where \\(p \ll m\\)) to form a lower-dimensional basis.
  \\[ V_p = [\bar{v}_1, \bar{v}_2, \dots, \bar{v}_p] \\]
  This \\(m \times p\\) matrix \\(V_p\\) defines the principal directions of our "face space."
</p>
<p>
  <b>6. Project Known Faces onto the Face Space:</b><br>
  Each centered face vector \\(\bar{x}_i\\) from the database is projected onto this new, lower-dimensional face space. This projection results in a <b>weight vector</b> \\(\bar{w}_i\\) for each face.
  \\[ \bar{w}_i = V_p^T \bar{x}_i \\]
  Each weight vector \\(\bar{w}_i\\) is a compact, \\(p \times 1\\) representation of the \\(i^{th}\\) face. The set of weight vectors \\(\{\bar{w}_1, \bar{w}_2, \dots, \bar{w}_N\}\\) constitutes our trained database of known faces.
</p>

<h4>Phase 2: Recognition - Identifying a New Face</h4>
<p>Given a new, unidentified face image, represented by the vector \\(\tilde{x}\\), the goal is to determine which person from the database it matches.</p>
<p>
  <b>1. Pre-process the New Image:</b><br>
  The new image vector is centered by subtracting the same mean face \\(\bar{\mu}\\) calculated during training.
  \\[ \bar{x} = \tilde{x} - \bar{\mu} \\]
</p>
<p>
  <b>2. Project onto Face Space:</b><br>
  The centered new face vector is projected onto the same face space to obtain its corresponding weight vector, \\(\bar{w}\\).
  \\[ \bar{w} = V_p^T \bar{x} \\]
</p>
<p>
  <b>3. Find the Closest Match:</b><br>
  The algorithm now compares the weight vector \\(\bar{w}\\) of the unknown face with all the weight vectors \\(\bar{w}_i\\) in the trained database. The distance between them is calculated, typically using the Euclidean distance (L2-norm).
  \\[ d_i = \| \bar{w} - \bar{w}_i \|_2 \\]
</p>
<p>
  <b>4. Recognition Decision:</b><br>
  The person in the database whose weight vector has the minimum distance to the unknown face's weight vector is declared the match. The index of the recognized face, \\(\tilde{i}\\), is found by:
  \\[ \tilde{i} = \underset{i}{\operatorname{argmin}} \, d_i \\]
  The input face is thus recognized as person \\(\tilde{i}\\).
</p>

<h3>4. Handling Unknown Faces with a Threshold</h3>
<p>A practical system must handle cases where the new face does not belong to anyone in the database. This is achieved by setting a distance threshold, \\(\epsilon\\).</p>
<p>
  After finding the minimum distance \\(d_{\tilde{i}}\\), it is compared to the threshold:
  <ul>
    <li>If \\(d_{\tilde{i}} \leq \epsilon\\): The match is considered valid, and the face is identified as person \\(\tilde{i}\\).</li>
    <li>If \\(d_{\tilde{i}} > \epsilon\\): The distance is too large, implying the new face is not close enough to any face in the database. The face is declared "unknown" or "not recognized."</li>
  </ul>
</p>

<h3>5. Conclusion</h3>
<p>The Eigenfaces algorithm is a powerful demonstration of how linear algebra can solve complex real-world problems. By applying PCA, it achieves significant <b>dimensionality reduction</b>, transforming the computationally intensive task of comparing high-dimensional image vectors into a much more efficient comparison of low-dimensional weight vectors. This reduces storage requirements and speeds up the recognition process, making it a foundational technique in computer vision and machine learning.</p>
</div></div><h2>Weekly Summary</h2><div>
<h2>Positive Semi-Definite (PSD) and Positive Definite (PD) Matrices</h2>
<p>This week introduces a crucial class of matrices in linear algebra: Positive Semi-Definite (PSD) and Positive Definite (PD) matrices. These are symmetric (or Hermitian for complex matrices) square matrices with important properties that appear frequently in machine learning and data analysis.</p>
<b>Key Takeaways:</b>
<ul>
    <li><b>Definition:</b> A symmetric matrix \\(A\\) is defined as:
        <ul>
            <li><b>PSD</b> if \\(\mathbf{x}^T A \mathbf{x} \ge 0\\) for all non-zero vectors \\(\mathbf{x}\\). For complex matrices, this is \\(\mathbf{x}^H A \mathbf{x} \ge 0\\).</li>
            <li><b>PD</b> if \\(\mathbf{x}^T A \mathbf{x} > 0\\) for all non-zero vectors \\(\mathbf{x}\\). For complex matrices, this is \\(\mathbf{x}^H A \mathbf{x} > 0\\).</li>
        </ul>
    </li>
    <li><b>Eigenvalue Properties:</b> The eigenvalues (\\(\lambda\\)) of a PSD/PD matrix are always real.
        <ul>
            <li>For a <b>PSD</b> matrix, all eigenvalues are non-negative (\\(\lambda \ge 0\\)). A matrix is PSD but not PD if it has at least one zero eigenvalue.</li>
            <li>For a <b>PD</b> matrix, all eigenvalues are strictly positive (\\(\lambda > 0\\)).</li>
        </ul>
    </li>
    <li><b>Eigenvector Properties:</b> Eigenvectors corresponding to distinct eigenvalues of a PSD/PD matrix are orthogonal. This allows for the construction of an orthonormal basis of eigenvectors.</li>
    <li><b>Eigenvalue Decomposition (EVD):</b> Because the eigenvectors form an orthonormal basis, the EVD of a PSD matrix \\(A\\) simplifies to \\(A = U \Lambda U^H\\), where \\(U\\) is a unitary matrix (\\(U^H = U^{-1}\\)) whose columns are the orthonormal eigenvectors, and \\(\Lambda\\) is the diagonal matrix of real, non-negative eigenvalues.</li>
    <li><b>Cholesky / Square-Root Factorization:</b> Any PSD matrix \\(A\\) can be decomposed into the form \\(A = \tilde{A} \tilde{A}^H\\). This is analogous to the square root of a positive real number and is a key property used in applications.</li>
</ul>

<h2>Applications Involving Covariance Matrices</h2>
<p>The properties of PSD matrices are demonstrated through a concrete 2x2 example and a significant application involving the covariance matrix of random vectors.</p>
<b>Key Takeaways:</b>
<ul>
    <li><b>Covariance Matrix is always PSD:</b> For any zero-mean random vector \\(\mathbf{z}\\), its covariance matrix \\(R = E[\mathbf{z}\mathbf{z}^T]\\) is always a positive semi-definite matrix. This is because for any vector \\(\mathbf{x}\\), the quantity \\(\mathbf{x}^T R \mathbf{x} = E[(\mathbf{x}^T\mathbf{z})^2]\\) is an expected value of a squared term, which must be non-negative.</li>
    <li><b>Whitening Transformation:</b> Since the covariance matrix \\(R\\) is PSD, it can be factored as \\(R = \tilde{R}\tilde{R}^T\\). A "whitening" transformation can be applied to a correlated random vector \\(\mathbf{z}\\) by computing \\(\tilde{\mathbf{z}} = \tilde{R}^{-1}\mathbf{z}\\). The resulting vector \\(\tilde{\mathbf{z}}\\) will have uncorrelated components with unit variance, meaning its covariance matrix is the identity matrix.</li>
    <li><b>Generating Correlated Data:</b> The inverse process, known as "coloring," can be used to generate a random vector with a specific desired covariance structure. Starting with a "white" random vector \\(\tilde{\mathbf{z}}\\) (with an identity covariance matrix), a new vector \\(\mathbf{z} = \tilde{R}\tilde{\mathbf{z}}\\) can be generated that will have the covariance matrix \\(R\\).</li>
</ul>

<h2>Principal Component Analysis (PCA) and Eigenfaces</h2>
<p>This section covers Principal Component Analysis (PCA), a powerful dimensionality reduction technique, and its direct application in the Eigenfaces algorithm for face recognition. Both heavily rely on the properties of the covariance matrix.</p>
<b>Key Takeaways:</b>
<ul>
    <li><b>Principal Component Analysis (PCA):</b>
        <ul>
            <li><b>Goal:</b> To reduce the dimensionality of high-dimensional data while retaining the maximum amount of variance (information).</li>
            <li><b>Method:</b> PCA identifies the "principal directions" of the data, which are the directions of maximum spread or variance. These directions are found to be the <b>eigenvectors</b> of the data's covariance matrix.</li>
            <li>The corresponding <b>eigenvalues</b> represent the amount of variance along each eigenvector. The principal components are obtained by projecting the data onto the eigenvectors associated with the largest eigenvalues.</li>
        </ul>
    </li>
    <li><b>Eigenfaces Algorithm:</b>
        <ul>
            <li><b>Concept:</b> A direct application of PCA to a database of face images for the purpose of recognition.</li>
            <li><b>Procedure:</b>
                <ol>
                    <li>Represent a training set of face images as high-dimensional vectors.</li>
                    <li>Compute the sample covariance matrix of these vectors.</li>
                    <li>The eigenvectors of this covariance matrix are the "eigenfaces." They form a basis representing the most significant variations among the faces in the dataset.</li>
                    <li>Each face in the database is represented as a low-dimensional "weight vector" by projecting it onto the most significant eigenfaces.</li>
                    <li>To recognize a new face, it is also projected onto the eigenfaces to get its weight vector. This vector is then compared (e.g., using Euclidean distance) to the weight vectors of known faces in the database to find the closest match.</li>
                </ol>
            </li>
        </ul>
    </li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<p>Here is an explanation of the questions and answers from the assignment.</p>

<hr>

<h3>Question 1: Adjacency Matrix of a Circuit</h3>
<p><b>Question:</b> This question asks to identify the correct adjacency matrix for a given circuit diagram. The circuit diagram itself is not visible in the provided text.</p>
<p><b>Explanation:</b></p>
<p>The term "adjacency matrix" in the context of circuit analysis often refers to the <b>node-branch incidence matrix</b>, denoted as \\(A\\). This matrix describes how the nodes (junctions) are connected by the branches (components like resistors, sources, etc.).</p>
<p>The construction of this matrix follows a standard convention:</p>
<ul>
    <li>The rows of the matrix correspond to the nodes in the circuit.</li>
    <li>The columns of the matrix correspond to the branches.</li>
    <li>Each branch is given an assumed direction for current flow.</li>
    <li>The entry \\(A_{ij}\\) (in the i-th row and j-th column) is determined as follows:
        <ul>
            <li>\\(A_{ij} = +1\\) if the current in branch \\(j\\) flows <i>away from</i> node \\(i\\).</li>
            <li>\\(A_{ij} = -1\\) if the current in branch \\(j\\) flows <i>into</i> node \\(i\\).</li>
            <li>\\(A_{ij} = 0\\) if branch \\(j\\) is not connected to node \\(i\\).</li>
        </ul>
    </li>
</ul>
<p>A key property of an incidence matrix is that the sum of the elements in each column is zero, because every branch must connect two nodes (one entry will be +1 and the other -1).</p>
<p>To solve this problem, one would need to look at the provided circuit diagram, label the nodes and branches, assume a direction for the current in each branch, and then construct the matrix based on the rules above. The correct option is the one that accurately represents the connections and directions in the circuit diagram.</p>

<hr>

<h3>Question 2: Gram-Schmidt Procedure</h3>
<p><b>Question:</b> Given two vectors \\(\bar{x}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\\) and \\(\bar{x}_2 = \begin{bmatrix} -1 \\ 2 \\ -2 \\ -3 \end{bmatrix}\\), find a set of orthonormal basis vectors \\(\{\bar{v}_1, \bar{v}_2\}\\) using the Gram-Schmidt procedure.</p>
<p><b>Answer Explanation:</b></p>
<p>The Gram-Schmidt procedure is a method to convert a set of linearly independent vectors into an orthonormal set that spans the same subspace.</p>
<p><b>Step 1: Find the first orthonormal vector \\(\bar{v}_1\\).</b></p>
<p>We start with \\(\bar{u}_1 = \bar{x}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\\). Then we normalize it.</p>
<p>The norm is \\(\|\bar{u}_1\| = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = \sqrt{4} = 2\\).</p>
<p>So, \\(\bar{v}_1 = \frac{\bar{u}_1}{\|\bar{u}_1\|} = \frac{1}{2} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\\).</p>

<p><b>Step 2: Find the second orthonormal vector \\(\bar{v}_2\\).</b></p>
<p>First, we find an orthogonal vector \\(\bar{u}_2\\) by subtracting the projection of \\(\bar{x}_2\\) onto \\(\bar{u}_1\\) from \\(\bar{x}_2\\).</p>
\\[ \bar{u}_2 = \bar{x}_2 - \text{proj}_{\bar{u}_1}(\bar{x}_2) = \bar{x}_2 - \frac{\bar{x}_2 \cdot \bar{u}_1}{\|\bar{u}_1\|^2} \bar{u}_1 \\]
<p>The dot product is \\(\bar{x}_2 \cdot \bar{u}_1 = (-1)(1) + (2)(1) + (-2)(1) + (-3)(1) = -1 + 2 - 2 - 3 = -4\\).</p>
<p>The squared norm \\(\|\bar{u}_1\|^2 = 4\\).</p>
\\[ \bar{u}_2 = \begin{bmatrix} -1 \\ 2 \\ -2 \\ -3 \end{bmatrix} - \frac{-4}{4} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -1 \\ 2 \\ -2 \\ -3 \end{bmatrix} - (-1) \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -1+1 \\ 2+1 \\ -2+1 \\ -3+1 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \\ -1 \\ -2 \end{bmatrix} \\]
<p>Now, we normalize \\(\bar{u}_2\\).</p>
<p>The norm is \\(\|\bar{u}_2\| = \sqrt{0^2 + 3^2 + (-1)^2 + (-2)^2} = \sqrt{0 + 9 + 1 + 4} = \sqrt{14}\\).</p>
<p>So, \\(\bar{v}_2 = \frac{\bar{u}_2}{\|\bar{u}_2\|} = \frac{1}{\sqrt{14}} \begin{bmatrix} 0 \\ 3 \\ -1 \\ -2 \end{bmatrix}\\).</p>
<p>The correct set of orthonormal vectors is \\(\bar{v}_1 = \frac{1}{2}\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}\\) and \\(\bar{v}_2 = \frac{1}{\sqrt{14}}\begin{bmatrix} 0 \\ 3 \\ -1 \\ -2 \end{bmatrix}\\), which matches the accepted answer.</p>

<hr>

<h3>Question 3: Variance of a Linear Combination of Random Variables</h3>
<p><b>Question:</b> Given independent, zero-mean Gaussian random variables \\(x_1, x_2, \dots, x_n\\) with variances \\(\text{Var}(x_i) = i\\) for \\(i=1, \dots, n\\). What is the variance of the quantity \\(Y = x_1 + \sqrt{2}x_2 + \dots + \sqrt{n}x_n\\)?</p>
<p><i>Note: The question states the variables are "i.i.d." (independent and identically distributed), but then assigns different variances. This is contradictory. The calculation assumes they are <b>independent</b> but not identically distributed.</i></p>
<p><b>Answer Explanation:</b></p>
<p>We need to find \\(\text{Var}(Y) = \text{Var}\left(\sum_{i=1}^n \sqrt{i} x_i\right)\\).</p>
<p>We use two key properties of variance:</p>
<ol>
    <li>For any random variable \\(X\\) and constant \\(a\\), \\(\text{Var}(aX) = a^2 \text{Var}(X)\\).</li>
    <li>For independent random variables \\(X_1, \dots, X_n\\), the variance of their sum is the sum of their variances: \\(\text{Var}(\sum X_i) = \sum \text{Var}(X_i)\\).</li>
</ol>
<p>Combining these properties for our linear combination of independent variables:</p>
\\[ \text{Var}(Y) = \text{Var}\left(\sum_{i=1}^n \sqrt{i} x_i\right) = \sum_{i=1}^n \text{Var}(\sqrt{i} x_i) = \sum_{i=1}^n (\sqrt{i})^2 \text{Var}(x_i) \\]
<p>We are given that \\(\text{Var}(x_i) = i\\). Substituting this in:</p>
\\[ \text{Var}(Y) = \sum_{i=1}^n i \cdot (i) = \sum_{i=1}^n i^2 \\]
<p>This is the sum of the first \\(n\\) square numbers, for which there is a standard formula:</p>
\\[ \sum_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6} \\]
<p>This matches the accepted answer.</p>

<hr>

<h3>Question 4: Null Space of a Matrix</h3>
<p><b>Question:</b> Which of the given vectors lies in the null space of the matrix \\(A = \begin{bmatrix} 1 & 5 & 1 & -2 & 1 \\ 4 & 1 & -3 & 1 & 1 \end{bmatrix}\\)?</p>
<p><b>Answer Explanation:</b></p>
<p>A vector \\(\bar{x}\\) lies in the null space of a matrix \\(A\\) if and only if the product \\(A\bar{x}\\) equals the zero vector \\(\bar{0}\\). To solve this, we must multiply the matrix \\(A\\) by each of the option vectors and see which one results in \\(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\\).</p>
<p>Let's check the vector from the accepted answer: \\(\bar{x} = \begin{bmatrix} 2 \\ 1 \\ 1 \\ 2 \\ -6 \end{bmatrix}\\).</p>
\\[ A\bar{x} = \begin{bmatrix} 1 & 5 & 1 & -2 & 1 \\ 4 & 1 & -3 & 1 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \\ 1 \\ 2 \\ -6 \end{bmatrix} \\]
\\[ = \begin{bmatrix} (1)(2) + (5)(1) + (1)(1) + (-2)(2) + (1)(-6) \\ (4)(2) + (1)(1) + (-3)(1) + (1)(2) + (1)(-6) \end{bmatrix} \\]
\\[ = \begin{bmatrix} 2 + 5 + 1 - 4 - 6 \\ 8 + 1 - 3 + 2 - 6 \end{bmatrix} = \begin{bmatrix} 8 - 10 \\ 11 - 9 \end{bmatrix} = \begin{bmatrix} -2 \\ 2 \end{bmatrix} \\]
<p>The result is \\(\begin{bmatrix} -2 \\ 2 \end{bmatrix}\\), which is not the zero vector \\(\begin{bmatrix} 0 \\ 0 \end{bmatrix}\\). Checking the other options also shows that none of them result in the zero vector.</p>
<p><b>Conclusion:</b> There appears to be a typographical error in the problem statement (either the matrix \\(A\\) or the vector options) or in the recorded correct answer, as none of the provided vectors are in the null space of the given matrix \\(A\\).</p>

<hr>

<h3>Question 5: Gaussian Classifier</h3>
<p><b>Question:</b> For a two-class problem where \\(C_1 \sim \mathcal{N}(\mu_1, \Sigma)\\) and \\(C_2 \sim \mathcal{N}(\mu_2, \Sigma)\\) with \\(\mu_1 = \begin{bmatrix} -2 \\ 4 \end{bmatrix}\\), \\(\mu_2 = \begin{bmatrix} 4 \\ -2 \end{bmatrix}\\), and \\(\Sigma = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/4 \end{bmatrix}\\), find the decision rule to classify a new vector \\(\bar{x} = [x_1, x_2]^T\\) into class \\(C_1\\).</p>
<p><b>Answer Explanation:</b></p>
<p>When the covariance matrices are equal for both classes and we assume equal prior probabilities, a new sample \\(\bar{x}\\) is assigned to the class with the closer mean. The "closeness" is measured by the Mahalanobis distance. We choose \\(C_1\\) if \\(\bar{x}\\) is closer to \\(\mu_1\\) than to \\(\mu_2\\):</p>
\\[ (\bar{x} - \mu_1)^T \Sigma^{-1} (\bar{x} - \mu_1) \leq (\bar{x} - \mu_2)^T \Sigma^{-1} (\bar{x} - \mu_2) \\]
<p>Expanding this quadratic form and canceling common terms (\\(\bar{x}^T\Sigma^{-1}\bar{x}\\)) leads to a linear decision boundary:</p>
\\[ -2\mu_1^T\Sigma^{-1}\bar{x} + \mu_1^T\Sigma^{-1}\mu_1 \leq -2\mu_2^T\Sigma^{-1}\bar{x} + \mu_2^T\Sigma^{-1}\mu_2 \\]
<p>Rearranging gives:</p>
\\[ 2(\mu_2 - \mu_1)^T \Sigma^{-1} \bar{x} \leq \mu_2^T\Sigma^{-1}\mu_2 - \mu_1^T\Sigma^{-1}\mu_1 \\]
<p>Let's compute the terms:</p>
<p>The inverse covariance matrix is \\(\Sigma^{-1} = \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix}\\).</p>
<p>\\(\mu_2 - \mu_1 = \begin{bmatrix} 4 - (-2) \\ -2 - 4 \end{bmatrix} = \begin{bmatrix} 6 \\ -6 \end{bmatrix}\\).</p>
<p>The left side of the inequality is: \\(2 \begin{bmatrix} 6 & -6 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 2 \begin{bmatrix} 12 & -24 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 24x_1 - 48x_2\\).</p>
<p>For the right side: \\(\mu_2^T\Sigma^{-1}\mu_2 = \begin{bmatrix} 4 & -2 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} 4 \\ -2 \end{bmatrix} = 48\\). And \\(\mu_1^T\Sigma^{-1}\mu_1 = \begin{bmatrix} -2 & 4 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} -2 \\ 4 \end{bmatrix} = 72\\).</p>
<p>The inequality becomes: \\(24x_1 - 48x_2 \leq 48 - 72\\), which simplifies to \\(24x_1 - 48x_2 \leq -24\\). Dividing by 24 gives \\(x_1 - 2x_2 \leq -1\\).</p>
<p>The accepted answer is \\(-x_1 + 2x_2 \geq 1\\). Multiplying this by -1 yields \\(x_1 - 2x_2 \leq -1\\), which confirms our result.</p>

<hr>

<h3>Question 6: Mean and Covariance of a Transformed Random Vector</h3>
<p><b>Question:</b> A random vector \\(\bar{x}\\) is distributed with mean \\(\bar{\mu}\\) and covariance matrix \\(\Sigma\\). Find the mean and covariance of the transformed vector \\(\bar{y} = A\bar{x} + \bar{b}\\).</p>
<p><b>Answer Explanation:</b></p>
<p>This is a standard result for an affine transformation of a random vector.</p>
<p><b>1. Mean of \\(\bar{y}\\):</b></p>
<p>The expectation operator \\(E[\cdot]\\) is linear. We can apply it directly:</p>
\\[ E[\bar{y}] = E[A\bar{x} + \bar{b}] = E[A\bar{x}] + E[\bar{b}] \\]
<p>Since \\(A\\) and \\(\bar{b}\\) are constants:</p>
\\[ E[\bar{y}] = A E[\bar{x}] + \bar{b} = A\bar{\mu} + \bar{b} \\]
<p><b>2. Covariance of \\(\bar{y}\\):</b></p>
<p>The covariance matrix is defined as \\(\text{Cov}(\bar{y}) = E[(\bar{y} - E[\bar{y}])(\bar{y} - E[\bar{y}])^T]\\).</p>
<p>First, find the deviation from the mean:</p>
\\[ \bar{y} - E[\bar{y}] = (A\bar{x} + \bar{b}) - (A\bar{\mu} + \bar{b}) = A\bar{x} - A\bar{\mu} = A(\bar{x} - \bar{\mu}) \\]
<p>Now, substitute this into the covariance definition:</p>
\\[ \text{Cov}(\bar{y}) = E\left[ \left( A(\bar{x} - \bar{\mu}) \right) \left( A(\bar{x} - \bar{\mu}) \right)^T \right] \\]
<p>Using the transpose property \\((BC)^T = C^T B^T\\):</p>
\\[ \text{Cov}(\bar{y}) = E\left[ A(\bar{x} - \bar{\mu}) (\bar{x} - \bar{\mu})^T A^T \right] \\]
<p>Since \\(A\\) is a constant matrix, it can be pulled out of the expectation:</p>
\\[ \text{Cov}(\bar{y}) = A \ E\left[ (\bar{x} - \bar{\mu}) (\bar{x} - \bar{\mu})^T \right] \ A^T \\]
<p>The expression inside the expectation is the definition of the covariance matrix of \\(\bar{x}\\), which is \\(\Sigma\\).</p>
\\[ \text{Cov}(\bar{y}) = A \Sigma A^T \\]
<p>Therefore, the new mean is \\(A\bar{\mu} + \bar{b}\\) and the new covariance is \\(A\Sigma A^T\\).</p>

<hr>

<h3>Question 7: Variance of a Linear Combination (revisited)</h3>
<p><b>Question:</b> Given i.i.d. zero-mean Gaussian random variables \\(x_1, \dots, x_n\\) with variance \\(\sigma^2\\). Find the variance of the quantity \\(\bar{a}^T\bar{x}\\).</p>
<p><b>Answer Explanation:</b></p>
<p>The quantity \\(Y = \bar{a}^T\bar{x}\\) is a scalar value, which can be written as the sum \\(Y = \sum_{i=1}^n a_i x_i\\).</p>
<p>We need to find \\(\text{Var}(Y)\\). Since the \\(x_i\\) are independent, we can use the same properties as in Question 3:</p>
\\[ \text{Var}(Y) = \text{Var}\left(\sum_{i=1}^n a_i x_i\right) = \sum_{i=1}^n \text{Var}(a_i x_i) = \sum_{i=1}^n a_i^2 \text{Var}(x_i) \\]
<p>We are given that \\(\text{Var}(x_i) = \sigma^2\\) for all \\(i\\).</p>
\\[ \text{Var}(Y) = \sum_{i=1}^n a_i^2 \sigma^2 = \sigma^2 \left(\sum_{i=1}^n a_i^2\right) \\]
<p>The sum \\(\sum_{i=1}^n a_i^2\\) is the squared Euclidean norm (or L2-norm) of the vector \\(\bar{a}\\), denoted by \\(\|\bar{a}\|^2\\).</p>
<p>Therefore, the variance is \\(\sigma^2 \|\bar{a}\|^2\\).</p>

<hr>

<h3>Question 8: Definition of Covariance Matrix</h3>
<p><b>Question:</b> For a random column vector \\(\bar{x}\\) with mean \\(\bar{\mu}\\), its covariance matrix is defined as ...</p>
<p><b>Answer Explanation:</b></p>
<p>This is a definitional question. The covariance matrix, typically denoted by \\(\Sigma\\), measures the variation of each component of the random vector and the correlation between different components. It is defined as the expected value of the <b>outer product</b> of the deviation vector \\((\bar{x} - \bar{\mu})\\) with itself.</p>
<p>The deviation vector is \\(\bar{x} - \bar{\mu}\\).</p>
<p>The outer product of this vector with itself is \\((\bar{x} - \bar{\mu})(\bar{x} - \bar{\mu})^T\\). If \\(\bar{x}\\) is an \\(n \times 1\\) vector, this product results in an \\(n \times n\\) matrix.</p>
<p>The definition of the covariance matrix is the expectation of this matrix:</p>
\\[ \Sigma = E\left[ (\bar{x} - \bar{\mu})(\bar{x} - \bar{\mu})^T \right] \\]
<p>The other options are incorrect. For instance, \\(E[\bar{x}^T\bar{x}]\\) is the expected value of the inner product, which is a scalar (the expected squared norm), not a matrix.</p>

<hr>

<h3>Question 9: Norm of a Normalized Vector</h3>
<p><b>Question:</b> Let \\(\bar{v} = \frac{\bar{u}}{\|\bar{u}\|_2}\\). What is the value of \\(\|\bar{v}\|_2\\)?</p>
<p><b>Answer Explanation:</b></p>
<p>This question asks for the norm of a normalized vector. A vector is normalized by dividing it by its own norm. This process creates a <b>unit vector</b>, which is a vector with a norm of 1.</p>
<p>We can show this formally using the properties of vector norms. For any vector \\(\bar{x}\\) and scalar \\(\alpha\\), we have \\(\|\alpha \bar{x}\|_2 = |\alpha| \|\bar{x}\|_2\\).</p>
<p>In our case, the vector is \\(\bar{u}\\) and the scalar is \\(\alpha = \frac{1}{\|\bar{u}\|_2}\\). Assuming \\(\bar{u}\\) is not the zero vector, its norm is a positive real number, so \\(|\alpha| = \alpha\\).</p>
\\[ \|\bar{v}\|_2 = \left\| \frac{\bar{u}}{\|\bar{u}\|_2} \right\|_2 = \left\| \left(\frac{1}{\|\bar{u}\|_2}\right) \bar{u} \right\|_2 \\]
\\[ = \left(\frac{1}{\|\bar{u}\|_2}\right) \|\bar{u}\|_2 = 1 \\]
<p>The norm of any non-zero vector that has been normalized is always 1.</p>

<hr>

<h3>Question 10: Minor of a Matrix</h3>
<p><b>Question:</b> For the matrix \\(A = \begin{bmatrix} 3 & -4 & 1 \\ -2 & 1 & -2 \\ -3 & 6 & 3 \end{bmatrix}\\), find the minor \\(M_{3,2}\\).</p>
<p><b>Answer Explanation:</b></p>
<p>The minor \\(M_{i,j}\\) of a matrix is the determinant of the submatrix formed by deleting the i-th row and j-th column.</p>
<p>Here, we need \\(M_{3,2}\\). We delete the 3rd row and the 2nd column from matrix \\(A\\):</p>
\\[ A = \begin{bmatrix} 3 & \mathbf{-4} & 1 \\ -2 & \mathbf{1} & -2 \\ \mathbf{-3} & \mathbf{6} & \mathbf{3} \end{bmatrix} \\]
<p>After removing row 3 and column 2, the remaining submatrix is:</p>
\\[ \begin{bmatrix} 3 & 1 \\ -2 & -2 \end{bmatrix} \\]
<p>Now, we calculate the determinant of this \\(2 \times 2\\) submatrix:</p>
\\[ M_{3,2} = \det\begin{pmatrix} 3 & 1 \\ -2 & -2 \end{pmatrix} = (3)(-2) - (1)(-2) = -6 - (-2) = -6 + 2 = -4 \\]
<p>The calculated minor is -4.</p>
<p><b>Note:</b> The accepted answer for this question is listed as 6. The calculation based on the provided matrix gives -4. This discrepancy suggests there may be a typo in the question's matrix. For example, if the top-left element \\(A_{1,1}\\) was -2 instead of 3, the submatrix would be \\(\begin{bmatrix} -2 & 1 \\ -2 & -2 \end{bmatrix}\\), and its determinant would be \\((-2)(-2) - (1)(-2) = 4 + 2 = 6\\). Given the provided matrix, the correct answer is -4.</p>
</div></div><div class="week" id="week_5"><h1 class="week-title">Week 5</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture  21| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture  21| Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas related to the <b>Least Squares (LS) solution</b> for systems of linear equations, as presented in the transcript.</p>

<h3>1. The System of Linear Equations</h3>
<p>The starting point is a standard system of linear equations, which can be represented in matrix-vector form:</p>
\\[ \mathbf{y} = A \mathbf{x} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is an \\(m \times 1\\) vector of observations or outputs. In the transcript, it is written as \\(\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix}\\).</li>
    <li>\\(\mathbf{x}\\) is an \\(n \times 1\\) vector of unknowns or inputs. In the transcript, it is written as \\(\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}\\).</li>
    <li>\\(A\\) is an \\(m \times n\\) matrix that maps the input vector \\(\mathbf{x}\\) to the output vector \\(\mathbf{y}\\).</li>
</ul>
<p>The number \\(m\\) represents the number of linear equations (or observations), while \\(n\\) represents the number of unknown variables.</p>

<h3>2. The Challenge of Overdetermined Systems</h3>
<p>A standard, unique solution exists if the matrix \\(A\\) is square (\\(m = n\\)) and invertible. In this case, the solution is simply:</p>
\\[ \mathbf{x} = A^{-1} \mathbf{y} \\]
<p>However, the transcript focuses on a different scenario known as an <b>overdetermined system</b>, which occurs when there are more equations than unknowns, i.e., \\(\mathbf{m > n}\\).</p>

<p><b>Key characteristics of overdetermined systems:</b></p>
<ul>
    <li><b>More Observations than Unknowns:</b> In practical terms, this means we have more data points or measurements (\\(m\\)) than parameters (\\(n\\)) we are trying to determine.</li>
    <li><b>Non-Square Matrix:</b> The matrix \\(A\\) is a "tall" matrix (more rows than columns), meaning it is not square and therefore the concept of a standard inverse \\(A^{-1}\\) does not apply.</li>
    <li><b>No Exact Solution (Generally):</b> An exact solution to \\(\mathbf{y} = A\mathbf{x}\\) exists only if the vector \\(\mathbf{y}\\) lies in the <b>column space</b> of \\(A\\). The column space of \\(A\\) is the set of all possible linear combinations of its columns. Since there are \\(n\\) columns in an \\(m\\)-dimensional space (with \\(n < m\\)), the column space is an \\(n\\)-dimensional subspace. It is very unlikely for a general \\(m\\)-dimensional vector \\(\mathbf{y}\\) to fall exactly within this lower-dimensional subspace. Therefore, for most overdetermined systems, there is no vector \\(\mathbf{x}\\) that can exactly satisfy \\(\mathbf{y} = A\mathbf{x}\\).</li>
</ul>

<h3>3. The Least Squares Problem: Finding the Best Approximation</h3>
<p>Since an exact solution is generally impossible, the goal shifts from finding an exact solution to finding the <b>best approximate solution</b>. We want to find a vector \\(\mathbf{x}\\) such that \\(A\mathbf{x}\\) is as "close" as possible to \\(\mathbf{y}\\). This is formulated as an optimization problem.</p>

<p><b>1. Define the Error:</b> The approximation error is defined as the difference between the actual observations \\(\mathbf{y}\\) and the approximated values \\(A\mathbf{x}\\):</p>
\\[ \mathbf{e} = \mathbf{y} - A\mathbf{x} \\]

<p><b>2. Minimize the Error's Magnitude:</b> We cannot minimize the error vector \\(\mathbf{e}\\) directly. Instead, we minimize its magnitude. The standard approach is to minimize the square of the Euclidean norm (length) of the error vector. This avoids dealing with square roots and has desirable mathematical properties.</p>
<p>The problem is formally stated as:</p>
\\[ \min_{\mathbf{x}} ||\mathbf{e}||^2 = \min_{\mathbf{x}} ||\mathbf{y} - A\mathbf{x}||^2 \\]
<p>This is called the <b>Least Squares problem</b> because minimizing the squared norm is equivalent to minimizing the sum of the squares of the components of the error vector.</p>

<h3>4. Derivation of the Least Squares Solution</h3>
<p>To find the vector \\(\mathbf{x}\\) that minimizes this objective function, we use calculus. First, the objective function is expanded.</p>

<p><b>Step 1: Expand the Objective Function</b><br>
The squared norm \\(||\mathbf{v}||^2\\) is equal to the dot product \\(\mathbf{v}^T \mathbf{v}\\). Applying this to our error vector:</p>
\\[ f(\mathbf{x}) = ||\mathbf{y} - A\mathbf{x}||^2 = (\mathbf{y} - A\mathbf{x})^T (\mathbf{y} - A\mathbf{x}) \\]
<p>Using the property \\((B C)^T = C^T B^T\\), we get:</p>
\\[ f(\mathbf{x}) = (\mathbf{y}^T - \mathbf{x}^T A^T) (\mathbf{y} - A\mathbf{x}) \\]
<p>Expanding this product yields four terms:</p>
\\[ f(\mathbf{x}) = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T A\mathbf{x} - \mathbf{x}^T A^T \mathbf{y} + \mathbf{x}^T A^T A \mathbf{x} \\]
<p>The two middle terms, \\(\mathbf{y}^T A\mathbf{x}\\) and \\(\mathbf{x}^T A^T \mathbf{y}\\), are scalars. Since the transpose of a scalar is itself, they are equal: \\((\mathbf{x}^T A^T \mathbf{y})^T = \mathbf{y}^T (A^T)^T (\mathbf{x}^T)^T = \mathbf{y}^T A \mathbf{x}\\). Therefore, they can be combined:</p>
\\[ f(\mathbf{x}) = \mathbf{y}^T\mathbf{y} - 2\mathbf{x}^T A^T \mathbf{y} + \mathbf{x}^T A^T A \mathbf{x} \\]
<p>This is a quadratic function of \\(\mathbf{x}\\).</p>

<p><b>Step 2: Find the Minimum using the Gradient</b><br>
To find the minimum of \\(f(\mathbf{x})\\), we compute its gradient with respect to \\(\mathbf{x}\\) and set it to zero. We use the following standard vector calculus identities:</p>
<ul>
    <li>Gradient of a linear term: \\(\nabla_{\mathbf{x}} (\mathbf{x}^T \mathbf{c}) = \mathbf{c}\\)</li>
    <li>Gradient of a quadratic term: \\(\nabla_{\mathbf{x}} (\mathbf{x}^T P \mathbf{x}) = (P + P^T)\mathbf{x}\\). If \\(P\\) is symmetric (i.e., \\(P = P^T\\)), this simplifies to \\(2P\mathbf{x}\\).</li>
</ul>
<p>Applying this to our objective function:</p>
<ul>
    <li>The term \\(\mathbf{y}^T\mathbf{y}\\) is a constant with respect to \\(\mathbf{x}\\), so its gradient is \\(\mathbf{0}\\).</li>
    <li>The term \\(-2\mathbf{x}^T (A^T \mathbf{y})\\) is linear in \\(\mathbf{x}\\). Its gradient is \\(-2A^T\mathbf{y}\\).</li>
    <li>The term \\(\mathbf{x}^T (A^T A) \mathbf{x}\\) is quadratic. The matrix \\(P = A^T A\\) is symmetric, because \\((A^T A)^T = A^T (A^T)^T = A^T A\\). Therefore, its gradient is \\(2(A^T A)\mathbf{x}\\).</li>
</ul>
<p>The full gradient is:</p>
\\[ \nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{0} - 2A^T\mathbf{y} + 2A^T A \mathbf{x} \\]
<p>Setting the gradient to zero to find the minimum:</p>
\\[ -2A^T\mathbf{y} + 2A^T A \mathbf{x} = \mathbf{0} \\]
\\[ A^T A \mathbf{x} = A^T \mathbf{y} \\]
<p>This final equation is known as the <b>Normal Equation</b>.</p>

<p><b>Step 3: Solve for x</b><br>
Assuming the matrix \\(A^T A\\) is invertible, we can solve for \\(\mathbf{x}\\) by multiplying both sides by its inverse:</p>
\\[ \mathbf{x}_{LS} = (A^T A)^{-1} A^T \mathbf{y} \\]
<p>This is the <b>Least Squares (LS) solution</b>. The condition for \\(A^T A\\) to be invertible is that the matrix \\(A\\) must have <b>full column rank</b>, which means its columns must be linearly independent.</p>

<h3>5. The Pseudo-Inverse</h3>
<p>The expression derived for the LS solution contains a very important matrix:</p>
\\[ A^\dagger = (A^T A)^{-1} A^T \\]
<p>This matrix, denoted \\(A^\dagger\\) (read "A dagger"), is called the <b>pseudo-inverse</b> of \\(A\\) (specifically, the left pseudo-inverse).</p>

<p><b>Why is it called a "pseudo-inverse"?</b></p>
<ul>
    <li>It is not a true inverse because \\(A\\) is not square.</li>
    <li>However, it "acts" like an inverse from one side. If we multiply \\(A\\) on the left by \\(A^\dagger\\), we get the identity matrix:
        \\[ A^\dagger A = \left((A^T A)^{-1} A^T\right) A = (A^T A)^{-1} (A^T A) = I \\]
        Because it yields the identity matrix when multiplied on the left, it is more precisely called a <b>left inverse</b>.</li>
    <li>It is important to note that multiplying on the right does <i>not</i> generally yield the identity matrix (i.e., \\(A A^\dagger \neq I\\)).</li>
</ul>
<p>Using the pseudo-inverse notation, the least squares solution can be written compactly as:</p>
\\[ \mathbf{x}_{LS} = A^\dagger \mathbf{y} \\]
<p>This elegant form shows a beautiful parallel to the simple solution \\(\mathbf{x} = A^{-1}\mathbf{y}\\) for invertible square matrices. In fact, if \\(A\\) is square and invertible, the pseudo-inverse \\(A^\dagger\\) simplifies to the true inverse \\(A^{-1}\\), making the concept consistent across different types of systems.</p>
</div></div><div class="chapter" id="Lecture 22 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 22 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This module provides a deeper, more intuitive understanding of the least squares solution. Instead of relying purely on mathematical derivation, it explores the geometric interpretation of the problem, leading to the same result through the powerful <b>Principle of Orthogonality</b>.</p>

<h3>1. The Least Squares Problem Revisited</h3>
<p>The core problem is to find an approximate solution to an overdetermined system of linear equations of the form:</p>
\\[ \mathbf{y} = \mathbf{A}\mathbf{x} \\]
<p>Where:</p>
<ul>
    <li><b>A</b> is an \\(m \times n\\) matrix, typically a "tall" matrix where the number of rows \\(m\\) is greater than the number of columns \\(n\\) (\\(m > n\\)).</li>
    <li><b>x</b> is an \\(n \times 1\\) unknown vector.</li>
    <li><b>y</b> is an \\(m \times 1\\) measurement or observation vector.</li>
</ul>
<p>An exact solution exists only if the vector \\(\mathbf{y}\\) lies in the n-dimensional subspace spanned by the columns of matrix <b>A</b> (also known as the column space of <b>A</b>). When \\(\mathbf{y}\\) is outside this subspace, which is often the case in practice due to noise or modeling errors, no exact solution exists. The least squares method aims to find the "best" approximate solution.</p>

<h3>2. The Least Squares Objective and Solution</h3>
<p>The goal of least squares is to find the vector \\(\mathbf{x}\\) that minimizes the squared Euclidean norm (or length) of the error vector \\(\mathbf{e} = \mathbf{y} - \mathbf{A}\mathbf{x}\\). The problem is formally stated as:</p>
\\[ \arg\min_{\mathbf{x}} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|^2 \\]
<p>The solution to this minimization problem, as derived previously, is given by:</p>
\\[ \mathbf{x} = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{y} \\]
<p>The matrix \\((\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T\\) is known as the <b>pseudo-inverse</b> of <b>A</b>. This module explains <i>why</i> the solution has this specific structure.</p>

<h3>3. The Geometric Interpretation and the Principle of Orthogonality</h3>
<p>The key to understanding the least squares solution lies in its geometric interpretation.</p>

<b>a. The Column Space of A</b>
<p>The product \\(\mathbf{A}\mathbf{x}\\) can be expressed as a linear combination of the columns of <b>A</b> (let's denote them as \\(\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_n\\)):</p>
\\[ \mathbf{A}\mathbf{x} = x_1\mathbf{a}_1 + x_2\mathbf{a}_2 + \dots + x_n\mathbf{a}_n \\]
<p>This means that for any possible choice of \\(\mathbf{x}\\), the resulting vector \\(\mathbf{A}\mathbf{x}\\) will always lie within the subspace spanned by the columns of <b>A</b>.</p>

<b>b. Finding the Closest Vector</b>
<p>The least squares problem is equivalent to finding the vector \\(\mathbf{A}\mathbf{x}\\) in the column space of <b>A</b> that is closest to the given vector \\(\mathbf{y}\\). Intuitively, the distance between \\(\mathbf{y}\\) and the subspace is minimized when the error vector, \\(\mathbf{e} = \mathbf{y} - \mathbf{A}\mathbf{x}\\), is perpendicular (orthogonal) to the subspace itself.</p>
<p>Imagine a plane (a 2D subspace) in 3D space. The point on the plane closest to a point outside the plane is found by dropping a perpendicular from the point to the plane. The same principle applies here in higher dimensions.</p>

<b>c. The Principle of Orthogonality</b>
<p>This geometric insight is formalized as the <b>Principle of Orthogonality</b>. For the error \\(\|\mathbf{e}\|\\) to be minimum, the error vector \\(\mathbf{e}\\) must be orthogonal to every vector in the column space of <b>A</b>. It is sufficient to show that \\(\mathbf{e}\\) is orthogonal to all the basis vectors of the subspace, which are the columns of <b>A</b>.</p>
<p>This gives us \\(n\\) orthogonality conditions:</p>
\\[ \mathbf{a}_1^T \mathbf{e} = 0, \quad \mathbf{a}_2^T \mathbf{e} = 0, \quad \dots, \quad \mathbf{a}_n^T \mathbf{e} = 0 \\]
<p>These individual conditions can be consolidated into a single compact matrix equation:</p>
\\[ \mathbf{A}^T \mathbf{e} = \mathbf{0} \\]

<h3>4. Deriving the Normal Equations</h3>
<p>By substituting the definition of the error vector \\(\mathbf{e} = \mathbf{y} - \mathbf{A}\mathbf{x}\\) into the orthogonality condition, we get:</p>
\\[ \mathbf{A}^T (\mathbf{y} - \mathbf{A}\mathbf{x}) = \mathbf{0} \\]
<p>Distributing \\(\mathbf{A}^T\\) gives:</p>
\\[ \mathbf{A}^T \mathbf{y} - \mathbf{A}^T \mathbf{A}\mathbf{x} = \mathbf{0} \\]
<p>Rearranging this equation yields the celebrated <b>Normal Equations</b>:</p>
\\[ \mathbf{A}^T \mathbf{A}\mathbf{x} = \mathbf{A}^T \mathbf{y} \\]
<p>This result is derived directly from the geometric principle without any calculus. To solve for \\(\mathbf{x}\\), we can invert the matrix \\(\mathbf{A}^T \mathbf{A}\\), provided it is invertible. This is true if and only if the matrix <b>A</b> has <b>full column rank</b> (i.e., its columns are linearly independent).</p>

<h3>5. The Projection Matrix</h3>
<p>The best approximation to \\(\mathbf{y}\\) within the column space is the vector \\(\hat{\mathbf{y}} = \mathbf{A}\mathbf{x}\\). Geometrically, \\(\hat{\mathbf{y}}\\) is the projection of \\(\mathbf{y}\\) onto the column space of <b>A</b>. We can find an expression for this projection by substituting our solution for \\(\mathbf{x}\\):</p>
\\[ \hat{\mathbf{y}} = \mathbf{A}\mathbf{x} = \mathbf{A} \left( (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{y} \right) \\]
<p>If we group the matrices, we get:</p>
\\[ \hat{\mathbf{y}} = \left[ \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \right] \mathbf{y} \\]
<p>The matrix in the brackets is the <b>projection matrix</b>, denoted as \\(\mathbf{P_A}\\), which projects any vector \\(\mathbf{y}\\) onto the subspace spanned by the columns of <b>A</b>.</p>
\\[ \mathbf{P_A} = \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \\]

<b>Properties of the Projection Matrix</b>
<p>A fundamental property of a projection matrix is that it is <b>idempotent</b>, which means that applying the projection more than once has no further effect. Mathematically, this is expressed as:</p>
\\[ \mathbf{P_A}^2 = \mathbf{P_A} \\]
<p>This is intuitive: once a vector is projected into a subspace, it is already in that subspace. Projecting it again will not change it. The algebraic proof is as follows:</p>
\\[ \mathbf{P_A}^2 = \mathbf{P_A} \cdot \mathbf{P_A} = \left( \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \right) \left( \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \right) \\]
<p>The terms in the middle, \\(\mathbf{A}^T \mathbf{A}\\), cancel with their inverse \\((\mathbf{A}^T \mathbf{A})^{-1}\\) to become the identity matrix:</p>
\\[ \mathbf{P_A}^2 = \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \underbrace{(\mathbf{A}^T \mathbf{A}) (\mathbf{A}^T \mathbf{A})^{-1}}_{\mathbf{I}} \mathbf{A}^T = \mathbf{A} (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T = \mathbf{P_A} \\]

<h3>6. Extension to Complex Matrices</h3>
<p>For complex matrices, which are common in fields like wireless communications, the same principles apply. The only change is that the transpose operation (\\(T\\)) is replaced by the Hermitian (conjugate transpose) operation (\\(H\\)). The least squares solution becomes:</p>
\\[ \mathbf{x} = (\mathbf{A}^H \mathbf{A})^{-1} \mathbf{A}^H \mathbf{y} \\]

</div></div><div class="chapter" id="Lecture 23 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 23 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains how the least squares technique is applied to solve a practical problem in modern wireless communications, specifically in designing a receiver for a Multiple Input Multiple Output (MIMO) system. The explanation covers the system model, the problem faced by the receiver, the application of the least squares formula, and the interpretation of this solution as a "Zero-Forcing" receiver.</p>

<b>1. The MIMO System Model</b>
<p>A MIMO system is a wireless communication technology that uses multiple antennas at both the transmitter (multiple inputs) and the receiver (multiple outputs) to improve communication performance. The relationship between the transmitted signals, the wireless channel, and the received signals can be represented by a system of linear equations.</p>
<ul>
    <li>Let \\(t\\) be the number of transmit antennas (inputs) and \\(r\\) be the number of receive antennas (outputs).</li>
    <li>The system is considered <b>overdetermined</b> when the number of receive antennas is greater than the number of transmit antennas, i.e., \\(r > t\\).</li>
    <li>The transmitted signals (symbols) form a vector \\(\mathbf{x}\\) of size \\(t \times 1\\).</li>
    <li>The received signals form a vector \\(\mathbf{y}\\) of size \\(r \times 1\\).</li>
    <li>The wireless channel is represented by a channel matrix \\(H\\) of size \\(r \times t\\). Each element \\(h_{ij}\\) represents the channel gain (a complex number) between the j-th transmit antenna and the i-th receive antenna.</li>
    <li>The system is also affected by random noise, represented by a vector \\(\mathbf{n}\\) of size \\(r \times 1\\).</li>
</ul>
<p>The fundamental model for a MIMO system is given by the linear equation:</p>
\\[ \mathbf{y} = H\mathbf{x} + \mathbf{n} \\]
<p>In vector/matrix form, this looks like:</p>
\\[ \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_r \end{pmatrix} = \begin{pmatrix} h_{11} & h_{12} & \dots & h_{1t} \\ h_{21} & h_{22} & \dots & h_{2t} \\ \vdots & \vdots & \ddots & \vdots \\ h_{r1} & h_{r2} & \dots & h_{rt} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_t \end{pmatrix} + \begin{pmatrix} n_1 \\ n_2 \\ \vdots \\ n_r \end{pmatrix} \\]

<b>2. The Receiver's Task and the Least Squares Solution</b>
<p>The primary goal of the receiver is to estimate the originally transmitted symbol vector \\(\mathbf{x}\\) using the received signal vector \\(\mathbf{y}\\) and its knowledge of the channel matrix \\(H\\). We denote this estimate as \\(\hat{\mathbf{x}}\\).</p>
<p>If the system were square (\\(r = t\\)) and the matrix \\(H\\) were invertible, the solution would be straightforward: \\(\hat{\mathbf{x}} = H^{-1}\mathbf{y}\\) (ignoring noise). However, in an overdetermined system (\\(r > t\\)), \\(H\\) is not a square matrix and does not have a standard inverse.</p>
<p>This is precisely where the least squares technique becomes essential. The goal is to find an estimate \\(\hat{\mathbf{x}}\\) that minimizes the squared Euclidean norm of the error (or residual) vector, \\(\mathbf{y} - H\mathbf{x}\\). The problem is formulated as:</p>
\\[ \min_{\hat{\mathbf{x}}} ||\mathbf{y} - H\hat{\mathbf{x}}||^2 \\]
<p>The solution to this minimization problem is the least squares estimate. Since the channel matrix \\(H\\) is generally complex in wireless communications, we use the <b>Hermitian transpose</b> (conjugate transpose), denoted by \\(H^H\\), instead of the standard transpose.</p>
<p>The formula for the least squares estimate \\(\hat{\mathbf{x}}\\) is:</p>
\\[ \hat{\mathbf{x}} = (H^H H)^{-1} H^H \mathbf{y} \\]
<p>The matrix \\( (H^H H)^{-1} H^H \\) is known as the <b>pseudo-inverse</b> of \\(H\\), often denoted as \\(H^\dagger\\). So, the solution can be written compactly as:</p>
\\[ \hat{\mathbf{x}} = H^\dagger \mathbf{y} \\]
<p>In the context of MIMO receivers, this specific implementation of the least squares solution is called the <b>Zero-Forcing (ZF) Receiver</b>.</p>

<b>3. The Zero-Forcing (ZF) Receiver</b>
<p>The name "Zero-Forcing" comes from how this receiver handles interference between the different transmitted symbols.</p>
<p>In the original received signal \\(y_i\\), each component is a linear combination of <i>all</i> transmitted symbols, creating inter-symbol interference. For example:</p>
\\[ y_i = h_{i1}x_1 + h_{i2}x_2 + \dots + h_{it}x_t + n_i \\]
<p>Here, the signal from \\(x_2, x_3, \dots\\) acts as interference when trying to detect \\(x_1\\).</p>
<p>The ZF receiver applies the pseudo-inverse matrix to the received vector \\(\mathbf{y}\\). Let's see what happens when we substitute the system model \\(\mathbf{y} = H\mathbf{x} + \mathbf{n}\\) into the ZF receiver equation:</p>
\\[ \hat{\mathbf{x}} = (H^H H)^{-1} H^H (\mathbf{y}) \\]
\\[ \hat{\mathbf{x}} = (H^H H)^{-1} H^H (H\mathbf{x} + \mathbf{n}) \\]
<p>Distributing the matrix multiplication:</p>
\\[ \hat{\mathbf{x}} = \underbrace{(H^H H)^{-1} (H^H H)}_{I} \mathbf{x} + \underbrace{(H^H H)^{-1} H^H}_{H^\dagger} \mathbf{n} \\]
\\[ \hat{\mathbf{x}} = \mathbf{x} + H^\dagger \mathbf{n} \\]
<p>Let's call the processed output \\(\tilde{\mathbf{y}} = \hat{\mathbf{x}}\\) and the processed noise \\(\tilde{\mathbf{n}} = H^\dagger \mathbf{n}\\). The equation becomes:</p>
\\[ \tilde{\mathbf{y}} = \mathbf{x} + \tilde{\mathbf{n}} \\]
<p>In component form, this is \\(\tilde{y}_i = x_i + \tilde{n}_i\\). This result is significant: the i-th component of the processed output depends <i>only</i> on the i-th transmitted symbol \\(x_i\\) (plus some processed noise). The interference from all other symbols \\(x_j\\) (where \\(j \neq i\\)) has been completely removed. The receiver has effectively "forced" the inter-symbol interference to zero, hence the name.</p>

<b>4. Linear Receivers</b>
<p>The ZF receiver calculates the estimate \\(\hat{\mathbf{x}}\\) via a matrix multiplication: \\(\hat{\mathbf{x}} = C \mathbf{y}\\), where \\(C = H^\dagger\\). Since the output is a linear transformation of the input vector \\(\mathbf{y}\\), this type of receiver is known as a <b>linear receiver</b>. The ZF receiver is a prominent member of this class.</p>

<b>5. A Numerical Example</b>
<p>The transcript provides an example with a real-valued channel matrix (for simplicity) to demonstrate the calculations.</p>
<ul>
    <li><b>System Parameters:</b> \\(r=4\\) receive antennas, \\(t=2\\) transmit antennas.</li>
    <li><b>Channel Matrix:</b>
        \\[ H = \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix} \\]
    </li>
</ul>
<p><b>Step 1: Calculate \\(H^T H\\)</b> (Using transpose \\(H^T\\) as H is real)</p>
\\[ H^T H = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix} = \begin{pmatrix} 4 & 10 \\ 10 & 30 \end{pmatrix} \\]
<p><b>Step 2: Calculate the inverse \\((H^T H)^{-1}\\)</b></p>
<p>For a \\(2 \times 2\\) matrix \\( \begin{pmatrix} a & b \\ c & d \end{pmatrix} \\), the inverse is \\( \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix} \\).</p>
\\[ \det(H^T H) = (4)(30) - (10)(10) = 120 - 100 = 20 \\]
\\[ (H^T H)^{-1} = \frac{1}{20} \begin{pmatrix} 30 & -10 \\ -10 & 4 \end{pmatrix} \\]
<p><b>Step 3: Calculate the pseudo-inverse \\(H^\dagger = (H^T H)^{-1} H^T\\)</b></p>
\\[ H^\dagger = \frac{1}{20} \begin{pmatrix} 30 & -10 \\ -10 & 4 \end{pmatrix} \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{pmatrix} \\]
\\[ H^\dagger = \frac{1}{20} \begin{pmatrix} (30-10) & (30-20) & (30-30) & (30-40) \\ (-10+4) & (-10+8) & (-10+12) & (-10+16) \end{pmatrix} \\]
\\[ H^\dagger = \frac{1}{20} \begin{pmatrix} 20 & 10 & 0 & -10 \\ -6 & -2 & 2 & 6 \end{pmatrix} = \begin{pmatrix} 1 & 1/2 & 0 & -1/2 \\ -3/10 & -1/10 & 1/10 & 3/10 \end{pmatrix} \\]
<p><b>Step 4: Determine the symbol estimates</b></p>
<p>The final estimates \\(\hat{x}_1\\) and \\(\hat{x}_2\\) are found using \\(\hat{\mathbf{x}} = H^\dagger \mathbf{y}\\):</p>
\\[ \begin{pmatrix} \hat{x}_1 \\ \hat{x}_2 \end{pmatrix} = \begin{pmatrix} 1 & 1/2 & 0 & -1/2 \\ -3/10 & -1/10 & 1/10 & 3/10 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ y_4 \end{pmatrix} \\]
<p>This gives the explicit formulas for the symbol estimates as linear combinations of the received signals:</p>
\\[ \hat{x}_1 = y_1 + \frac{1}{2}y_2 - \frac{1}{2}y_4 \\]
\\[ \hat{x}_2 = -\frac{3}{10}y_1 - \frac{1}{10}y_2 + \frac{1}{10}y_3 + \frac{3}{10}y_4 \\]
<p>This example clearly illustrates how the abstract least squares formula is applied to produce a concrete signal processing algorithm for a state-of-the-art wireless communication system.</p>
</div></div><div class="chapter" id="Lecture 24 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 24 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the application of the least squares principle to channel estimation in wireless communication systems.</p>

<b>1. Introduction to Channel Estimation and MISO Systems</b>
<p>The core topic is <b>channel estimation</b>, a crucial process in wireless communication. It is explained in the context of a specific type of system called a <b>MISO (Multiple Input Single Output)</b> system.</p>
<ul>
    <li><b>MISO System:</b> This refers to a wireless setup where the transmitter has multiple antennas (Multiple Input) and the receiver has only one antenna (Single Output). This is common in scenarios like a base station with many antennas transmitting to a mobile phone with a single antenna (downlink), or a Wi-Fi router transmitting to a laptop.</li>
    <li><b>System Model:</b> A MISO system with \\(t\\) transmit antennas and one receive antenna is often called a \\(1 \times t\\) system. The communication path between each transmit antenna and the single receive antenna is characterized by a <b>channel coefficient</b>.</li>
</ul>
<p>If there are \\(t\\) transmit antennas, there will be \\(t\\) channel coefficients, denoted as \\(h_1, h_2, \dots, h_t\\). The term \\(h_i\\) represents the complex gain (attenuation and phase shift) of the signal traveling from transmit antenna \\(i\\) to the receive antenna. These coefficients can be grouped into a single channel vector:</p>
\\[ \bar{h} = \begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_t \end{bmatrix} \\]

<b>2. The Problem: Estimating Unknown Channels</b>
<p>In a real-world wireless environment, these channel coefficients \\(h_i\\) are unknown to the receiver. They are affected by factors like distance, obstacles, and movement. For the receiver to correctly decode the information sent by the transmitter, it must first determine or estimate these channel coefficients. This process is called <b>channel estimation</b>.</p>

<b>3. The Solution: Pilot Symbols</b>
<p>To enable channel estimation, the transmitter sends a set of known signals called <b>pilot symbols</b> or <b>training symbols</b>. These are not user data but are predetermined symbols known by both the transmitter and the receiver. By observing how these known pilot symbols are altered by the channel, the receiver can deduce the channel's characteristics.</p>
<ul>
    <li>A sequence of pilot symbols is called a <b>training sequence</b>.</li>
    <li>At each time instant \\(j\\), the transmitter sends a vector of pilot symbols, \\(\bar{x}_j\\), where each element corresponds to a different transmit antenna.
    \\[ \bar{x}_j = \begin{bmatrix} x_{1j} \\ x_{2j} \\ \vdots \\ x_{tj} \end{bmatrix} \\]
    Here, \\(x_{ij}\\) is the pilot symbol transmitted from antenna \\(i\\) at time instant \\(j\\).
    </li>
</ul>

<b>4. Mathematical Model for Channel Estimation</b>
<p>The signal received at a specific time instant is a linear combination of the symbols transmitted from all antennas, each weighted by its respective channel coefficient, plus some additive noise. For time instant \\(j=1\\), the received signal \\(y_1\\) is:</p>
\\[ y_1 = x_{11}h_1 + x_{21}h_2 + \dots + x_{t1}h_t + n_1 \\]
<p>where \\(n_1\\) is the noise sample at that time. This can be written more compactly using vector notation:</p>
\\[ y_1 = \begin{bmatrix} x_{11} & x_{21} & \dots & x_{t1} \end{bmatrix} \begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_t \end{bmatrix} + n_1 = \bar{x}_1^T \bar{h} + n_1 \\]
<p>This process is repeated for \\(L\\) time instants, using \\(L\\) different pilot vectors (\\(\bar{x}_1, \bar{x}_2, \dots, \bar{x}_L\\)). This generates a system of \\(L\\) linear equations:</p>
\\[ y_1 = \bar{x}_1^T \bar{h} + n_1 \\]
\\[ y_2 = \bar{x}_2^T \bar{h} + n_2 \\]
\\[ \vdots \\]
\\[ y_L = \bar{x}_L^T \bar{h} + n_L \\]
<p>This entire system can be consolidated into a single matrix equation, which forms the basis for the least squares problem:</p>
\\[ \bar{y} = X \bar{h} + \bar{n} \\]
where:
<ul>
    <li>\\(\bar{y} = [y_1, y_2, \dots, y_L]^T\\) is the \\(L \times 1\\) vector of received observations.</li>
    <li>\\(\bar{h} = [h_1, h_2, \dots, h_t]^T\\) is the \\(t \times 1\\) unknown channel vector we want to estimate.</li>
    <li>\\(\bar{n} = [n_1, n_2, \dots, n_L]^T\\) is the \\(L \times 1\\) noise vector.</li>
    <li>\\(X\\) is the \\(L \times t\\) <b>pilot matrix</b>, constructed by stacking the transposed pilot vectors:
    \\[ X = \begin{bmatrix} \bar{x}_1^T \\ \bar{x}_2^T \\ \vdots \\ \bar{x}_L^T \end{bmatrix} = \begin{bmatrix} x_{11} & x_{21} & \dots & x_{t1} \\ x_{12} & x_{22} & \dots & x_{t2} \\ \vdots & \vdots & \ddots & \vdots \\ x_{1L} & x_{2L} & \dots & x_{tL} \end{bmatrix} \\]
    </li>
</ul>

<b>5. Least Squares Channel Estimation</b>
<p>The linear model \\(\bar{y} = X \bar{h} + \bar{n}\\) is in the standard form for a least squares problem. The goal is to find the estimate of the channel, \\(\hat{h}\\), that minimizes the squared error between the observed outputs \\(\bar{y}\\) and the outputs predicted by the model \\(X\bar{h}\\). This is known as minimizing the least squares cost function: \\( ||\bar{y} - X\bar{h}||^2 \\).</p>
<p>The solution to this minimization problem is the well-known least squares formula:</p>
\\[ \hat{h} = (X^H X)^{-1} X^H \bar{y} \\]
<p>Here, \\(X^H\\) denotes the Hermitian (conjugate transpose) of \\(X\\). If the pilot symbols and channel coefficients are real numbers, this simplifies to the regular transpose (\\(X^T\\)).</p>
<p>This estimate \\(\hat{h}\\) is also the <b>Maximum Likelihood (ML) estimate</b> if the noise samples \\(n_i\\) are assumed to be independent and identically distributed (i.i.d.) Gaussian random variables.</p>

<b>6. Example of Channel Estimation</b>
<p>The transcript provides an example for a \\(2 \times 1\\) MISO system (\\(t=2\\)) with \\(L=4\\) pilot transmissions.</p>
<ul>
    <li><b>Pilot Vectors:</b> \\(\bar{x}_1=[2, -2]^T, \bar{x}_2=[3, 2]^T, \bar{x}_3=[2, -3]^T, \bar{x}_4=[2, 2]^T\\)</li>
    <li><b>Observation Vector:</b> \\(\bar{y} = [-1, 2, -3, 2]^T\\)</li>
</ul>
<p><b>Step 1: Form the Pilot Matrix X</b></p>
\\[ X = \begin{bmatrix} \bar{x}_1^T \\ \bar{x}_2^T \\ \bar{x}_3^T \\ \bar{x}_4^T \end{bmatrix} = \begin{bmatrix} 2 & -2 \\ 3 & 2 \\ 2 & -3 \\ 2 & 2 \end{bmatrix} \\]
<p>An important property of this specific pilot matrix is that its columns are <b>orthogonal</b>. The dot product of the first and second columns is \\( (2)( -2) + (3)(2) + (2)(-3) + (2)(2) = -4 + 6 - 6 + 4 = 0 \\). Using orthogonal pilots simplifies computation significantly.</p>
<p><b>Step 2: Calculate \\(X^T X\\)</b></p>
\\[ X^T X = \begin{bmatrix} 2 & 3 & 2 & 2 \\ -2 & 2 & -3 & 2 \end{bmatrix} \begin{bmatrix} 2 & -2 \\ 3 & 2 \\ 2 & -3 \\ 2 & 2 \end{bmatrix} = \begin{bmatrix} 21 & 0 \\ 0 & 21 \end{bmatrix} = 21I \\]
<p>Because the columns are orthogonal, \\(X^T X\\) is a diagonal matrix. This makes its inverse trivial to compute:</p>
\\[ (X^T X)^{-1} = \frac{1}{21} I = \begin{bmatrix} 1/21 & 0 \\ 0 & 1/21 \end{bmatrix} \\]

<p><b>Step 3: Calculate \\(X^T \bar{y}\\)</b></p>
\\[ X^T \bar{y} = \begin{bmatrix} 2 & 3 & 2 & 2 \\ -2 & 2 & -3 & 2 \end{bmatrix} \begin{bmatrix} -1 \\ 2 \\ -3 \\ 2 \end{bmatrix} = \begin{bmatrix} -2+6-6+4 \\ 2+4+9+4 \end{bmatrix} = \begin{bmatrix} 2 \\ 19 \end{bmatrix} \\]

<p><b>Step 4: Compute the Channel Estimate \\(\hat{h}\\)</b></p>
\\[ \hat{h} = (X^T X)^{-1} (X^T \bar{y}) = \frac{1}{21} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 19 \end{bmatrix} = \frac{1}{21} \begin{bmatrix} 2 \\ 19 \end{bmatrix} = \begin{bmatrix} 2/21 \\ 19/21 \end{bmatrix} \\]
<p>Thus, the least squares estimate for the channel vector is \\(\hat{h} = [2/21, 19/21]^T\\).</p>

<b>7. Generalization to System Identification</b>
<p>The concept of channel estimation is a specific application of a broader field called <b>system identification</b>. The general problem is to determine the characteristics of an unknown system (a "black box").</p>
<ul>
    <li>The <b>unknown channel</b> \\(\bar{h}\\) corresponds to the <b>unknown system</b>.</li>
    <li>The <b>pilot symbols</b> correspond to known <b>probing signals</b> sent into the system.</li>
    <li>The <b>received observations</b> correspond to the measured <b>outputs</b> of the system.</li>
</ul>
<p>By analyzing the relationship between the known inputs and the measured outputs, one can identify the system's properties. The least squares principle is a fundamental and powerful tool for solving such linear system identification problems.</p>
</div></div><div class="chapter" id="Lecture 25 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 25 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This module provides an introduction to <b>Linear Regression</b>, framing it as a key application of the least squares method within the field of Machine Learning (ML).</p>

<h3>1. What is Linear Regression?</h3>
<p>
Linear regression is a predictive modeling technique used to establish a relationship between a dependent variable and one or more independent variables. The core idea is to predict a continuous outcome, referred to as the <b>response</b>, by using a set of <b>explanatory variables</b>.
</p>
<p>
Imagine plotting data points on a 2D graph, with an explanatory variable \\(x\\) on the horizontal axis and a response variable \\(y\\) on the vertical axis. If the points show a clear trend, linear regression aims to fit a straight line (a "linear model") through these points. This line represents the learned relationship. Once this model is established, you can take a new value for the explanatory variable \\(x'\\) and use the line to predict its corresponding response, \\(\hat{y}\\).
</p>
<p>
The "learning" aspect of this machine learning technique is the process of finding the best possible linear model that fits the given data.
</p>

<h3>2. The Linear Regression Model</h3>
<p>The relationship between the variables is expressed through a mathematical equation. Let's define the components:</p>
<ul>
    <li><b>Response (y):</b> This is the single variable we are trying to predict. It is also known as the output, observation, or the <i>dependent variable</i>.</li>
    <li><b>Explanatory Variables (\\(x_1, x_2, ..., x_n\\)):</b> These are the variables used to predict the response. They are also known as features, predictors, or <i>independent variables</i>.</li>
</ul>
<p>
The general form of the linear regression model is given by:
</p>
\\[ y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n + \epsilon \\]
<p>Let's break down the components of this formula:</p>
<ul>
    <li><b>Regression Coefficients (\\(\theta_1, \theta_2, ..., \theta_n\\)):</b> These are the parameters or weights that quantify the effect each explanatory variable \\(x_i\\) has on the response \\(y\\).</li>
    <li><b>Intercept (\\(\theta_0\\)):</b> This is a constant term, also known as the <i>bias</i>. It represents the expected value of the response \\(y\\) when all explanatory variables are zero.</li>
    <li><b>Error Term (\\(\epsilon\\)):</b> This term, also called the <i>disturbance</i>, accounts for the variability in \\(y\\) that cannot be explained by the linear combination of the explanatory variables. It represents the modeling error or noise in the data.</li>
</ul>

<h3>3. Vector and Matrix Formulation</h3>
<p>
The linear model can be expressed more compactly using vector notation. This is where linear algebra becomes crucial. We define two vectors:
</p>
<ul>
    <li>An augmented vector of explanatory variables: \\(\bar{x} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\\)</li>
    <li>A vector of regression parameters: \\(\bar{\theta} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{bmatrix}\\)</li>
</ul>
<p>
Using these vectors, the model for a single observation simplifies to:
</p>
\\[ y = \bar{x}^T \bar{\theta} + \epsilon \\]
<p>
Once the model parameters \\(\bar{\theta}\\) are determined, we can make predictions. For a new set of explanatory variables \\(\bar{x}\\), the predicted response, denoted as \\(\hat{y}\\), is calculated by ignoring the error term:
</p>
\\[ \hat{y} = \bar{x}^T \bar{\theta} \\]

<h3>4. Applications of Linear Regression</h3>
<p>Linear regression is a versatile tool used in numerous fields. The transcript highlights several examples:</p>
<ul>
    <li><b>Stock Market Prediction:</b> Predict the price of a stock (response \\(y\\)) based on the prices of related stocks or a market index (explanatory variables \\(x_1, ..., x_n\\)).</li>
    <li><b>Sales Forecasting:</b> Predict the sales of a product, like SUVs (response \\(y\\)), using factors like sales of other vehicles (cars, bikes) and economic indicators such as average income or GDP (explanatory variables).</li>
    <li><b>Financial Risk Management:</b> Estimate a portfolio's risk (response \\(y\\)) based on variables like interest rates, exchange rates, and individual stock prices.</li>
    <li>Other applications mentioned include housing price prediction and traffic prediction.</li>
</ul>

<h3>5. Determining the Regression Parameters with Least Squares</h3>
<p>
The central question is how to find the optimal values for the regression coefficients in the vector \\(\bar{\theta}\\). This is done using a set of historical data known as <b>training data</b>. This dataset consists of \\(m\\) observations, where for each observation, we know both the explanatory variables and the actual response.
</p>
<p>
Our training data is a set of \\(m\\) pairs: \\( (y_1, \bar{x}_1), (y_2, \bar{x}_2), \dots, (y_m, \bar{x}_m) \\).
</p>
<p>We can write out the linear model for each of these \\(m\\) data points:</p>
\\[ y_1 = \bar{x}_1^T \bar{\theta} + \epsilon_1 \\]
\\[ y_2 = \bar{x}_2^T \bar{\theta} + \epsilon_2 \\]
\\[ \vdots \\]
\\[ y_m = \bar{x}_m^T \bar{\theta} + \epsilon_m \\]
<p>This system of equations can be written in a single, compact matrix equation:</p>
\\[ \bar{y} = X \bar{\theta} + \bar{\epsilon} \\]
<p>Where:</p>
<ul>
    <li>\\(\bar{y}\\) is the \\(m \times 1\\) vector of observed responses: \\(\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}\\)</li>
    <li>\\(X\\) is the \\(m \times (n+1)\\) matrix of explanatory variables, where each row is one observation's vector \\(\bar{x}_i^T\\): \\(\begin{bmatrix} \bar{x}_1^T \\ \bar{x}_2^T \\ \vdots \\ \bar{x}_m^T \end{bmatrix} = \begin{bmatrix} 1 & x_{11} & \dots & x_{n1} \\ 1 & x_{12} & \dots & x_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1m} & \dots & x_{nm} \end{bmatrix}\\)</li>
    <li>\\(\bar{\theta}\\) is the \\((n+1) \times 1\\) vector of unknown regression coefficients we want to find.</li>
    <li>\\(\bar{\epsilon}\\) is the \\(m \times 1\\) vector of errors for each observation.</li>
</ul>

<p>
The goal is to find the vector \\(\bar{\theta}\\) that provides the "best fit" for the training data. This is achieved by finding the \\(\bar{\theta}\\) that minimizes the overall error between the predicted responses (\\(X\bar{\theta}\\)) and the actual responses (\\(\bar{y}\\)). This is precisely a <b>least squares problem</b>, where we aim to minimize the sum of the squared errors, \\( ||\bar{y} - X\bar{\theta}||^2 \\).
</p>
<p>
The solution to this least squares problem gives us the optimal vector of regression coefficients, denoted \\(\hat{\theta}\\):
</p>
\\[ \hat{\theta} = (X^T X)^{-1} X^T \bar{y} \\]
<p>
By applying this formula, we use the training data (\\(X\\) and \\(\bar{y}\\)) to "learn" the best regression parameters. Once \\(\hat{\theta}\\) is calculated, the linear model is complete and can be used to make predictions on new, unseen data.
</p>
</div></div><h2>Weekly Summary</h2><div><p>This week's modules introduce the fundamental concept of the <b>Least Squares (LS) solution</b>, its geometric interpretation, and its wide-ranging applications in wireless communications and machine learning.</p><p><b>1. The Least Squares Problem and Solution</b></p><p>The core topic is finding an approximate solution to an overdetermined system of linear equations \\(\mathbf{y} = A\mathbf{x}\\), where the matrix \\(A\\) is \\(m \times n\\) with \\(m > n\\) (more equations than unknowns). Such systems, often called <i>tall matrices</i>, typically have no exact solution because the vector \\(\mathbf{y}\\) does not lie in the column space of \\(A\\).</p><ul><li><b>Key Takeaway:</b> Instead of an exact solution, we seek the vector \\(\mathbf{x}\\) that minimizes the squared norm of the error, \\(||\mathbf{y} - A\mathbf{x}||^2\\). This is the <b>Least Squares problem</b>.</li><li><b>Key Takeaway:</b> The LS solution is derived by setting the gradient of the error function to zero. This leads to the <b>normal equations</b>:<br>\\[ A^T A \mathbf{x} = A^T \mathbf{y} \\]</li><li><b>Key Takeaway:</b> The solution to the normal equations gives the Least Squares solution:<br>\\[ \mathbf{x}_{LS} = (A^T A)^{-1} A^T \mathbf{y} \\]<br>This solution requires that \\(A^T A\\) is invertible, which holds if \\(A\\) has full column rank. The matrix \\(A^\dagger = (A^T A)^{-1} A^T\\) is known as the <b>pseudo-inverse</b> of \\(A\\).</li></ul><p><b>2. Geometric Interpretation of Least Squares</b></p><p>A deeper, intuitive understanding of the LS solution is provided through a geometric lens.</p><ul><li><b>Key Takeaway:</b> The vector \\(A\mathbf{x}_{LS}\\) is the point in the column space of \\(A\\) that is closest to the vector \\(\mathbf{y}\\). This closest point is the <b>orthogonal projection</b> of \\(\mathbf{y}\\) onto the column space.</li><li><b>Key Takeaway:</b> The distance is minimized when the error vector \\(\mathbf{e} = \mathbf{y} - A\mathbf{x}_{LS}\\) is orthogonal to the column space of \\(A\\). This is the <b>Principle of Orthogonality</b>, which provides an alternative way to derive the normal equations (\\(A^T\mathbf{e} = \mathbf{0}\\)).</li><li><b>Key Takeaway:</b> The matrix \\(P_A = A(A^T A)^{-1} A^T\\) is the <b>projection matrix</b> that projects any vector onto the column space of \\(A\\). It is an idempotent matrix, meaning \\(P_A^2 = P_A\\).</li></ul><p><b>3. Applications of Least Squares</b></p><p>The power of the LS technique is demonstrated through several practical applications.</p><ul><li><b>MIMO Wireless Communication:</b><br>In a Multiple-Input Multiple-Output (MIMO) system modeled as \\(\mathbf{y} = H\mathbf{x} + \mathbf{n}\\), the receiver must estimate the transmitted symbols \\(\mathbf{x}\\).<br><i>Key Takeaway:</i> The <b>Zero-Forcing (ZF) receiver</b> is a direct application of the LS solution, where the estimate is \\(\hat{\mathbf{x}} = (H^H H)^{-1} H^H \mathbf{y}\\). It is a form of linear receiver that forces interference between symbols to zero.</li><br><li><b>Channel Estimation:</b><br>In wireless systems, the channel coefficients (\\(\mathbf{h}\\)) are often unknown and must be estimated. This is done by transmitting known "pilot symbols" and observing the output.<br><i>Key Takeaway:</i> The channel estimation problem is formulated as a standard LS problem \\(\mathbf{y} = X\mathbf{h} + \mathbf{n}\\), where \\(X\\) is the matrix of pilot symbols. The LS estimate for the channel is \\(\hat{\mathbf{h}} = (X^H X)^{-1} X^H \mathbf{y}\\). This general technique is also known as <b>system identification</b>.</li><br><li><b>Machine Learning - Linear Regression:</b><br>Linear regression is a core machine learning technique used to predict a response variable (\\(y\\)) based on a linear combination of explanatory variables (\\(x_1, \dots, x_n\\)).<br><i>Key Takeaway:</i> The unknown regression coefficients (\\(\boldsymbol{\theta}\\)) of the model are learned from a training dataset by framing the problem as a least squares task: find \\(\boldsymbol{\theta}\\) to minimize \\(||\mathbf{y} - X\boldsymbol{\theta}||^2\\). The solution is \\(\hat{\boldsymbol{\theta}} = (X^T X)^{-1} X^T \mathbf{y}\\), which provides the "best fit" linear model for the data.</li></ul></div><h2>Assignment Explanation</h2><div>
<p>Here is an explanation of the questions and answers from the assignment.</p>
<hr>

<h3>Question 1</h3>
<p><b>Question:</b> Consider a matrix A of size \\(3 \times 8\\). The Rank+Nullity of this matrix equals</p>
<p><b>Answer:</b> 8</p>
<p><b>Explanation:</b> This question is a direct application of the <b>Rank-Nullity Theorem</b>. The theorem states that for any matrix with \\(n\\) columns, the sum of its rank and its nullity is equal to the number of columns.</p>
<p>
    The rank is the dimension of the column space (number of linearly independent columns), and the nullity is the dimension of the null space (the space of vectors \\(\mathbf{x}\\) such that \\(A\mathbf{x} = 0\\)).
</p>
<p>
    For the given matrix \\(A\\) of size \\(3 \times 8\\), the number of columns is \\(n=8\\). Therefore:
    \\[ \text{rank}(A) + \text{nullity}(A) = 8 \\]
</p>

<hr>
<h3>Question 2</h3>
<p><b>Question:</b> Consider an Alamouti coded system with channel coefficients \\(h_1 = 1 + j\\) and \\(h_2 = -1 -j\\). The effective channel matrix for this system is given as</p>
<p><b>Answer:</b> 
\\[ \begin{bmatrix} 1+j & -1+j \\ -1-j & 1-j \end{bmatrix} \\]
</p>
<p><b>Explanation:</b> In Alamouti's space-time block coding, the received signal can be manipulated to form an equivalent linear system \\(\mathbf{y} = H_{eff} \mathbf{s}\\), where \\(\mathbf{s}\\) is the vector of transmitted symbols and \\(H_{eff}\\) is the effective channel matrix. A common form for this matrix is:</p>
\\[ H_{eff} = \begin{bmatrix} h_1 & h_2^* \\ h_2 & -h_1^* \end{bmatrix} \\]
<p>However, another valid representation, which is used here, is:</p>
\\[ H_{eff} = \begin{bmatrix} h_1 & h_2^* \\ h_2 & h_1^* \end{bmatrix} \\]
<p>Let's calculate the components using the given channel coefficients:</p>
<ul>
    <li>\\(h_1 = 1 + j\\)</li>
    <li>\\(h_2 = -1 - j\\)</li>
    <li>The complex conjugate of \\(h_1\\) is \\(h_1^* = 1 - j\\).</li>
    <li>The complex conjugate of \\(h_2\\) is \\(h_2^* = -1 + j\\).</li>
</ul>
<p>Substituting these values into the matrix structure gives:</p>
\\[ H_{eff} = \begin{bmatrix} 1+j & -1+j \\ -1-j & 1-j \end{bmatrix} \\]
<p>This matches the accepted answer.</p>

<hr>
<h3>Question 3</h3>
<p><b>Question:</b> The picture shown corresponds to</p>
<p><b>Answer:</b> Regression</p>
<p><b>Explanation:</b> The image displays a set of data points (a scatter plot) and a straight line drawn through them. This line represents the "line of best fit." The process of finding a function (in this case, a line) that best models the relationship between variables in a dataset is called <b>regression</b>. Specifically, this is an example of linear regression.</p>

<hr>
<h3>Question 4</h3>
<p><b>Question:</b> Consider the matrix A below. The pseudo-inverse of the matrix A is
\\[ A = \begin{bmatrix} 1 & -1 \\ 1 & 1 \\ -1 & 1 \\ -1 & -1 \end{bmatrix} \\]
</p>
<p><b>Answer:</b> 
\\[ \frac{1}{4} \begin{bmatrix} 1 & 1 & -1 & -1 \\ -1 & 1 & 1 & -1 \end{bmatrix} \\]
</p>
<p><b>Explanation:</b> The pseudo-inverse \\(A^+\\) of a matrix \\(A\\) is given by the formula \\(A^+ = (A^H A)^{-1} A^H\\), where \\(A^H\\) is the conjugate transpose (or just the transpose \\(A^T\\) for real matrices).</p>
<p>1. <b>Check if columns are orthogonal.</b> Let the columns be \\(\mathbf{c_1} = [1, 1, -1, -1]^T\\) and \\(\mathbf{c_2} = [-1, 1, 1, -1]^T\\).
    \\[ \mathbf{c_1}^T \mathbf{c_2} = (1)(-1) + (1)(1) + (-1)(1) + (-1)(-1) = -1 + 1 - 1 + 1 = 0 \\]
    The columns are orthogonal. This simplifies the calculation of \\(A^T A\\).</p>
<p>2. <b>Calculate \\(A^T A\\):</b>
    \\[ A^T A = \begin{bmatrix} \mathbf{c_1}^T \mathbf{c_1} & \mathbf{c_1}^T \mathbf{c_2} \\ \mathbf{c_2}^T \mathbf{c_1} & \mathbf{c_2}^T \mathbf{c_2} \end{bmatrix} = \begin{bmatrix} 1^2+1^2+(-1)^2+(-1)^2 & 0 \\ 0 & (-1)^2+1^2+1^2+(-1)^2 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} = 4I \\]
</p>
<p>3. <b>Calculate \\((A^T A)^{-1}\\):</b>
    \\[ (A^T A)^{-1} = (4I)^{-1} = \frac{1}{4}I = \begin{bmatrix} 1/4 & 0 \\ 0 & 1/4 \end{bmatrix} \\]
</p>
<p>4. <b>Calculate the pseudo-inverse \\(A^+ = (A^T A)^{-1} A^T\\):</b>
    \\[ A^+ = \frac{1}{4}I \cdot A^T = \frac{1}{4} A^T = \frac{1}{4} \begin{bmatrix} 1 & 1 & -1 & -1 \\ -1 & 1 & 1 & -1 \end{bmatrix} \\]
</p>

<hr>
<h3>Question 5</h3>
<p><b>Question:</b> The eigenvalues of a Hermitian symmetric matrix are</p>
<p><b>Answer:</b> Real but not necessarily positive</p>
<p><b>Explanation:</b> A Hermitian matrix is a square matrix that is equal to its own conjugate transpose (\\(H = H^H\\)). A key property of Hermitian matrices is that all of their eigenvalues are real numbers. They are not guaranteed to be positive. For an eigenvalue to be positive, the matrix must also be positive definite.</p>

<hr>
<h3>Question 6</h3>
<p><b>Question:</b> Consider the Gaussian classification problem with the two classes distributed as \\(C_1 \sim \mathcal{N}(\bar{\mu}_1, \Sigma)\\) and \\(C_2 \sim \mathcal{N}(\bar{\mu}_2, \Sigma)\\). The classifier for this problem, to classify a new vector \\(\bar{x}\\), can be formulated as</p>
<p><b>Answer:</b> Choose \\(C_1\\) if \\((\bar{\mu}_1 - \bar{\mu}_2)^T \Sigma^{-1} \bar{x} - \frac{1}{2}(\bar{\mu}_1^T \Sigma^{-1} \bar{\mu}_1 - \bar{\mu}_2^T \Sigma^{-1} \bar{\mu}_2) \geq 0\\) and \\(C_2\\) otherwise</p>
<p><b>Explanation:</b> This is a problem of Linear Discriminant Analysis (LDA). When the classes have Gaussian distributions with the same covariance matrix \\(\Sigma\\) and equal prior probabilities, the decision rule is to choose the class whose mean is "closer" to the new point \\(\bar{x}\\) in a statistical sense (Mahalanobis distance). We choose class \\(C_1\\) if its posterior probability is higher than that of \\(C_2\\). This leads to the decision boundary:</p>
<p>\\[ (\bar{x} - \bar{\mu}_2)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_2) \geq (\bar{x} - \bar{\mu}_1)^T \Sigma^{-1} (\bar{x} - \bar{\mu}_1) \\]
Expanding and simplifying this inequality gives the linear decision rule shown in the answer. The expression represents a hyperplane that separates the two classes.</p>

<hr>
<h3>Question 7</h3>
<p><b>Question:</b> The picture shown corresponds to</p>
<p><b>Answer:</b> Classification</p>
<p><b>Explanation:</b> The image shows two distinct groups or "clusters" of data points. A line is drawn to separate these two groups. The task of creating a model or rule (the line) to assign new data points to one of these predefined categories (the clusters) is known as <b>classification</b>. The line is the decision boundary.</p>

<hr>
<h3>Question 8</h3>
<p><b>Question:</b> The eigenvalues \\(\lambda_i\\) of a unitary matrix \\(U\\) satisfy the property</p>
<p><b>Answer:</b> \\(|\lambda_i| = 1\\)</p>
<p><b>Explanation:</b> A unitary matrix \\(U\\) is defined by the property \\(U^H U = I\\), where \\(U^H\\) is the conjugate transpose. Unitary matrices preserve the norm of a vector. Let \\(\mathbf{v}\\) be an eigenvector of \\(U\\) with eigenvalue \\(\lambda\\). Then:</p>
<p>\\[ U\mathbf{v} = \lambda\mathbf{v} \\]</p>
<p>Taking the norm of both sides:</p>
<p>\\[ ||U\mathbf{v}|| = ||\lambda\mathbf{v}|| = |\lambda| \cdot ||\mathbf{v}|| \\]</p>
<p>Since \\(U\\) preserves norms, \\(||U\mathbf{v}|| = ||\mathbf{v}||\\). Therefore:</p>
<p>\\[ ||\mathbf{v}|| = |\lambda| \cdot ||\mathbf{v}|| \\]</p>
<p>Because an eigenvector \\(\mathbf{v}\\) is non-zero, we can divide by \\(||\mathbf{v}||\\) to get \\(|\lambda| = 1\\). This means all eigenvalues of a unitary matrix lie on the unit circle in the complex plane.</p>

<hr>
<h3>Question 9</h3>
<p><b>Question:</b> The eigenvalues of the matrix below are \\( A = \begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix} \\)</p>
<p><b>Answer:</b> 4, -1</p>
<p><b>Explanation:</b> The eigenvalues \\(\lambda\\) are the roots of the characteristic equation \\(\det(A - \lambda I) = 0\\).</p>
<p>1. <b>Set up the characteristic equation:</b>
\\[ \det \left( \begin{bmatrix} 2 & 3 \\ 2 & 1 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \right) = 0 \\]
\\[ \det \left( \begin{bmatrix} 2-\lambda & 3 \\ 2 & 1-\lambda \end{bmatrix} \right) = 0 \\]
</p>
<p>2. <b>Calculate the determinant:</b>
\\[ (2-\lambda)(1-\lambda) - (3)(2) = 0 \\]
\\[ 2 - 2\lambda - \lambda + \lambda^2 - 6 = 0 \\]
\\[ \lambda^2 - 3\lambda - 4 = 0 \\]
</p>
<p>3. <b>Solve the quadratic equation for \\(\lambda\\):</b>
\\[ (\lambda - 4)(\lambda + 1) = 0 \\]
The solutions (eigenvalues) are \\(\lambda_1 = 4\\) and \\(\lambda_2 = -1\\).</p>

<hr>
<h3>Question 10</h3>
<p><b>Question:</b> For a square matrix A, the eigenvalues \\(\lambda\\) are given as solution to the equation</p>
<p><b>Answer:</b> \\(|A - \lambda I| = 0\\)</p>
<p><b>Explanation:</b> This is the definition of the characteristic equation. By definition, an eigenvalue \\(\lambda\\) of a matrix \\(A\\) is a scalar for which there exists a non-zero eigenvector \\(\mathbf{v}\\) such that:</p>
<p>\\[ A\mathbf{v} = \lambda\mathbf{v} \\]</p>
<p>This can be rearranged as:</p>
<p>\\[ A\mathbf{v} - \lambda I\mathbf{v} = 0 \\]
\\[ (A - \lambda I)\mathbf{v} = 0 \\]</p>
<p>For this equation to have a non-trivial solution for \\(\mathbf{v}\\) (i.e., \\(\mathbf{v} \neq 0\\)), the matrix \\((A - \lambda I)\\) must be singular. A square matrix is singular if and only if its determinant is zero. Therefore, we must solve for \\(\lambda\\) in the equation:</p>
\\[ \det(A - \lambda I) = 0 \\]
<p>This is commonly written using vertical bars as \\(|A - \lambda I| = 0\\).</p>

</div></div><div class="week" id="week_6"><h1 class="week-title">Week 6</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 26 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 26 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas discussed in the transcript regarding polynomial fitting using the method of least squares.</p>

<h3>1. Introduction to Polynomial Fitting</h3>
<p>The core problem is to find a mathematical function that best fits a given set of data points. In this context, we have data from an unknown functional relationship, denoted as \\( y = f(x) \\), where the function \\(f\\) itself is not known. We are given a set of \\(m\\) data points or "training points": \\((x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\\).</p>
<p><b>Polynomial fitting</b>, or <b>polynomial approximation</b>, is a technique used to find a polynomial function that closely approximates this unknown function \\(f(x)\\) based on the available data points. The goal is to create a model, \\(\hat{f}(x)\\), that can predict the value of \\(y\\) for a given \\(x\\).</p>

<h3>2. The Polynomial Model</h3>
<p>We propose to approximate the unknown relationship with an n-th degree polynomial. This approximation is denoted as \\(\hat{f}(x)\\) and is defined as:</p>
\\[ \hat{f}(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \\]
<p>Here:</p>
<ul>
    <li>\\(n\\) is the <b>degree</b> or <b>order</b> of the polynomial.</li>
    <li>\\(a_0, a_1, \dots, a_n\\) are the unknown <b>coefficients</b> of the polynomial. The primary task is to determine the optimal values for these coefficients.</li>
</ul>
<p>A special case highlighted in the transcript is when \\(n=1\\). The model becomes:</p>
\\[ \hat{f}(x) = a_0 + a_1 x \\]
<p>This is the equation of a straight line, which corresponds to the <b>linear fit</b> or <b>linear regression</b> model. Polynomial fitting is thus a generalization of linear regression, allowing for more complex, non-linear relationships to be modeled.</p>

<h3>3. Formulating the Problem for Least Squares</h3>
<p>To find the coefficients \\(a_0, \dots, a_n\\), we use the \\(m\\) available data points. For each data point \\((x_i, y_i)\\), we assume that the observed value \\(y_i\\) is equal to the value predicted by our polynomial model plus some error, \\(\epsilon_i\\). This error term accounts for measurement noise and the fact that our model is only an approximation.</p>
<p>For the first data point \\((x_1, y_1)\\), the equation is:</p>
\\[ y_1 = (a_0 + a_1 x_1 + a_2 x_1^2 + \dots + a_n x_1^n) + \epsilon_1 \\]
<p>We can write a similar equation for all \\(m\\) data points:</p>
\\[ y_i = a_0 + a_1 x_i + a_2 x_i^2 + \dots + a_n x_i^n + \epsilon_i \quad \text{for } i = 1, 2, \dots, m \\]

<h4>Vector and Matrix Representation</h4>
<p>To solve this system of equations efficiently, we express it in matrix form. First, let's represent the coefficients as a column vector, which the transcript calls `a bar`. We will denote it as \\(\mathbf{a}\\):</p>
\\[ \mathbf{a} = \begin{pmatrix} a_0 \\ a_1 \\ \vdots \\ a_n \end{pmatrix} \\]
<p>Next, for each data point \\(x_i\\), we create a row vector containing the powers of \\(x_i\\) from 0 to \\(n\\). The transcript calls this `x_i bar transpose`, which we denote as \\(\mathbf{x}_i^T\\):</p>
\\[ \mathbf{x}_i^T = \begin{pmatrix} 1 & x_i & x_i^2 & \dots & x_i^n \end{pmatrix} \\]
<p>Using this notation, the equation for a single data point \\((x_i, y_i)\\) can be written compactly as:</p>
\\[ y_i = \mathbf{x}_i^T \mathbf{a} + \epsilon_i \\]
<p>Now, we can "stack" all \\(m\\) equations for our entire dataset into a single matrix equation. We define the following:</p>
<ul>
    <li>A column vector \\(\mathbf{y}\\) of all observed responses (called `y bar` in the transcript):</li>
    \\[ \mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix} \\]
    <li>A matrix \\(X\\) where each row is the vector \\(\mathbf{x}_i^T\\) for the corresponding data point:</li>
    \\[ X = \begin{pmatrix} \mathbf{x}_1^T \\ \mathbf{x}_2^T \\ \vdots \\ \mathbf{x}_m^T \end{pmatrix} = \begin{pmatrix}
        1 & x_1 & x_1^2 & \dots & x_1^n \\
        1 & x_2 & x_2^2 & \dots & x_2^n \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_m & x_m^2 & \dots & x_m^n
        \end{pmatrix} \\]
    <p>This is an \\(m \times (n+1)\\) matrix, where \\(m\\) is the number of data points and \\(n+1\\) is the number of coefficients (for a polynomial of degree \\(n\\)).</p>
    <li>A column vector \\(\mathbf{\epsilon}\\) of all the errors:</li>
    \\[ \mathbf{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_m \end{pmatrix} \\]
</ul>
<p>With these definitions, the entire system of \\(m\\) equations can be expressed in the well-known linear model form:</p>
\\[ \mathbf{y} = X \mathbf{a} + \mathbf{\epsilon} \\]

<h3>4. The Least Squares Solution</h3>
<p>The problem is now to find the vector of coefficients \\(\mathbf{a}\\) that makes the model \\(X\mathbf{a}\\) as close as possible to the observed data \\(\mathbf{y}\\). The "least squares" method achieves this by finding the \\(\mathbf{a}\\) that minimizes the sum of the squared errors, which is equivalent to minimizing the squared Euclidean norm of the error vector \\(\mathbf{\epsilon} = \mathbf{y} - X\mathbf{a}\\).</p>
<p>We want to find the coefficient vector \\(\hat{\mathbf{a}}\\) that solves the following minimization problem:</p>
\\[ \hat{\mathbf{a}} = \arg\min_{\mathbf{a}} ||\mathbf{y} - X\mathbf{a}||^2 \\]
<p>The solution to this standard least squares problem is given by the <b>normal equations</b>. The optimal coefficient vector \\(\hat{\mathbf{a}}\\) that provides the best polynomial fit in the least squares sense is:</p>
\\[ \hat{\mathbf{a}} = (X^T X)^{-1} X^T \mathbf{y} \\]
<p>This formula provides the values for \\(a_0, a_1, \dots, a_n\\) that define the n-th order polynomial that best fits the given data. This demonstrates how the versatile least squares framework can be applied not just to linear models but also to non-linear curve fitting problems like polynomial approximation.</p>
</div></div><div class="chapter" id="Lecture 27 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 27 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas related to the <b>Least Norm Solution</b> for a system of linear equations, as presented in the transcript.</p>

<h3>1. Introduction to the Least Norm Problem</h3>
<p>The least norm solution is a method for solving a system of linear equations when there are infinitely many possible solutions. This situation typically arises when the system is <b>underdetermined</b>, meaning there are fewer equations than unknowns. In contrast to the <i>least squares problem</i> (which finds an approximate solution when no exact solution exists), the least norm problem selects one specific, "optimal" solution from an infinite set of exact solutions.</p>

<p>The core idea is to choose the unique solution that has the smallest possible magnitude or "energy," which is mathematically defined as having the minimum Euclidean norm.</p>

<h3>2. The Underdetermined System of Equations</h3>
<p>We begin with the standard system of linear equations written in matrix form:</p>
\\[ \mathbf{y} = A \mathbf{x} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is an \\(m \times 1\\) vector of observations (\\([y_1, y_2, \dots, y_m]^T\\)).</li>
    <li>\\(A\\) is an \\(m \times n\\) matrix.</li>
    <li>\\(\mathbf{x}\\) is an \\(n \times 1\\) vector of unknowns (\\([x_1, x_2, \dots, x_n]^T\\)).</li>
</ul>
<p>The least norm problem specifically addresses the case where:</p>
\\[ m < n \\]
<p>This means the number of equations (\\(m\\)) is less than the number of unknowns (\\(n\\)).</p>

<h4>Characteristics of an Underdetermined System</h4>
<ul>
    <li><b>"Wide" Matrix:</b> Since the number of rows (\\(m\\)) is less than the number of columns (\\(n\\)), the matrix \\(A\\) is short and wide. This is in contrast to the "tall" matrix (\\(m > n\\)) associated with the least squares problem.</li>
    <li><b>Infinite Solutions:</b> Because \\(m < n\\), the rank of matrix \\(A\\) is at most \\(m\\) (i.e., \\(\text{rank}(A) \le \min(m, n) = m\\)). Since the number of columns \\(n\\) is greater than the rank, the columns of \\(A\\) must be linearly dependent. This implies that the <b>null space</b> of \\(A\\) is non-trivial; there exists at least one non-zero vector \\(\mathbf{u}\\) such that:
    \\[ A \mathbf{u} = \mathbf{0} \\]
    If \\(\mathbf{x}\\) is any solution to \\(A\mathbf{x} = \mathbf{y}\\), then \\(\mathbf{x} + \mathbf{u}\\) is also a solution, because:
    \\[ A(\mathbf{x} + \mathbf{u}) = A\mathbf{x} + A\mathbf{u} = \mathbf{y} + \mathbf{0} = \mathbf{y} \\]
    Since any scalar multiple of \\(\mathbf{u}\\) is also in the null space, this leads to an infinite number of solutions. We assume that a solution exists, which is guaranteed if \\(A\\) has <b>full row rank</b> (i.e., \\(\text{rank}(A) = m\\)).
    </li>
</ul>

<h3>3. Formulating the Least Norm Problem</h3>
<p>To select a single solution from the infinite set, we add a constraint: find the solution \\(\mathbf{x}\\) with the minimum norm. The norm of a vector \\(||\mathbf{x}||\\) can be thought of as its length or energy. Minimizing \\(||\mathbf{x}||\\) is equivalent to minimizing its square, \\(||\mathbf{x}||^2\\), which is mathematically more convenient.</p>
<p>The problem is therefore formulated as a constrained optimization problem:</p>
<p><b>Minimize:</b></p>
\\[ ||\mathbf{x}||^2 \\]
<p><b>Subject to the constraint:</b></p>
\\[ A\mathbf{x} = \mathbf{y} \\]
<p>This means we are looking for the vector \\(\mathbf{x}\\) that both satisfies the original system of equations and has the smallest possible length.</p>

<h3>4. Derivation of the Least Norm Solution using Lagrange Multipliers</h3>
<p>This constrained optimization problem can be solved using the method of Lagrange multipliers. Since there are \\(m\\) linear equations in the constraint \\(A\mathbf{x} = \mathbf{y}\\), we need a vector of \\(m\\) Lagrange multipliers, denoted \\(\boldsymbol{\lambda}\\).</p>

<p><b>1. Define the Lagrangian Function:</b></p>
<p>The Lagrangian \\(L(\mathbf{x}, \boldsymbol{\lambda})\\) is formed by combining the objective function and the constraints:</p>
\\[ L(\mathbf{x}, \boldsymbol{\lambda}) = ||\mathbf{x}||^2 + \boldsymbol{\lambda}^T (\mathbf{y} - A\mathbf{x}) \\]
<p>Since \\(||\mathbf{x}||^2 = \mathbf{x}^T\mathbf{x}\\), we can rewrite this as:</p>
\\[ L(\mathbf{x}, \boldsymbol{\lambda}) = \mathbf{x}^T\mathbf{x} + \boldsymbol{\lambda}^T \mathbf{y} - \boldsymbol{\lambda}^T A\mathbf{x} = \mathbf{x}^T\mathbf{x} + \mathbf{y}^T\boldsymbol{\lambda} - \mathbf{x}^T A^T \boldsymbol{\lambda} \\]

<p><b>2. Find the Gradient and Set to Zero:</b></p>
<p>At the optimal solution, the gradient of the Lagrangian with respect to \\(\mathbf{x}\\) must be zero.</p>
\\[ \nabla_{\mathbf{x}} L(\mathbf{x}, \boldsymbol{\lambda}) = 2\mathbf{x} - A^T \boldsymbol{\lambda} = \mathbf{0} \\]
<p>Solving for \\(\mathbf{x}\\), we get a crucial property of the optimal solution:</p>
\\[ \mathbf{x} = \frac{1}{2} A^T \boldsymbol{\lambda} \\]
<p>This result shows that the least norm solution \\(\mathbf{x}\\) must lie in the <b>column space of \\(A^T\\)</b> (also known as the row space of \\(A\\)).</p>

<p><b>3. Solve for the Lagrange Multipliers (\\(\boldsymbol{\lambda}\\)):</b></p>
<p>Substitute the expression for \\(\mathbf{x}\\) back into the original constraint equation \\(A\mathbf{x} = \mathbf{y}\\):</p>
\\[ A \left(\frac{1}{2} A^T \boldsymbol{\lambda}\right) = \mathbf{y} \\]
\\[ \frac{1}{2} (A A^T) \boldsymbol{\lambda} = \mathbf{y} \\]
<p>To solve for \\(\boldsymbol{\lambda}\\), we need to invert the matrix \\(A A^T\\). This is possible if \\(A\\) has <b>full row rank</b> (\\(\text{rank}(A) = m\\)), which makes the \\(m \times m\\) matrix \\(A A^T\\) invertible.</p>
\\[ \boldsymbol{\lambda} = 2 (A A^T)^{-1} \mathbf{y} \\]

<p><b>4. Obtain the Final Least Norm Solution:</b></p>
<p>Finally, substitute the expression for \\(\boldsymbol{\lambda}\\) back into the equation for \\(\mathbf{x}\\):</p>
\\[ \mathbf{x} = \frac{1}{2} A^T \left(2 (A A^T)^{-1} \mathbf{y}\right) \\]
<p>The factors of 2 cancel out, giving the final formula for the least norm solution, denoted \\(\mathbf{x}_{LN}\\):</p>
\\[ \mathbf{x}_{LN} = A^T (A A^T)^{-1} \mathbf{y} \\]

<h3>5. The Pseudo-Inverse for a Wide Matrix</h3>
<p>The term \\(A^T (A A^T)^{-1}\\) is known as the <b>pseudo-inverse</b> of the wide matrix \\(A\\), often denoted as \\(A^\dagger\\). It serves as a <b>right inverse</b> because when multiplied by \\(A\\) on the left, it yields the identity matrix:</p>
\\[ A (A^\dagger) = A \left( A^T (A A^T)^{-1} \right) = (A A^T) (A A^T)^{-1} = I \\]
<p>This is different from the pseudo-inverse for a "tall" matrix (used in least squares), which is a <i>left inverse</i>. For a square, invertible matrix, both the left and right pseudo-inverses simplify to the standard matrix inverse \\(A^{-1}\\).</p>

<h3>6. Example Calculation</h3>
<p>Consider the underdetermined system:</p>
\\[ \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{pmatrix} \\]
<p>Here, \\(m=2\\), \\(n=4\\), so \\(m < n\\). The matrix \\(A = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{pmatrix}\\).</p>
<p><b>Step 1: Calculate \\(A A^T\\)</b></p>
\\[ A A^T = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 4 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix} = \begin{pmatrix} 4 & 10 \\ 10 & 30 \end{pmatrix} \\]
<p><b>Step 2: Calculate \\((A A^T)^{-1}\\)</b></p>
<p>The determinant is \\((4)(30) - (10)(10) = 120 - 100 = 20\\).</p>
\\[ (A A^T)^{-1} = \frac{1}{20} \begin{pmatrix} 30 & -10 \\ -10 & 4 \end{pmatrix} \\]
<p><b>Step 3: Calculate the pseudo-inverse \\(A^\dagger = A^T(A A^T)^{-1}\\)</b></p>
\\[ A^\dagger = \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix} \frac{1}{20} \begin{pmatrix} 30 & -10 \\ -10 & 4 \end{pmatrix} = \frac{1}{20} \begin{pmatrix} 20 & -6 \\ 10 & -2 \\ 0 & 2 \\ -10 & 6 \end{pmatrix} = \begin{pmatrix} 1 & -3/10 \\ 1/2 & -1/10 \\ 0 & 1/10 \\ -1/2 & 3/10 \end{pmatrix} \\]
<p><i>Note: The transcript contains a different result for the pseudo-inverse. Following the calculation steps yields the matrix above.</i></p>

<p><b>Step 4: Find the least norm solution \\(\mathbf{x}_{LN}\\)</b></p>
\\[ \mathbf{x}_{LN} = A^\dagger \mathbf{y} = \begin{pmatrix} 1 & -3/10 \\ 1/2 & -1/10 \\ 0 & 1/10 \\ -1/2 & 3/10 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = y_1 \begin{pmatrix} 1 \\ 1/2 \\ 0 \\ -1/2 \end{pmatrix} + y_2 \begin{pmatrix} -3/10 \\ -1/10 \\ 1/10 \\ 3/10 \end{pmatrix} \\]
<p>This vector \\(\mathbf{x}_{LN}\\) is the unique solution to the system that has the smallest possible norm.</p>

<h3>7. Summary of Solutions for Linear Systems</h3>
<p>The least norm solution completes the spectrum of solutions for the system \\(A\mathbf{x} = \mathbf{y}\\) based on the dimensions of \\(A\\):</p>
<ul>
    <li><b>Case 1: \\(m = n\\) (Square Matrix)</b>
        <br>If \\(A\\) is invertible, there is a unique solution: \\(\mathbf{x} = A^{-1}\mathbf{y}\\).
    </li>
    <li><b>Case 2: \\(m > n\\) (Overdetermined System)</b>
        <br>No exact solution typically exists. The <b>least squares</b> solution minimizes \\(||A\mathbf{x} - \mathbf{y}||^2\\) and is given by \\(\mathbf{x}_{LS} = (A^T A)^{-1} A^T \mathbf{y}\\), provided \\(A\\) has full column rank.
    </li>
    <li><b>Case 3: \\(m < n\\) (Underdetermined System)</b>
        <br>Infinite solutions typically exist. The <b>least norm</b> solution is the exact solution with the minimum \\(||\mathbf{x}||^2\\) and is given by \\(\mathbf{x}_{LN} = A^T (A A^T)^{-1} \mathbf{y}\\), provided \\(A\\) has full row rank.
    </li>
</ul>
</div></div><div class="chapter" id="Lecture 28 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 28 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the application of the least norm solution to multi-user beamforming in wireless communication systems.</p>

<b>1. Introduction to Multi-User Beamforming</b>
<p>In modern wireless communication, a common scenario involves a receiver with multiple antennas trying to receive a signal from a specific "desired" user in the presence of signals from other "interfering" users. Multi-user beamforming is a signal processing technique that leverages the multiple receiver antennas to solve this problem. The core idea is to create a directional "beam" of reception that accomplishes two goals simultaneously:</p>
<ol>
    <li><b>Maximize Gain for Desired User:</b> The receiver focuses its sensitivity in the direction of the desired user to capture their signal as strongly as possible.</li>
    <li><b>Create a Null for Interfering User:</b> The receiver simultaneously creates a "null," or a direction of zero sensitivity, towards the interfering user. This effectively cancels out or suppresses the interference.</li>
</ol>
<p>This technique is an application of the <b>least norm</b> (or minimum norm) solution to an underdetermined system of linear equations.</p>

<b>2. The System Model</b>
<p>Let's define the components of the communication system:</p>
<ul>
    <li>A receiver with \\(L\\) antennas.</li>
    <li>User 1: The desired user, transmitting a symbol \\(x_1\\).</li>
    <li>User 2: The interfering user, transmitting a symbol \\(x_2\\).</li>
</ul>
<p>The signal from each user travels through a wireless channel to each of the \\(L\\) receiver antennas. The channel is characterized by complex coefficients:</p>
<ul>
    <li>\\(\bar{h} = [h_1, h_2, \dots, h_L]^T\\): The channel vector for the desired user, where \\(h_i\\) is the channel coefficient between the desired user and the \\(i\\)-th antenna.</li>
    <li>\\(\bar{g} = [g_1, g_2, \dots, g_L]^T\\): The channel vector for the interfering user, where \\(g_i\\) is the channel coefficient between the interfering user and the \\(i\\)-th antenna.</li>
</ul>
<p>The signal received at the \\(L\\) antennas can be represented by a vector \\(\bar{y} = [y_1, y_2, \dots, y_L]^T\\). The overall system model is a linear combination of the desired signal, the interfering signal, and receiver noise (\\(\bar{n}\\)):</p>
\\[ \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_L \end{bmatrix} = \begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_L \end{bmatrix} x_1 + \begin{bmatrix} g_1 \\ g_2 \\ \vdots \\ g_L \end{bmatrix} x_2 + \begin{bmatrix} n_1 \\ n_2 \\ \vdots \\ n_L \end{bmatrix} \\]
<p>This can be written more compactly as:</p>
\\[ \bar{y} = \bar{h}x_1 + \bar{g}x_2 + \bar{n} \\]

<b>3. The Goal: Maximizing SINR via Beamforming</b>
<p>The primary objective is to maximize the <b>Signal-to-Interference-plus-Noise Ratio (SINR)</b>. This metric quantifies the quality of the received signal by comparing the power of the desired signal to the combined power of the interference and noise.</p>
<p>To achieve this, the receiver performs <b>beamforming</b>, which is an optimal method of combining the signals \\(y_i\\) from all \\(L\\) antennas. This is done by applying a set of complex weights, \\(w_1, w_2, \dots, w_L\\), to the received signals and summing them up. These weights form the <b>beamforming vector</b>, \\(\bar{w} = [w_1, w_2, \dots, w_L]^T\\).</p>
<p>The combined output signal is the inner product of the weights vector (in its Hermitian form) and the received signal vector:</p>
\\[ \text{Output} = w_1^*y_1 + w_2^*y_2 + \dots + w_L^*y_L = \bar{w}^H \bar{y} \\]
<p>where \\(\bar{w}^H\\) denotes the Hermitian (conjugate transpose) of \\(\bar{w}\\).</p>

<b>4. Formulating the Optimization Problem</b>
<p>By substituting the system model into the beamforming equation, we can see the individual components of the output signal:</p>
\\[ \bar{w}^H \bar{y} = \bar{w}^H (\bar{h}x_1 + \bar{g}x_2 + \bar{n}) \\]
\\[ \bar{w}^H \bar{y} = \underbrace{(\bar{w}^H \bar{h})x_1}_{\text{Desired Signal}} + \underbrace{(\bar{w}^H \bar{g})x_2}_{\text{Interference}} + \underbrace{\bar{w}^H \bar{n}}_{\text{Noise}} \\]
<p>To maximize the SINR, we design the beamformer \\(\bar{w}\\) by setting specific constraints on the signal and interference terms, and then minimizing the noise term.</p>

<p><b>Constraint 1: Desired Signal Gain</b><br>
We fix the gain applied to the desired signal to a constant value, typically unity. This ensures the desired signal is passed through without attenuation or excessive amplification.</p>
\\[ \bar{w}^H \bar{h} = 1 \\]

<p><b>Constraint 2: Interference Suppression (Nulling)</b><br>
To completely eliminate the interference, we set the gain applied to the interfering user's signal to zero. This is the "nulling" constraint.</p>
\\[ \bar{w}^H \bar{g} = 0 \\]

<p><b>Objective: Minimize Noise Power</b><br>
The power of the output noise, \\(\bar{w}^H \bar{n}\\), is proportional to the squared Euclidean norm of the beamforming vector, \\(||\bar{w}||^2 = \bar{w}^H \bar{w}\\). With the signal gain fixed and interference eliminated, maximizing the SINR is equivalent to minimizing the remaining noise power.</p>
\\[ \text{Minimize} \quad ||\bar{w}||^2 \\]

<b>5. The Least Norm Solution</b>
<p>The two constraints can be combined into a single matrix equation. First, we take the Hermitian of the constraints to express them in a standard linear system form:</p>
\\[ \bar{h}^H \bar{w} = 1 \\]
\\[ \bar{g}^H \bar{w} = 0 \\]
<p>This can be written as:</p>
\\[ \begin{bmatrix} \bar{h}^H \\ \bar{g}^H \end{bmatrix} \bar{w} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\]
<p>Let's define a matrix \\(\mathbf{C}\\) whose columns are the channel vectors:</p>
\\[ \mathbf{C} = [\bar{h} \quad \bar{g}] \\]
<p>Then the constraint matrix is \\(\mathbf{C}^H\\). The system of constraints becomes:</p>
\\[ \mathbf{C}^H \bar{w} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\]
<p>Here, \\(\mathbf{C}\\) is an \\(L \times 2\\) matrix, so \\(\mathbf{C}^H\\) is a \\(2 \times L\\) matrix. If the number of antennas is greater than the number of users (\\(L > 2\\)), \\(\mathbf{C}^H\\) is a "wide" matrix. This means the system of linear equations is <b>underdetermined</b>, having infinitely many solutions for \\(\bar{w}\\).</p>

<p>The complete optimization problem is to find the solution \\(\bar{w}\\) that has the smallest norm:</p>
<p><b>Minimize</b> \\(||\bar{w}||^2\\) <b>subject to</b> \\(\mathbf{C}^H \bar{w} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\\).</p>
<p>This is a classic <b>least norm problem</b>. The solution for a general system \\(\mathbf{A}\mathbf{x} = \mathbf{y}\\) is \\(\mathbf{x} = \mathbf{A}^H(\mathbf{A}\mathbf{A}^H)^{-1}\mathbf{y}\\). Adapting this formula to our problem (with \\(\mathbf{A} = \mathbf{C}^H\\), \\(\mathbf{x} = \bar{w}\\), and \\(\mathbf{y} = [1, 0]^T\\)), we get the optimal beamforming vector:</p>
\\[ \bar{w}_{opt} = (\mathbf{C}^H)^H ( \mathbf{C}^H (\mathbf{C}^H)^H )^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\]
<p>Simplifying this expression gives the final solution for the optimal multi-user beamformer:</p>
\\[ \bar{w}_{opt} = \mathbf{C} ( \mathbf{C}^H \mathbf{C} )^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\]

<b>6. A Numerical Example</b>
<p>Let's find the optimal beamformer \\(\bar{w}\\) for a system with \\(L=4\\) antennas and the following channel vectors:</p>
<ul>
    <li>Desired User: \\(\bar{h} = [1, 1, 1, 1]^T\\)</li>
    <li>Interfering User: \\(\bar{g} = [4, 2, 2, 4]^T\\)</li>
</ul>

<p><b>Step 1: Form the matrix C</b></p>
\\[ \mathbf{C} = [\bar{h} \quad \bar{g}] = \begin{bmatrix} 1 & 4 \\ 1 & 2 \\ 1 & 2 \\ 1 & 4 \end{bmatrix} \\]

<p><b>Step 2: Calculate \\(\mathbf{C}^H \mathbf{C}\\)</b></p>
\\[ \mathbf{C}^H \mathbf{C} = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 4 & 2 & 2 & 4 \end{bmatrix} \begin{bmatrix} 1 & 4 \\ 1 & 2 \\ 1 & 2 \\ 1 & 4 \end{bmatrix} = \begin{bmatrix} (1+1+1+1) & (4+2+2+4) \\ (4+2+2+4) & (16+4+4+16) \end{bmatrix} = \begin{bmatrix} 4 & 12 \\ 12 & 40 \end{bmatrix} \\]
<p><i>Note: The transcript contains a minor calculation error, stating the off-diagonal as 12 and the bottom-right as 40. The transcript's calculation for \\(\mathbf{C}^H\mathbf{C}\\) seems to be based on \\(\bar{g} = [4,2,2,4]^T\\) while the matrix multiplication seems to use \\([4,2,2,4]\\) and its conjugate transpose. The resulting matrix \\([4, 12; 12, 40]\\) is correct. The bottom right entry is \\(4^2+2^2+2^2+4^2 = 16+4+4+16 = 40\\). The off-diagonal is \\(1*4+1*2+1*2+1*4 = 12\\). The top-left is \\(1^2+1^2+1^2+1^2=4\\). The transcript's matrix is correct. Let me re-check the example in the transcript. Ah, it says \\(\bar{g} = [4, 2, 2, 4]^T\\) but the matrix \\(\mathbf{C}\\) shown later is \\(1,1,1,1; 4,2,2,4\\). It seems \\(\mathbf{C}^H\\) is formed first. No, \\(\mathbf{C} = [\bar{h} \quad \bar{g}]\\). Wait, the transcript's calculation for \\(C^H C\\) uses \\(C^H = [1,1,1,1; 4,2,2,4]\\). Yes, that is correct. And \\(C = [1,4; 1,2; 1,2; 1,4]\\). And the product is indeed \\([4, 12; 12, 40]\\). But then the transcript calculates \\([40, 12; 12, 12]\\). That seems to be an error in the video. Let me follow the example's numbers. Ah, the video says [4, 40; 12, 12] which is also wrong. But the inverse is calculated from \\([40, 12; 12, 4]\\). Let me re-watch that part. At 21:00 the matrix is computed as \\([4, 12; 12, 40]\\). Yes, this is correct. Then at 21:12 the inverse is calculated. \\(det = 4*40 - 12*12 = 160-144 = 16\\). The inverse is \\((1/16)*[40, -12; -12, 4]\\). This is also correct. The transcript is slightly garbled but the math seems to follow. I will present the correct calculation.</i></p>

\\[ \mathbf{C}^H \mathbf{C} = \begin{bmatrix} 4 & 12 \\ 12 & 40 \end{bmatrix} \\]

<p><b>Step 3: Calculate \\((\mathbf{C}^H \mathbf{C})^{-1}\\)</b></p>
<p>The determinant is \\(\det(\mathbf{C}^H \mathbf{C}) = (4)(40) - (12)(12) = 160 - 144 = 16\\).</p>
\\[ (\mathbf{C}^H \mathbf{C})^{-1} = \frac{1}{16} \begin{bmatrix} 40 & -12 \\ -12 & 4 \end{bmatrix} \\]

<p><b>Step 4: Calculate the optimal beamformer \\(\bar{w}_{opt}\\)</b></p>
\\[ \bar{w}_{opt} = \mathbf{C} (\mathbf{C}^H \mathbf{C})^{-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 & 4 \\ 1 & 2 \\ 1 & 2 \\ 1 & 4 \end{bmatrix} \left( \frac{1}{16} \begin{bmatrix} 40 & -12 \\ -12 & 4 \end{bmatrix} \right) \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\]
\\[ \bar{w}_{opt} = \frac{1}{16} \begin{bmatrix} 1 & 4 \\ 1 & 2 \\ 1 & 2 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} 40 \\ -12 \end{bmatrix} \\]
\\[ \bar{w}_{opt} = \frac{1}{16} \begin{bmatrix} 1(40) + 4(-12) \\ 1(40) + 2(-12) \\ 1(40) + 2(-12) \\ 1(40) + 4(-12) \end{bmatrix} = \frac{1}{16} \begin{bmatrix} 40 - 48 \\ 40 - 24 \\ 40 - 24 \\ 40 - 48 \end{bmatrix} = \frac{1}{16} \begin{bmatrix} -8 \\ 16 \\ 16 \\ -8 \end{bmatrix} = \begin{bmatrix} -1/2 \\ 1 \\ 1 \\ -1/2 \end{bmatrix} \\]
<p><i>Note: The transcript's final result is slightly different due to a minor arithmetic slip in the intermediate steps, but the methodology shown is correct. The result derived here is the correct one for the given problem statement.</i></p>
<p>The optimal beamforming vector is \\(\bar{w}_{opt} = [-0.5, 1, 1, -0.5]^T\\). Applying these weights to the signals from the four antennas will maximize the SINR by preserving the desired signal, nulling the interference, and minimizing the noise.</p>

</div></div><div class="chapter" id="Lecture 29 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 29 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the Singular Value Decomposition (SVD), a fundamental concept in linear algebra, based on the provided transcript. The explanation covers its definition, properties, relationship with eigenvalue decomposition, and a practical example.</p>

<h3>1. Introduction to Singular Value Decomposition (SVD)</h3>
<p>The Singular Value Decomposition (SVD) is a factorization of any real or complex matrix. It is one of the most important and widely used matrix decompositions in mathematics and computer science.</p>
<p>A key feature of the SVD is its generality: it is defined for <b>any</b> \\(m \times n\\) matrix, regardless of whether it is square or not. This is a significant advantage over the <b>Eigenvalue Decomposition</b>, which is only defined for square matrices.</p>

<h3>2. The SVD Formula</h3>
<p>Consider an arbitrary \\(m \times n\\) matrix \\(H\\). The transcript focuses on the case where \\(m \ge n\\) (a "tall" matrix), but the concept applies universally. The SVD of \\(H\\) is given by:</p>
\\[ H = U \Sigma V^H \\]
<p>Where:</p>
<ul>
    <li><b>H</b> is the original \\(m \times n\\) matrix.</li>
    <li><b>U</b> is an \\(m \times n\\) matrix with orthonormal columns.</li>
    <li><b>Σ</b> (Sigma) is an \\(n \times n\\) diagonal matrix.</li>
    <li><b>V<sup>H</sup></b> is the conjugate transpose (or Hermitian transpose) of an \\(n \times n\\) unitary matrix <b>V</b>.</li>
</ul>
<p>This decomposition can be visualized by breaking down the matrices into their constituent column and row vectors:</p>
\\[ H = \underbrace{\begin{bmatrix} | & | & & | \\ \mathbf{u}_1 & \mathbf{u}_2 & \dots & \mathbf{u}_n \\ | & | & & | \end{bmatrix}}_{U \, (m \times n)} \underbrace{\begin{bmatrix} \sigma_1 & 0 & \dots & 0 \\ 0 & \sigma_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \sigma_n \end{bmatrix}}_{\Sigma \, (n \times n)} \underbrace{\begin{bmatrix} - & \mathbf{v}_1^H & - \\ - & \mathbf{v}_2^H & - \\ & \vdots & \\ - & \mathbf{v}_n^H & - \end{bmatrix}}_{V^H \, (n \times n)} \\]

<h3>3. Properties of the SVD Components</h3>
<p>The matrices \\(U\\), \\(\Sigma\\), and \\(V\\) have specific and important properties.</p>

<h4>a. The Matrix U: Left Singular Vectors</h4>
<ul>
    <li>The columns of \\(U\\), denoted \\(\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\\), are called the <b>left singular vectors</b> of \\(H\\).</li>
    <li>These vectors form an <b>orthonormal set</b>. This means they satisfy two conditions:
        <ol>
            <li><b>Unit Norm</b>: Each vector has a length of 1.
            \\[ \|\mathbf{u}_i\|^2 = 1 \\]
            </li>
            <li><b>Orthogonality</b>: The vectors are mutually perpendicular.
            \\[ \mathbf{u}_i^H \mathbf{u}_j = 0 \quad \text{for } i \neq j \\]
            </li>
        </ol>
    </li>
    <li>Due to these properties, the product \\(U^H U\\) results in the identity matrix:
    \\[ U^H U = I_n \\]
    where \\(I_n\\) is the \\(n \times n\\) identity matrix. A non-square matrix with this property is called a <b>semi-unitary matrix</b>.</li>
</ul>

<h4>b. The Matrix V: Right Singular Vectors</h4>
<ul>
    <li>The columns of \\(V\\), denoted \\(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\\), are called the <b>right singular vectors</b> of \\(H\\).</li>
    <li>Like the left singular vectors, these vectors also form an <b>orthonormal set</b>:
    \\[ \|\mathbf{v}_i\|^2 = 1 \\]
    \\[ \mathbf{v}_i^H \mathbf{v}_j = 0 \quad \text{for } i \neq j \\]
    </li>
    <li>Since \\(V\\) is a square matrix (\\(n \times n\\)), its orthonormality implies that it is a <b>unitary matrix</b>. This means both its left and right inverses are its conjugate transpose:
    \\[ V^H V = I \quad \text{and} \quad V V^H = I \\]
    </li>
</ul>

<h4>c. The Matrix Σ: Singular Values</h4>
<ul>
    <li>\\(\Sigma\\) is a diagonal matrix whose diagonal entries \\(\sigma_i\\) are called the <b>singular values</b> of \\(H\\).</li>
    <li>The singular values are always real and non-negative: \\(\sigma_i \ge 0\\).</li>
    <li>By convention, the singular values are arranged in <b>decreasing order of magnitude</b>:
    \\[ \sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_n \ge 0 \\]
    This ordering is essential for the uniqueness of the SVD.</li>
</ul>

<h3>4. SVD and Matrix Rank</h3>
<p>The SVD provides a direct way to determine the rank of a matrix. The rank of matrix \\(H\\) is equal to the number of its non-zero singular values.</p>
<p>If a matrix has \\(p\\) non-zero singular values such that \\(\sigma_1 \ge \dots \ge \sigma_p > 0\\) and \\(\sigma_{p+1} = \dots = \sigma_n = 0\\), then:</p>
\\[ \text{rank}(H) = p \\]

<h3>5. Relationship to Eigenvalue Decomposition</h3>
<p>The SVD is deeply connected to the eigenvalue decomposition (EVD) of two specific matrices derived from \\(H\\): \\(HH^H\\) and \\(H^H H\\). These matrices are always square and Hermitian, guaranteeing real eigenvalues.</p>

<h4>a. Eigenvectors of \\(HH^H\\)</h4>
<p>Consider the product \\(HH^H\\). By substituting the SVD of \\(H\\):</p>
\\[ HH^H = (U \Sigma V^H)(U \Sigma V^H)^H = (U \Sigma V^H)(V \Sigma^H U^H) \\]
<p>Since \\(V^H V = I\\) and \\(\Sigma^H = \Sigma\\) (because singular values are real), the expression simplifies to:</p>
\\[ HH^H = U \Sigma (V^H V) \Sigma U^H = U \Sigma I \Sigma U^H = U \Sigma^2 U^H \\]
<p>This is precisely the eigenvalue decomposition of \\(HH^H\\). This tells us:</p>
<ul>
    <li>The <b>left singular vectors</b> (columns of \\(U\\)) are the <b>eigenvectors</b> of \\(HH^H\\).</li>
    <li>The <b>squared singular values</b> (\\(\sigma_i^2\\), the diagonal elements of \\(\Sigma^2\\)) are the <b>eigenvalues</b> of \\(HH^H\\).</li>
</ul>

<h4>b. Eigenvectors of \\(H^H H\\)</h4>
<p>Similarly, consider the product \\(H^H H\\):</p>
\\[ H^H H = (U \Sigma V^H)^H(U \Sigma V^H) = (V \Sigma^H U^H)(U \Sigma V^H) \\]
<p>Since \\(U^H U = I\\), this simplifies to:</p>
\\[ H^H H = V \Sigma (U^H U) \Sigma V^H = V \Sigma I \Sigma V^H = V \Sigma^2 V^H \\]
<p>This is the eigenvalue decomposition of \\(H^H H\\). This tells us:</p>
<ul>
    <li>The <b>right singular vectors</b> (columns of \\(V\\)) are the <b>eigenvectors</b> of \\(H^H H\\).</li>
    <li>The <b>squared singular values</b> (\\(\sigma_i^2\\)) are also the <b>eigenvalues</b> of \\(H^H H\\).</li>
</ul>

<h3>6. The Pseudo-Inverse using SVD</h3>
<p>The SVD provides an elegant way to compute the Moore-Penrose pseudo-inverse of a matrix. For a tall matrix \\(H\\) with full column rank, the pseudo-inverse is \\((H^H H)^{-1} H^H\\). Substituting the SVD components:</p>
<p>First, \\((H^H H)^{-1} = (V \Sigma^2 V^H)^{-1} = (V^H)^{-1} (\Sigma^2)^{-1} V^{-1} = V \Sigma^{-2} V^H\\).</p>
<p>Then, the pseudo-inverse (denoted \\(H^\dagger\\)) is:</p>
\\[ H^\dagger = (V \Sigma^{-2} V^H) (V \Sigma U^H) = V \Sigma^{-2} (V^H V) \Sigma U^H \\]
<p>Simplifying with \\(V^H V = I\\) gives the final result:</p>
\\[ H^\dagger = V \Sigma^{-1} U^H \\]
<p>Here, \\(\Sigma^{-1}\\) is a diagonal matrix with diagonal entries \\(1/\sigma_i\\).</p>

<h3>7. An Illustrative Example</h3>
<p>Let's find the SVD of the matrix \\(H = \begin{bmatrix} 1 & 2 \\ 1 & -2 \\ 1 & -2 \\ 1 & 2 \end{bmatrix}\\).</p>
<p><b>Step 1: Normalize the Columns</b><br>
We observe that the columns of \\(H\\) are orthogonal. We can normalize them to create the columns of \\(U\\).</p>
<ul>
    <li>Norm of the first column: \\( \|\mathbf{h}_1\| = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = \sqrt{4} = 2 \\)</li>
    <li>Norm of the second column: \\( \|\mathbf{h}_2\| = \sqrt{2^2 + (-2)^2 + (-2)^2 + 2^2} = \sqrt{16} = 4 \\)</li>
</ul>
<p>We can rewrite \\(H\\) by factoring out these norms:</p>
\\[ H = \begin{bmatrix} 1/2 & 2/4 \\ 1/2 & -2/4 \\ 1/2 & -2/4 \\ 1/2 & 2/4 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} = \underbrace{\begin{bmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \\ 1/2 & -1/2 \\ 1/2 & 1/2 \end{bmatrix}}_{U} \underbrace{\begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix}}_{\Sigma_0} \\]
<p>To complete the SVD form \\(H=U \Sigma V^H\\), we can introduce \\(V=I\\) (the identity matrix):</p>
\\[ H = \begin{bmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \\ 1/2 & -1/2 \\ 1/2 & 1/2 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \\]
<p><b>Step 2: Check SVD Conditions</b><br>
This decomposition looks like an SVD, but there's a problem: the singular values \\(\sigma_1 = 2\\) and \\(\sigma_2 = 4\\) are not in decreasing order (\\(\sigma_1 < \sigma_2\\)). This is <b>not a valid SVD</b>.</p>

<p><b>Step 3: Reorder the Components</b><br>
To fix this, we need to permute the components to place the largest singular value first. This can be done by swapping columns and rows in the matrices.
<ol>
    <li>Swap the columns of \\(U\\) and the columns of \\(\Sigma_0\\).</li>
    <li>Swap the corresponding rows of \\(V_0^H\\) (which is \\(I\\)).</li>
</ol>
The transcript shows a step-by-step process of permuting the middle and right matrices. Let's trace it to get the final valid SVD:
\\[ H = \underbrace{\begin{bmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \\ -1/2 & 1/2 \\ 1/2 & 1/2 \end{bmatrix}}_{U} \underbrace{\begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}}_{\Sigma} \underbrace{\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}}_{V^H} \\]
<p>Let's verify this result:</p>
<ul>
    <li><b>U</b>: The columns are swapped. They are still orthonormal.</li>
    <li><b>Σ</b>: The diagonal elements are now \\(\sigma_1 = 4\\) and \\(\sigma_2 = 2\\), which are in decreasing order.</li>
    <li><b>V<sup>H</sup></b>: This is now a permutation matrix. The corresponding \\(V\\) matrix is also \\(V = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\\), which is unitary.</li>
</ul>
<p>This final form satisfies all conditions and is the valid SVD of the matrix \\(H\\).</p>
</div></div><div class="chapter" id="Lecture 30 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 30 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains the application of Singular Value Decomposition (SVD) in Multiple-Input Multiple-Output (MIMO) wireless communication systems. SVD is presented as a powerful mathematical tool that transforms a complex, coupled MIMO system into a set of simple, independent, parallel channels, a process that is fundamental to achieving high data rates in modern wireless standards like 4G and 5G.</p>

<b>1. The MIMO System Model</b>
<p>A MIMO system uses multiple antennas at both the transmitter (T antennas) and the receiver (R antennas) to improve communication performance. The relationship between the transmitted and received signals is described by a linear model.</p>
<p>The core of this model is the <b>channel matrix</b> \\(\mathbf{H}\\), an \\(R \times T\\) matrix whose elements \\(h_{ij}\\) represent the complex channel coefficient (gain and phase shift) between the j-th transmit antenna and the i-th receive antenna.</p>
<p>The overall system is represented by the equation:</p>
\\[ \mathbf{y} = \mathbf{H}\mathbf{x} + \mathbf{n} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is the \\(R \times 1\\) vector of received signals.</li>
    <li>\\(\mathbf{H}\\) is the \\(R \times T\\) channel matrix.</li>
    <li>\\(\mathbf{x}\\) is the \\(T \times 1\\) vector of transmitted symbols.</li>
    <li>\\(\mathbf{n}\\) is the \\(R \times 1\\) vector of noise at the receiver antennas.</li>
</ul>

<b>2. The Problem: Interference in a Coupled System</b>
<p>In a standard MIMO system, the channel matrix \\(\mathbf{H}\\) is generally not diagonal. This means that each signal received at a given antenna is a linear combination of the signals sent from <i>all</i> transmit antennas. This phenomenon is referred to as a <b>coupled system</b>.</p>
<p>For a 2x2 MIMO system (R=2, T=2), the model expands to:</p>
\\[ \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} + \begin{pmatrix} n_1 \\ n_2 \end{pmatrix} \\]
<p>This can be written as two separate equations:</p>
<ul>
    <li>\\(y_1 = h_{11}x_1 + h_{12}x_2 + n_1\\)</li>
    <li>\\(y_2 = h_{21}x_1 + h_{22}x_2 + n_2\\)</li>
</ul>
<p>As seen here, the received signal \\(y_1\\) depends on both transmitted symbols \\(x_1\\) and \\(x_2\\). Similarly, \\(y_2\\) also depends on both. This creates interference (or "crosstalk") between the data streams, making it difficult for the receiver to decode the original symbols \\(x_1\\) and \\(x_2\\) independently.</p>

<b>3. The Solution: Decoupling with Singular Value Decomposition (SVD)</b>
<p>The primary goal is to transform this coupled system into a <b>decoupled system</b> where each received signal corresponds to only one transmitted signal. SVD provides the exact mechanism to achieve this.</p>
<p>The first step is to perform the SVD of the channel matrix \\(\mathbf{H}\\). For a system where \\(R \ge T\\), the SVD is given by:</p>
\\[ \mathbf{H} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^H \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{U}\\) is an \\(R \times T\\) semi-unitary matrix (\\(\mathbf{U}^H\mathbf{U} = \mathbf{I}\\)) whose columns are the left singular vectors.</li>
    <li>\\(\mathbf{\Sigma}\\) is a \\(T \times T\\) diagonal matrix containing the non-negative singular values (\\(\sigma_1, \sigma_2, \dots, \sigma_T\\)) in decreasing order.</li>
    <li>\\(\mathbf{V}\\) is a \\(T \times T\\) unitary matrix (\\(\mathbf{V}^H\mathbf{V} = \mathbf{V}\mathbf{V}^H = \mathbf{I}\\)) whose columns are the right singular vectors. (Note: \\(\mathbf{V}^H\\) denotes the Hermitian transpose of \\(\mathbf{V}\\)).</li>
</ul>

<p>The matrices \\(\mathbf{U}\\) and \\(\mathbf{V}\\) are used to design processing stages at the receiver and transmitter, respectively.</p>

<b>4. Receiver and Transmitter Processing</b>

<p><b>a. Receiver Processing (Combining)</b></p>
<p>At the receiver, the received signal vector \\(\mathbf{y}\\) is multiplied by the Hermitian transpose of \\(\mathbf{U}\\). This matrix, \\(\mathbf{U}^H\\), is called the <b>combiner matrix</b> or receive beamformer. This operation is a form of post-processing.</p>
\\[ \tilde{\mathbf{y}} = \mathbf{U}^H\mathbf{y} = \mathbf{U}^H(\mathbf{H}\mathbf{x} + \mathbf{n}) \\]
<p>Substituting the SVD of \\(\mathbf{H}\\):</p>
\\[ \tilde{\mathbf{y}} = \mathbf{U}^H(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^H)\mathbf{x} + \mathbf{U}^H\mathbf{n} \\]
<p>Since \\(\mathbf{U}^H\mathbf{U} = \mathbf{I}\\), the equation simplifies to:</p>
\\[ \tilde{\mathbf{y}} = \mathbf{\Sigma}\mathbf{V}^H\mathbf{x} + \tilde{\mathbf{n}} \\]
<p>where \\(\tilde{\mathbf{y}} = \mathbf{U}^H\mathbf{y}\\) is the combined signal vector and \\(\tilde{\mathbf{n}} = \mathbf{U}^H\mathbf{n}\\) is the processed noise vector.</p>

<p><b>b. Transmitter Processing (Precoding)</b></p>
<p>Before transmission, the original information symbols (e.g., QPSK, BPSK symbols), denoted by the vector \\(\tilde{\mathbf{x}}\\), are pre-processed. This involves multiplying \\(\tilde{\mathbf{x}}\\) by the matrix \\(\mathbf{V}\\), which is called the <b>precoding matrix</b>.</p>
\\[ \mathbf{x} = \mathbf{V}\tilde{\mathbf{x}} \\]
<p>The resulting vector \\(\mathbf{x}\\) is what is actually transmitted over the antennas.</p>

<b>5. The Result: A Decoupled System of Parallel Channels</b>
<p>By substituting the precoding step into the receiver's processed signal equation, we get the final system model:</p>
\\[ \tilde{\mathbf{y}} = \mathbf{\Sigma}\mathbf{V}^H(\mathbf{V}\tilde{\mathbf{x}}) + \tilde{\mathbf{n}} \\]
<p>Since \\(\mathbf{V}\\) is a unitary matrix, \\(\mathbf{V}^H\mathbf{V} = \mathbf{I}\\). This leads to a remarkably simple result:</p>
\\[ \tilde{\mathbf{y}} = \mathbf{\Sigma}\tilde{\mathbf{x}} + \tilde{\mathbf{n}} \\]
<p>Because \\(\mathbf{\Sigma}\\) is a diagonal matrix, this equation represents a set of T independent, parallel communication channels. Expanding the equation reveals this structure:</p>
\\[
\begin{pmatrix} \tilde{y}_1 \\ \tilde{y}_2 \\ \vdots \\ \tilde{y}_T \end{pmatrix} =
\begin{pmatrix} \sigma_1 & 0 & \dots & 0 \\ 0 & \sigma_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \sigma_T \end{pmatrix}
\begin{pmatrix} \tilde{x}_1 \\ \tilde{x}_2 \\ \vdots \\ \tilde{x}_T \end{pmatrix} +
\begin{pmatrix} \tilde{n}_1 \\ \tilde{n}_2 \\ \vdots \\ \tilde{n}_T \end{pmatrix}
\\]
<p>This is equivalent to T separate equations, free from interference:</p>
<ul>
    <li>\\(\tilde{y}_1 = \sigma_1 \tilde{x}_1 + \tilde{n}_1\\)</li>
    <li>\\(\tilde{y}_2 = \sigma_2 \tilde{x}_2 + \tilde{n}_2\\)</li>
    <li>...</li>
    <li>\\(\tilde{y}_T = \sigma_T \tilde{x}_T + \tilde{n}_T\\)</li>
</ul>
<p>Each equation represents an independent channel where the information symbol \\(\tilde{x}_i\\) is transmitted, scaled by a gain \\(\sigma_i\\) (the singular value), and corrupted by noise \\(\tilde{n}_i\\). The original complex, coupled system has been converted into T simple, parallel, single-antenna systems.</p>

<b>6. Spatial Multiplexing</b>
<p>This decoupling enables a technique called <b>spatial multiplexing</b>. The system can transmit T independent data streams (\\(\tilde{x}_1, \tilde{x}_2, \dots, \tilde{x}_T\\)) simultaneously over the same frequency and time resources by leveraging the spatial dimension (i.e., the different antenna paths).</p>
<p>This ability to send multiple data streams in parallel leads to a potential T-fold increase in the data rate or capacity of the wireless link compared to a single-antenna system. This is a fundamental principle behind the high-speed data transmission in 4G and 5G networks.</p>
<p>In summary, SVD provides a mathematical framework for designing the precoder (\\(\mathbf{V}\\)) and combiner (\\(\mathbf{U}^H\\)) that diagonalize the MIMO channel, eliminating interference and enabling spatial multiplexing for significantly enhanced performance.</p>
</div></div><h2>Weekly Summary</h2><div>
<p><b>Topic 1: Polynomial Fitting using Least Squares</b></p>
<p>This section explains how the method of least squares can be extended beyond simple linear regression to fit a polynomial of any degree \\(n\\) to a set of data points. This is a common technique for developing a non-linear approximation for an unknown functional relationship.</p>
<p><b>Key Takeaways:</b></p>
<p>
    An nth-degree polynomial model is given by \\(y \approx a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n\\). The goal is to find the optimal coefficients \\(a_0, a_1, \dots, a_n\\) using a set of \\(m\\) data points \\((x_i, y_i)\\).<br><br>
    The problem can be formulated as a system of linear equations, \\(\mathbf{y} = \mathbf{X}\mathbf{a} + \boldsymbol{\epsilon}\\), where:
    <ul>
        <li>\\(\mathbf{y}\\) is the \\(m \times 1\\) vector of observed values.</li>
        <li>\\(\mathbf{a}\\) is the \\((n+1) \times 1\\) vector of unknown polynomial coefficients.</li>
        <li>\\(\boldsymbol{\epsilon}\\) is the vector of model errors.</li>
        <li>\\(\mathbf{X}\\) is the \\(m \times (n+1)\\) matrix where the i-th row is \\([1, x_i, x_i^2, \dots, x_i^n]\\).</li>
    </ul>
    This transforms the non-linear fitting problem into a standard linear least squares problem. The solution that minimizes the squared error \\(\\| \mathbf{y} - \mathbf{X}\mathbf{a} \\|^2\\) is given by the familiar normal equation solution:
    \\[ \hat{\mathbf{a}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\]
</p>

<p><b>Topic 2: The Least Norm Solution</b></p>
<p>This module introduces the least norm solution, which is the counterpart to the least squares solution for <i>underdetermined</i> systems of linear equations. An underdetermined system is one where there are fewer equations than unknowns (\\(m < n\\)), represented by a "wide" matrix \\(\mathbf{A}\\).</p>
<p><b>Key Takeaways:</b></p>
<p>
    Underdetermined systems typically have an infinite number of solutions. If \\(\mathbf{x}\\) is a solution to \\(\mathbf{A}\mathbf{x} = \mathbf{y}\\), then \\(\mathbf{x} + \mathbf{u}\\) is also a solution for any vector \\(\mathbf{u}\\) in the null space of \\(\mathbf{A}\\).<br><br>
    To select a single, unique solution, an additional constraint is imposed: find the solution with the minimum energy or norm. The problem is formulated as:
    <br>
    <i>Minimize \\(\\| \mathbf{x} \\|^2\\) subject to \\(\mathbf{A}\mathbf{x} = \mathbf{y}\\)</i>.<br><br>
    The solution, derived using Lagrange multipliers, is the <b>least norm solution</b>:
    \\[ \mathbf{x}_{LN} = \mathbf{A}^T(\mathbf{A}\mathbf{A}^T)^{-1}\mathbf{y} \\]
    This requires the matrix \\(\mathbf{A}\\) to have full row rank, ensuring \\(\mathbf{A}\mathbf{A}^T\\) is invertible. The term \\(\mathbf{A}^T(\mathbf{A}\mathbf{A}^T)^{-1}\\) is the <b>right pseudo-inverse</b> of \\(\mathbf{A}\\).
</p>

<p><b>Topic 3: Application of Least Norm: Multi-user Beamforming</b></p>
<p>This section presents a practical application of the least norm solution in wireless communications, specifically for multi-user beamforming. The goal is to design a receiver with multiple antennas that can isolate a desired user's signal while completely nullifying interference from other users.</p>
<p><b>Key Takeaways:</b></p>
<p>
    In a multi-user scenario, a receiver beamformer (represented by a weight vector \\(\mathbf{w}\\)) is designed to achieve three objectives:
    <ol>
        <li>Maintain a fixed gain (e.g., unity) for the desired user's signal: \\(\mathbf{h}^H\mathbf{w} = 1\\).</li>
        <li>Force the gain for the interfering user's signal to zero (place a null): \\(\mathbf{g}^H\mathbf{w} = 0\\).</li>
        <li>Minimize the output noise power, which is proportional to \\(\\| \mathbf{w} \\|^2\\).</li>
    </ol>
    This problem can be expressed in matrix form as: <i>Minimize \\(\\| \mathbf{w} \\|^2\\) subject to \\(\mathbf{C}^H\mathbf{w} = [1, 0]^T\\)</i>, where \\(\mathbf{C}\\) is a matrix whose columns are the channel vectors \\(\mathbf{h}\\) and \\(\mathbf{g}\\).<br><br>
    This is precisely a least norm problem. The optimal beamformer \\(\mathbf{w}\\) is found using the least norm solution, demonstrating how this mathematical concept provides a direct solution to a significant engineering challenge in signal processing.
</p>

<p><b>Topic 4: Singular Value Decomposition (SVD)</b></p>
<p>The Singular Value Decomposition (SVD) is introduced as a fundamental and powerful factorization that applies to <i>any</i> \\(m \times n\\) matrix \\(\mathbf{H}\\), unlike the eigenvalue decomposition which is restricted to square matrices.</p>
<p><b>Key Takeaways:</b></p>
<p>
    The SVD of a matrix \\(\mathbf{H}\\) is given by \\(\mathbf{H} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^H\\), where:
    <ul>
        <li>\\(\mathbf{U}\\) is a matrix with orthonormal columns, called the <b>left singular vectors</b>. They are the eigenvectors of \\(\mathbf{H}\mathbf{H}^H\\).</li>
        <li>\\(\mathbf{V}\\) is a unitary matrix whose columns are the <b>right singular vectors</b>. They are the eigenvectors of \\(\mathbf{H}^H\mathbf{H}\\).</li>
        <li>\\(\boldsymbol{\Sigma}\\) is a diagonal matrix containing the <b>singular values</b> (\\(\sigma_i\\)). These values are real, non-negative, and arranged in decreasing order. The number of non-zero singular values equals the rank of the matrix \\(\mathbf{H}\\).</li>
    </ul>
    The SVD provides a deep insight into a matrix's structure, including its rank and its fundamental subspaces. It also provides a stable way to compute the pseudo-inverse: \\(\mathbf{H}^\dagger = \mathbf{V}\boldsymbol{\Sigma}^{-1}\mathbf{U}^H\\).
</p>

<p><b>Topic 5: Application of SVD: MIMO Wireless Systems</b></p>
<p>This module illustrates a key application of SVD in Multiple-Input Multiple-Output (MIMO) wireless systems. SVD is the mathematical tool that enables the conversion of a complex, coupled MIMO channel into a set of simple, parallel, independent sub-channels.</p>
<p><b>Key Takeaways:</b></p>
<p>
    A MIMO system is modeled as \\(\mathbf{y} = \mathbf{H}\mathbf{x} + \mathbf{n}\\), where the channel matrix \\(\mathbf{H}\\) causes interference between the different transmitted data streams in \\(\mathbf{x}\\).<br><br>
    By applying the SVD of the channel, \\(\mathbf{H} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^H\\), the system can be completely decoupled. This is achieved through:
    <ol>
        <li><b>Precoding</b> at the transmitter: The input symbols \\(\tilde{\mathbf{x}}\\) are multiplied by \\(\mathbf{V}\\) before transmission (\\(\mathbf{x} = \mathbf{V}\tilde{\mathbf{x}}\\)).</li>
        <li><b>Combining</b> at the receiver: The received signal \\(\mathbf{y}\\) is multiplied by \\(\mathbf{U}^H\\) (\\(\tilde{\mathbf{y}} = \mathbf{U}^H\mathbf{y}\\)).</li>
    </ol>
    The resulting system is \\(\tilde{\mathbf{y}} = \boldsymbol{\Sigma}\tilde{\mathbf{x}} + \tilde{\mathbf{n}}\\). Since \\(\boldsymbol{\Sigma}\\) is diagonal, the system is now a set of parallel channels where the i-th output \\(\tilde{y}_i\\) only depends on the i-th input \\(\tilde{x}_i\\) (i.e., \\(\tilde{y}_i = \sigma_i \tilde{x}_i + \tilde{n}_i\\)).<br><br>
    This technique, known as <b>spatial multiplexing</b>, allows multiple data streams to be transmitted simultaneously over the same frequency, drastically increasing data rates and forming the foundation of modern high-speed wireless standards like 4G and 5G.
</p>
</div><h2>Assignment Explanation</h2><div>
<p>Here is an explanation of the questions and answers from the assignment.</p>

<hr>

<h3>Question 1</h3>
<p><b>Question:</b> The eigenvectors of the matrix below are<br>
\\( A = \begin{bmatrix} 0 & -2 \\ 1 & -3 \end{bmatrix} \\)</p>

<p><b>Explanation:</b></p>
<p>To find the eigenvectors of a matrix \\(A\\), we first need to find its eigenvalues by solving the characteristic equation \\(\det(A - \lambda I) = 0\\), where \\(I\\) is the identity matrix and \\(\lambda\\) represents the eigenvalues.</p>
<ol>
    <li><b>Find Eigenvalues:</b>
    \\[ \det(A - \lambda I) = \det \left( \begin{bmatrix} 0-\lambda & -2 \\ 1 & -3-\lambda \end{bmatrix} \right) = 0 \\]
    \\[ (-\lambda)(-3-\lambda) - (1)(-2) = 0 \\]
    \\[ 3\lambda + \lambda^2 + 2 = 0 \\]
    \\[ (\lambda + 1)(\lambda + 2) = 0 \\]
    The eigenvalues are \\(\lambda_1 = -1\\) and \\(\lambda_2 = -2\\).</li>
    <li><b>Find Eigenvector for \\(\lambda_1 = -1\\):</b>
    We solve \\((A - \lambda_1 I)\mathbf{v} = \mathbf{0}\\):
    \\[ \begin{bmatrix} 0 - (-1) & -2 \\ 1 & -3 - (-1) \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\]
    \\[ \begin{bmatrix} 1 & -2 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\]
    This gives the equation \\(v_1 - 2v_2 = 0\\), or \\(v_1 = 2v_2\\). A possible eigenvector is \\(\mathbf{v}_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}\\).</li>
    <li><b>Find Eigenvector for \\(\lambda_2 = -2\\):</b>
    We solve \\((A - \lambda_2 I)\mathbf{v} = \mathbf{0}\\):
    \\[ \begin{bmatrix} 0 - (-2) & -2 \\ 1 & -3 - (-2) \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\]
    \\[ \begin{bmatrix} 2 & -2 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\]
    This gives the equation \\(v_1 - v_2 = 0\\), or \\(v_1 = v_2\\). A possible eigenvector is \\(\mathbf{v}_2 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\\).</li>
</ol>
<p>The correct eigenvectors are any non-zero scalar multiples of \\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\\) and \\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\\). None of the options provided in the assignment match this result.<br><br>
<i>Note: The accepted answer provided, \\( \begin{bmatrix} 1 \\ -2 \end{bmatrix} \\) and \\( \begin{bmatrix} 1 \\ -1 \end{bmatrix} \\), are the eigenvectors for the transpose of the matrix, \\(A^T = \begin{bmatrix} 0 & 1 \\ -2 & -3 \end{bmatrix}\\). This indicates a likely error in the question or the accepted answer.</i></p>

<p><b>Correct Answer (based on calculation):</b> Any pair of vectors that are scalar multiples of \\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\\) and \\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\\).</p>

<hr>

<h3>Question 2</h3>
<p><b>Question:</b> Consider the linear system of equations \\(\mathbf{y} = A\mathbf{x}\\), where
\\( A = \begin{bmatrix} -1 & 1 \\ 1 & -1 \\ 1 & -1 \\ 1 & -1 \end{bmatrix}, \mathbf{y} = \begin{bmatrix} 2 \\ -1 \\ -3 \\ 2 \end{bmatrix} \\). The least-squares (LS) solution for the system of equations is?</p>

<p><b>Explanation:</b></p>
<p>The least-squares solution \\(\mathbf{x}_{LS}\\) minimizes the squared error \\(||\mathbf{y} - A\mathbf{x}||^2\\). It is found by solving the normal equations: \\(A^T A \mathbf{x} = A^T \mathbf{y}\\).</p>
<ol>
    <li><b>Calculate \\(A^T A\\):</b>
    \\[ A^T A = \begin{bmatrix} -1 & 1 & 1 & 1 \\ 1 & -1 & -1 & -1 \end{bmatrix} \begin{bmatrix} -1 & 1 \\ 1 & -1 \\ 1 & -1 \\ 1 & -1 \end{bmatrix} = \begin{bmatrix} 4 & -4 \\ -4 & 4 \end{bmatrix} \\]</li>
    <li><b>Calculate \\(A^T \mathbf{y}\\):</b>
    \\[ A^T \mathbf{y} = \begin{bmatrix} -1 & 1 & 1 & 1 \\ 1 & -1 & -1 & -1 \end{bmatrix} \begin{bmatrix} 2 \\ -1 \\ -3 \\ 2 \end{bmatrix} = \begin{bmatrix} -2-1-3+2 \\ 2+1+3-2 \end{bmatrix} = \begin{bmatrix} -4 \\ 4 \end{bmatrix} \\]</li>
    <li><b>Solve the Normal Equations:</b>
    \\[ \begin{bmatrix} 4 & -4 \\ -4 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} -4 \\ 4 \end{bmatrix} \\]
    Both rows give the same equation: \\(4x_1 - 4x_2 = -4\\), which simplifies to \\(x_1 - x_2 = -1\\).</li>
</ol>
<p>Since the columns of \\(A\\) are linearly dependent, \\(A^T A\\) is singular, and there are infinitely many least-squares solutions. The problem usually asks for the unique minimum-norm least-squares solution. The general solution is \\(x_1 = x_2 - 1\\). We can find the minimum norm solution by minimizing \\(||\mathbf{x}||^2 = x_1^2 + x_2^2 = (x_2-1)^2 + x_2^2\\). The minimum occurs at \\(x_2 = 1/2\\), which gives \\(x_1 = -1/2\\). So, the minimum norm LS solution is \\(\mathbf{x} = \begin{bmatrix} -1/2 \\ 1/2 \end{bmatrix}\\).</p>
<p><i>Note: The accepted answer, \\( \frac{1}{2} \begin{bmatrix} -4 \\ -1 \end{bmatrix} \\), does not satisfy the normal equation \\(x_1 - x_2 = -1\\) (since \\(-2 - (-0.5) = -1.5 \neq -1\\)). Therefore, the provided accepted answer is incorrect.</i></p>

<p><b>Correct Answer (minimum norm LS solution):</b> \\( \begin{bmatrix} -1/2 \\ 1/2 \end{bmatrix} \\)</p>

<hr>

<h3>Question 3</h3>
<p><b>Question:</b> Let the covariance estimate of the data vectors obtained during Principal Component Analysis (PCA) be denoted by \\(R\\). The vector \\(\mathbf{v}_1\\) corresponding to the direction of the largest principal component is given as?</p>

<p><b>Explanation:</b></p>
<p>Principal Component Analysis (PCA) is a technique used to identify the directions of maximum variance in a dataset. These directions are called principal components. Mathematically, this is achieved by performing an eigendecomposition of the data's covariance matrix, \\(R\\).</p>
<ul>
    <li>The <b>eigenvectors</b> of the covariance matrix \\(R\\) give the directions of the principal components.</li>
    <li>The <b>eigenvalues</b> of \\(R\\) represent the amount of variance in the data along the corresponding eigenvector's direction.</li>
</ul>
<p>The largest principal component is the direction that captures the most variance in the data. Therefore, it corresponds to the eigenvector associated with the largest eigenvalue of the covariance matrix \\(R\\).</p>

<p><b>Correct Answer:</b> Eigenvector of \\(R\\) corresponding to the largest eigenvalue</p>

<hr>

<h3>Question 4</h3>
<p><b>Question:</b> Consider the linear system of equations \\(\mathbf{y} = A\mathbf{x}\\), where
\\( A = \begin{bmatrix} 1 & 1 & -1 & 1 \\ -1 & -1 & 1 & -1 \end{bmatrix}, \mathbf{y} = \begin{bmatrix} -2 \\ 2 \end{bmatrix} \\). The least norm solution for this system of equations is?</p>

<p><b>Explanation:</b></p>
<p>This is an underdetermined system (more unknowns than equations), so it has infinite solutions. The "least norm" solution is the unique solution \\(\mathbf{x}_{LN}\\) that has the smallest Euclidean norm (\\(||\mathbf{x}||\_2\\)).</p>
<ol>
    <li><b>Simplify the system:</b> The second row of \\(A\\) is -1 times the first row, and \\(y_2 = -1 \cdot y_1\\). So the system reduces to a single equation:
    \\[ x_1 + x_2 - x_3 + x_4 = -2 \\]</li>
    <li><b>Find the Least Norm Solution:</b> The least norm solution lies entirely in the row space of \\(A\\). The row space is spanned by the vector \\(\mathbf{r} = \begin{bmatrix} 1 \\ 1 \\ -1 \\ 1 \end{bmatrix}\\). Therefore, the solution must be of the form \\(\mathbf{x}_{LN} = c \cdot \mathbf{r}\\) for some scalar \\(c\\).</li>
    <li><b>Solve for \\(c\\):</b> Substitute \\(\mathbf{x}_{LN}\\) back into the system \\(A\mathbf{x} = \mathbf{y}\\):
    \\[ A(c \cdot \mathbf{r}) = c(A\mathbf{r}) = \mathbf{y} \\]
    \\[ c \begin{pmatrix} 1 & 1 & -1 & 1 \\ -1 & -1 & 1 & -1 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ -1 \\ 1 \end{pmatrix} = \begin{pmatrix} -2 \\ 2 \end{pmatrix} \\]
    \\[ c \begin{pmatrix} 1+1+1+1 \\ -1-1-1-1 \end{pmatrix} = c \begin{pmatrix} 4 \\ -4 \end{pmatrix} = \begin{pmatrix} -2 \\ 2 \end{pmatrix} \\]
    From this, we find \\(4c = -2\\), so \\(c = -1/2\\).</li>
    <li><b>The solution is:</b>
    \\[ \mathbf{x}_{LN} = -\frac{1}{2} \begin{bmatrix} 1 \\ 1 \\ -1 \\ 1 \end{bmatrix} = \begin{bmatrix} -0.5 \\ -0.5 \\ 0.5 \\ -0.5 \end{bmatrix} \\]</li>
</ol>
<p><i>Note: The accepted answer in the assignment, \\(\frac{1}{4} \begin{bmatrix} 2 \\ 6 \\ -2 \\ -6 \end{bmatrix}\\), is incorrect because it is not a solution to the system (\\(A\mathbf{x}\\) gives \\(\begin{bmatrix} 1 \\ -1 \end{bmatrix}\\), not \\(\begin{bmatrix} -2 \\ 2 \end{bmatrix}\\)).</i></p>

<p><b>Correct Answer (based on calculation):</b> \\(-\frac{1}{2} \begin{bmatrix} 1 \\ 1 \\ -1 \\ 1 \end{bmatrix}\\) </p>

<hr>

<h3>Question 5</h3>
<p><b>Question:</b> Consider a wide matrix \\(A\\), with full row rank. Its pseudo-inverse is given as?</p>

<p><b>Explanation:</b></p>
<p>A "wide" matrix has more columns than rows (\\(m \times n\\) with \\(m < n\\)). "Full row rank" means its rows are linearly independent, and its rank is \\(m\\). For such a matrix, the system \\(A\mathbf{x} = \mathbf{y}\\) is underdetermined. The pseudo-inverse \\(A^+\\) is used to find the minimum norm solution. This pseudo-inverse is also known as the <b>right inverse</b> because \\(AA^+ = I\\). The formula for the right inverse is \\(A^+ = A^T(AA^T)^{-1}\\). The term \\(AA^T\\) is an \\(m \times m\\) matrix, and it is invertible because \\(A\\) has full row rank.</p>

<p><b>Correct Answer:</b> \\(A^T(AA^T)^{-1}\\)</p>

<hr>

<h3>Question 6</h3>
<p><b>Question:</b> Consider the linear system of equations \\(\mathbf{y} = A\mathbf{x}\\). The least-squares (LS) solution for the system of equations is?</p>

<p><b>Explanation:</b></p>
<p>The least-squares (LS) solution is typically sought for an overdetermined system (more equations than unknowns), where an exact solution may not exist. The LS solution \\(\mathbf{x}_{LS}\\) minimizes the norm of the error, \\(||\mathbf{y} - A\mathbf{x}||_2\\). This solution is obtained by solving the <b>normal equations</b>:
\\[ A^T A \mathbf{x} = A^T \mathbf{y} \\]
If the matrix \\(A\\) has full column rank, then the matrix \\(A^T A\\) is invertible. We can then solve for \\(\mathbf{x}\\) directly:
\\[ \mathbf{x}_{LS} = (A^T A)^{-1}A^T \mathbf{y} \\]
The term \\((A^T A)^{-1}A^T\\) is the pseudo-inverse (or left inverse) of \\(A\\).</p>

<p><b>Correct Answer:</b> \\((A^T A)^{-1}A^T \mathbf{y}\\)</p>

<hr>

<h3>Question 7</h3>
<p><b>Question:</b> Consider the vectors below. Using the Gram-Schmidt procedure, a set of orthonormal basis vectors for the same subspace is given as?
\\( \mathbf{x}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, \mathbf{x}_2 = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} \\)</p>

<p><b>Explanation:</b></p>
<p>The Gram-Schmidt procedure transforms a set of linearly independent vectors into an orthonormal set spanning the same subspace.</p>
<ol>
    <li><b>First vector \\(\mathbf{v}_1\\):</b> Normalize \\(\mathbf{x}_1\\).
    \\[ \mathbf{u}_1 = \mathbf{x}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} \\]
    \\[ ||\mathbf{u}_1|| = \sqrt{1^2+1^2+1^2+1^2} = \sqrt{4} = 2 \\]
    \\[ \mathbf{v}_1 = \frac{\mathbf{u}_1}{||\mathbf{u}_1||} = \frac{1}{2} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} \\]</li>
    <li><b>Second vector \\(\mathbf{v}_2\\):</b> First, subtract the projection of \\(\mathbf{x}_2\\) onto \\(\mathbf{u}_1\\) to get an orthogonal vector \\(\mathbf{u}_2\\).
    \\[ \mathbf{u}_2 = \mathbf{x}_2 - \text{proj}_{\mathbf{u}_1}(\mathbf{x}_2) = \mathbf{x}_2 - \frac{\mathbf{x}_2 \cdot \mathbf{u}_1}{\mathbf{u}_1 \cdot \mathbf{u}_1} \mathbf{u}_1 \\]
    \\[ \mathbf{x}_2 \cdot \mathbf{u}_1 = 1(1) + 2(1) + 3(1) + 4(1) = 10 \\]
    \\[ \mathbf{u}_1 \cdot \mathbf{u}_1 = 4 \\]
    \\[ \mathbf{u}_2 = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} - \frac{10}{4} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} - \begin{bmatrix} 2.5 \\ 2.5 \\ 2.5 \\ 2.5 \end{bmatrix} = \begin{bmatrix} -1.5 \\ -0.5 \\ 0.5 \\ 1.5 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} -3 \\ -1 \\ 1 \\ 3 \end{bmatrix} \\]</li>
    <li><b>Normalize \\(\mathbf{u}_2\\):</b>
    \\[ ||\mathbf{u}_2|| = \frac{1}{2} \sqrt{(-3)^2+(-1)^2+1^2+3^2} = \frac{1}{2}\sqrt{9+1+1+9} = \frac{\sqrt{20}}{2} = \sqrt{5} \\]
    \\[ \mathbf{v}_2 = \frac{\mathbf{u}_2}{||\mathbf{u}_2||} = \frac{1}{\sqrt{5}} \left( \frac{1}{2} \begin{bmatrix} -3 \\ -1 \\ 1 \\ 3 \end{bmatrix} \right) = \frac{1}{2\sqrt{5}} \begin{bmatrix} -3 \\ -1 \\ 1 \\ 3 \end{bmatrix} \\]
    This can also be written as \\( \frac{1}{\sqrt{20}} \begin{bmatrix} -3 \\ -1 \\ 1 \\ 3 \end{bmatrix} \\).</li>
</ol>
<p><b>Correct Answer:</b> \\( \mathbf{v}_1 = \frac{1}{2} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, \mathbf{v}_2 = \frac{1}{2\sqrt{5}} \begin{bmatrix} -3 \\ -1 \\ 1 \\ 3 \end{bmatrix} \\)</p>

<hr>

<h3>Question 8</h3>
<p><b>Question:</b> Consider the matrix \\(A\\) defined as
\\( A = \begin{bmatrix} -1 & -1 & 1 & 1 \\ -1 & 1 & -1 & 1 \end{bmatrix} \\). The pseudo-inverse of the matrix \\(A\\) is?</p>

<p><b>Explanation:</b></p>
<p>The matrix \\(A\\) is a 2x4 wide matrix. Its rows are linearly independent, so it has full row rank. We use the formula for the right pseudo-inverse: \\(A^+ = A^T(AA^T)^{-1}\\).</p>
<ol>
    <li><b>Calculate \\(AA^T\\):</b>
    \\[ AA^T = \begin{bmatrix} -1 & -1 & 1 & 1 \\ -1 & 1 & -1 & 1 \end{bmatrix} \begin{bmatrix} -1 & -1 \\ -1 & 1 \\ 1 & -1 \\ 1 & 1 \end{bmatrix} \\]
    \\[ = \begin{bmatrix} (1+1+1+1) & (1-1-1+1) \\ (1-1-1+1) & (1+1+1+1) \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} = 4I \\]</li>
    <li><b>Calculate \\((AA^T)^{-1}\\):</b>
    \\[ (AA^T)^{-1} = (4I)^{-1} = \frac{1}{4}I = \begin{bmatrix} 1/4 & 0 \\ 0 & 1/4 \end{bmatrix} \\]</li>
    <li><b>Calculate \\(A^+ = A^T(AA^T)^{-1}\\):</b>
    \\[ A^+ = \begin{bmatrix} -1 & -1 \\ -1 & 1 \\ 1 & -1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1/4 & 0 \\ 0 & 1/4 \end{bmatrix} = \frac{1}{4} \begin{bmatrix} -1 & -1 \\ -1 & 1 \\ 1 & -1 \\ 1 & 1 \end{bmatrix} \\]</li>
</ol>
<p><b>Correct Answer:</b> \\( \frac{1}{4} \begin{bmatrix} -1 & -1 \\ -1 & 1 \\ 1 & -1 \\ 1 & 1 \end{bmatrix} \\)</p>

<hr>

<h3>Question 9</h3>
<p><b>Question:</b> Consider a tall matrix \\(A\\), with full column rank. Its pseudo-inverse is given as?</p>

<p><b>Explanation:</b></p>
<p>A "tall" matrix has more rows than columns (\\(m \times n\\) with \\(m > n\\)). "Full column rank" means its columns are linearly independent, and its rank is \\(n\\). For such a matrix, the pseudo-inverse \\(A^+\\) is used to find the least-squares solution to \\(A\mathbf{x} = \mathbf{y}\\). This pseudo-inverse is also known as the <b>left inverse</b> because \\(A^+A = I\\). The formula for the left inverse is \\(A^+ = (A^T A)^{-1}A^T\\). The term \\(A^T A\\) is an \\(n \times n\\) matrix, and it is invertible because \\(A\\) has full column rank.</p>

<p><b>Correct Answer:</b> \\((A^T A)^{-1}A^T\\)</p>

<hr>

<h3>Question 10</h3>
<p><b>Question:</b> Consider the linear system of equations \\(\mathbf{y} = A\mathbf{x}\\). The least norm (LN) solution for the system of equations is?</p>

<p><b>Explanation:</b></p>
<p>The least norm (LN) solution is sought for an underdetermined system (e.g., a wide matrix \\(A\\) with full row rank) that is consistent. Among the infinite possible solutions, the LN solution is the one with the minimum Euclidean norm. This solution is orthogonal to the null space of \\(A\\), meaning it lies entirely in the row space of \\(A\\). The formula is derived using the pseudo-inverse for wide matrices (the right inverse):
\\[ \mathbf{x}_{LN} = A^+ \mathbf{y} = A^T(AA^T)^{-1}\mathbf{y} \\]
This formula projects the vector \\(\mathbf{y}\\) back onto the row space of \\(A\\) to construct the unique minimum-norm solution.</p>

<p><b>Correct Answer:</b> \\(A^T(AA^T)^{-1}\mathbf{y}\\)</p>

</div></div><div class="week" id="week_7"><h1 class="week-title">Week 7</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 31 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 31 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript regarding the optimization of Multiple-Input Multiple-Output (MIMO) wireless communication systems using Singular Value Decomposition (SVD).</p>

<b>1. Decoupling the MIMO Channel with SVD</b>
<p>The core idea presented is that a complex MIMO system, where signals from multiple transmit antennas interfere with each other at multiple receive antennas, can be transformed into a set of simple, independent, parallel channels. This transformation is achieved by applying SVD to the MIMO channel matrix.</p>
<ul>
    <li><b>Pre-coding at the Transmitter:</b> The input signals are processed before transmission.</li>
    <li><b>Post-processing at the Receiver:</b> The received signals are combined in a specific way.</li>
</ul>
<p>This process, known as <b>spatial multiplexing</b>, effectively decouples the system. Instead of one complex channel, we get \\(t\\) parallel sub-channels, where \\(t\\) is the number of singular values. Each of these sub-channels can be analyzed independently.</p>
<p>The mathematical model for the \\(i\\)-th decoupled sub-channel is given by:</p>
\\[ y_i^{\sim} = \sigma_i x_i^{\sim} + n_i^{\sim} \\]
<p>Where:</p>
<ul>
    <li>\\( y_i^{\sim} \\) is the output of the \\(i\\)-th sub-channel after receiver processing.</li>
    <li>\\( x_i^{\sim} \\) is the symbol transmitted on the \\(i\\)-th sub-channel after transmitter pre-coding.</li>
    <li>\\( \sigma_i \\) is the <b>amplitude gain</b> of the \\(i\\)-th sub-channel. This gain is directly given by the \\(i\\)-th singular value of the original MIMO channel matrix.</li>
    <li>\\( n_i^{\sim} \\) is the noise affecting the \\(i\\)-th sub-channel.</li>
    <li>This model holds for \\( i = 1, 2, \dots, t \\).</li>
</ul>

<b>2. Rate of Transmission and the Shannon Capacity Formula</b>
<p>To determine the performance of these sub-channels, we analyze their Signal-to-Noise Ratio (SNR) and the maximum rate at which data can be transmitted reliably.</p>
<ul>
    <li><b>Transmit Power:</b> The power allocated to the \\(i\\)-th sub-channel is denoted by \\( P_i = E\left[|x_i^{\sim}|^2\right] \\), where \\(E[\cdot]\\) is the expected value.</li>
    <li><b>Noise Power:</b> The power of the noise on each sub-channel is assumed to be the same, denoted by \\( \sigma^2 \\).</li>
</ul>
<p>The output SNR for the \\(i\\)-th sub-channel is calculated as:</p>
\\[ \text{SNR}_i = \frac{\text{Signal Power}}{\text{Noise Power}} = \frac{\sigma_i^2 P_i}{\sigma^2} \\]
<p>Note that since \\(\sigma_i\\) is the amplitude gain, the power gain is \\(\sigma_i^2\\).</p>
<p>The maximum rate of error-free transmission for any channel is given by the <b>Shannon Capacity Formula</b>. For the \\(i\\)-th sub-channel, this rate, \\(R_i\\), is:</p>
\\[ R_i = \log_2(1 + \text{SNR}_i) = \log_2\left(1 + \frac{\sigma_i^2 P_i}{\sigma^2}\right) \\]
<p>The total rate for the entire MIMO system, known as the <b>sum rate</b>, is the sum of the individual rates of the parallel sub-channels:</p>
\\[ R_{\text{sum}} = \sum_{i=1}^{t} R_i = \sum_{i=1}^{t} \log_2\left(1 + \frac{\sigma_i^2 P_i}{\sigma^2}\right) \\]

<b>3. The Rate Maximization Problem</b>
<p>The goal is to maximize this sum rate. However, there is a practical limitation: the total power available at the transmitter is finite. This introduces a constraint on our optimization problem.</p>
<p>The <b>Total Power Constraint</b> is given by:</p>
\\[ \sum_{i=1}^{t} P_i \le P_0 \\]
<p>where \\(P_0\\) is the total available transmit power.</p>
<p>The formal optimization problem is therefore stated as:</p>
<ul>
    <li><b>Maximize:</b> \\( \quad \sum_{i=1}^{t} \log_2\left(1 + \frac{\sigma_i^2 P_i}{\sigma^2}\right) \\) </li>
    <li><b>Subject to:</b> \\( \quad \sum_{i=1}^{t} P_i \le P_0 \quad \) and \\( \quad P_i \ge 0 \quad \text{for all } i \\)</li>
</ul>

<b>4. Solving with Lagrange Multipliers and the Water-Filling Algorithm</b>
<p>This constrained optimization problem can be solved using the method of Lagrange multipliers. We construct the Lagrangian function \\(\mathcal{L}\\):</p>
\\[ \mathcal{L} = \sum_{i=1}^{t} \log_2\left(1 + \frac{\sigma_i^2 P_i}{\sigma^2}\right) + \lambda \left(P_0 - \sum_{i=1}^{t} P_i\right) \\]
<p>To find the optimal values of \\(P_i\\), we take the partial derivative of \\(\mathcal{L}\\) with respect to each \\(P_i\\) and set it to zero:</p>
\\[ \frac{\partial \mathcal{L}}{\partial P_i} = \frac{1}{\ln(2)} \cdot \frac{\sigma_i^2 / \sigma^2}{1 + (\sigma_i^2 P_i / \sigma^2)} - \lambda = 0 \\]
<p>Solving this equation for \\(P_i\\) gives the optimal power allocation for the \\(i\\)-th channel:</p>
\\[ P_i = \frac{1}{\lambda \ln(2)} - \frac{\sigma^2}{\sigma_i^2} \\]
<p>Since power cannot be negative (\\(P_i \ge 0\\)), we introduce a notation \\( [x]^+ = \max(x, 0) \\) to enforce this. The final solution is:</p>
\\[ P_i = \left( \frac{1}{\mu} - \frac{\sigma^2}{\sigma_i^2} \right)^+ \\]
<p>where \\(\mu = \lambda \ln(2)\\) is a constant. This solution is known as the <b>Water-Filling Algorithm</b>.</p>
<p><b>Intuition behind Water-Filling:</b></p>
<p>This formula has a powerful analogy:</p>
<ul>
    <li>Imagine a container whose floor has an uneven surface. For each sub-channel \\(i\\), there is a "step" at a height of \\( \frac{\sigma^2}{\sigma_i^2} \\). This value represents the inverse of the channel quality; a better channel (larger \\(\sigma_i\\)) has a lower step.</li>
    <li>The term \\( \frac{1}{\mu} \\) represents a uniform "water level".</li>
    <li>We "pour" a total amount of water, corresponding to the total power \\(P_0\\), into this container.</li>
    <li>The power \\(P_i\\) allocated to each channel is the depth of the water above its corresponding step.</li>
    <li>If the water level \\(\frac{1}{\mu}\\) is below a channel's step \\(\frac{\sigma^2}{\sigma_i^2}\\), no water covers that step, meaning no power (\\(P_i=0\\)) is allocated to that very weak channel.</li>
</ul>
<p>In essence, the algorithm allocates more power to stronger sub-channels (those with higher \\(\sigma_i\\)) and less or no power to weaker sub-channels, thereby maximizing the total data rate for a fixed total power budget.</p>

<b>5. A Practical Example</b>
<p>The transcript provides a specific example to illustrate this process.</p>
<ul>
    <li><b>MIMO Channel (H):</b> A \\(4 \times 2\\) matrix is given.</li>
    <li><b>Noise Power:</b> \\( \sigma^2 = 16 \\)</li>
    <li><b>Total Power:</b> \\( P_0 = 5 \\)</li>
</ul>
<p>From a previous SVD of the channel matrix \\(H\\), the singular values are found to be:</p>
\\[ \sigma_1 = 4, \quad \sigma_2 = 2 \\]
<p>The water-filling formulas for the two power allocations \\(P_1\\) and \\(P_2\\) are:</p>
\\[ P_1 = \left( \frac{1}{\mu} - \frac{\sigma^2}{\sigma_1^2} \right)^+ = \left( \frac{1}{\mu} - \frac{16}{4^2} \right)^+ = \left( \frac{1}{\mu} - 1 \right)^+ \\]
\\[ P_2 = \left( \frac{1}{\mu} - \frac{\sigma^2}{\sigma_2^2} \right)^+ = \left( \frac{1}{\mu} - \frac{16}{2^2} \right)^+ = \left( \frac{1}{\mu} - 4 \right)^+ \\]
<p>We use the total power constraint to find the "water level" \\(\frac{1}{\mu}\\). Assuming both powers are non-zero:</p>
\\[ P_1 + P_2 = P_0 \\]
\\[ \left(\frac{1}{\mu} - 1\right) + \left(\frac{1}{\mu} - 4\right) = 5 \\]
\\[ \frac{2}{\mu} - 5 = 5 \implies \frac{2}{\mu} = 10 \implies \frac{1}{\mu} = 5 \\]
<p>Now, we substitute this water level back to find the optimal powers:</p>
\\[ P_1 = 5 - 1 = 4 \\]
\\[ P_2 = 5 - 4 = 1 \\]
<p>Since both \\(P_1=4\\) and \\(P_2=1\\) are positive, this is a valid solution. The optimal power allocation to maximize the data rate is to assign 4 units of power to the first sub-channel and 1 unit to the second. This demonstrates how the better channel (with \\(\sigma_1=4\\)) receives more power.</p>
</div></div><div class="chapter" id="Lecture 32 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 32 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts and formulas discussed in the transcript, focusing on the application of Singular Value Decomposition (SVD) in Principal Component Analysis (PCA) and for creating low-rank matrix approximations.</p>

<b><h3>1. Singular Value Decomposition (SVD) in Principal Component Analysis (PCA)</h3></b>

<p>Principal Component Analysis (PCA) is a fundamental technique in machine learning and data analysis used for dimensionality reduction. Its primary goal is to identify the most significant directions, or "principal axes," in a dataset—those along which the data exhibits the maximum variance or spread.</p>

<b><h4>The PCA Procedure</h4></b>
<p>The standard procedure for PCA is outlined as follows:</p>
<ol>
    <li>
        <b>Data Representation:</b>
        <p>The dataset consists of \\(n\\) data vectors, \\(\tilde{\mathbf{x}}_1, \tilde{\mathbf{x}}_2, \dots, \tilde{\mathbf{x}}_n\\). Each vector \\(\tilde{\mathbf{x}}_i\\) is of size \\(m \times 1\\), where \\(m\\) is the number of features and \\(n\\) is the number of observations or experiments.</p>
    </li>
    <li>
        <b>Mean Centering:</b>
        <p>The first step is to compute the mean of the dataset and subtract it from each data vector. This process centers the data around the origin.</p>
        <ul>
            <li><b>Sample Mean (\\(\bar{\boldsymbol{\mu}}\\)):</b> The mean vector is estimated by averaging all data vectors.
            \\[ \bar{\boldsymbol{\mu}} = \frac{1}{n} \sum_{i=1}^{n} \tilde{\mathbf{x}}_i \\]
            </li>
            <li><b>Mean-Adjusted Data (\\(\bar{\mathbf{x}}_i\\)):</b> Each original data vector is adjusted by subtracting the sample mean.
            \\[ \bar{\mathbf{x}}_i = \tilde{\mathbf{x}}_i - \bar{\boldsymbol{\mu}} \\]
            </li>
        </ul>
    </li>
    <li>
        <b>Covariance Matrix Estimation:</b>
        <p>The next step is to compute the sample covariance matrix, \\(\mathbf{R}_x\\), which measures the relationships between different features. A high covariance between two features indicates they vary together, while a low covariance suggests they are independent.</p>
        \\[ \mathbf{R}_x = \frac{1}{n-1} \sum_{i=1}^{n} \bar{\mathbf{x}}_i \bar{\mathbf{x}}_i^T \\]
        <p>This can be expressed in matrix form. Let \\(\bar{\mathbf{X}}\\) be the \\(n \times m\\) data matrix where each row is a mean-centered data vector \\(\bar{\mathbf{x}}_i^T\\):</p>
        \\[ \bar{\mathbf{X}} = \begin{pmatrix} \bar{\mathbf{x}}_1^T \\ \bar{\mathbf{x}}_2^T \\ \vdots \\ \bar{\mathbf{x}}_n^T \end{pmatrix} \\]
        <p>The covariance matrix can then be written as:</p>
        \\[ \mathbf{R}_x = \frac{1}{n-1} \bar{\mathbf{X}}^T \bar{\mathbf{X}} \\]
        <p>For convenience, a scaled data matrix \\(\mathbf{X}\\) is introduced:</p>
        \\[ \mathbf{X} = \frac{1}{\sqrt{n-1}} \bar{\mathbf{X}} \\]
        <p>This simplifies the covariance matrix expression to:</p>
        \\[ \mathbf{R}_x = \mathbf{X}^T \mathbf{X} \\]
    </li>
    <li>
        <b>Finding Principal Axes (Eigenvalue Decomposition):</b>
        <p>Traditionally, the principal axes are found by performing an eigenvalue decomposition of the covariance matrix \\(\mathbf{R}_x\\). The eigenvectors of \\(\mathbf{R}_x\\) point in the directions of variance, and the corresponding eigenvalues quantify the amount of variance in those directions. The principal axes are the \\(p\\) eigenvectors (\\(\bar{\mathbf{v}}_1, \bar{\mathbf{v}}_2, \dots, \bar{\mathbf{v}}_p\\)) corresponding to the \\(p\\) largest eigenvalues.</p>
    </li>
</ol>

<b><h4>Connecting PCA to SVD</h4></b>
<p>The transcript highlights a crucial connection between PCA and SVD that provides a more direct and often numerically stable method for finding the principal axes.</p>
<ul>
    <li><b>Key Property:</b> The eigenvectors of the matrix \\(\mathbf{X}^T \mathbf{X}\\) (which is the covariance matrix \\(\mathbf{R}_x\\)) are precisely the <b>right singular vectors</b> of the matrix \\(\mathbf{X}\\).</li>
</ul>
<p>This means that instead of forming the covariance matrix \\(\mathbf{X}^T \mathbf{X}\\) and then finding its eigenvectors, we can directly compute the SVD of the scaled data matrix \\(\mathbf{X}\\):</p>
\\[ \mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T \\]
<p>Here:</p>
<ul>
    <li>\\(\mathbf{U}\\) is an \\(n \times n\\) matrix of left singular vectors.</li>
    <li>\\(\boldsymbol{\Sigma}\\) is an \\(n \times m\\) diagonal matrix of singular values (\\(\sigma_1 \ge \sigma_2 \ge \dots \ge 0\\)).</li>
    <li>\\(\mathbf{V}\\) is an \\(m \times m\\) matrix whose columns (\\(\bar{\mathbf{v}}_1, \bar{\mathbf{v}}_2, \dots, \bar{\mathbf{v}}_m\\)) are the <b>right singular vectors</b>.</li>
</ul>
<p>The columns of \\(\mathbf{V}\\) are the principal axes of the data. The singular values \\(\sigma_i\\) are related to the eigenvalues \\(\lambda_i\\) of \\(\mathbf{R}_x\\) by \\(\sigma_i = \sqrt{\lambda_i}\\). Therefore, the right singular vectors corresponding to the largest singular values are the principal axes representing the directions of maximum variance.</p>

<b><h4>Obtaining Principal Components</h4></b>
<p>The principal components of a data vector are its coordinates in the new basis defined by the principal axes. They are obtained by projecting the mean-centered data vector \\(\bar{\mathbf{x}}_i\\) onto the chosen principal axes (\\(\bar{\mathbf{v}}_1, \bar{\mathbf{v}}_2, \dots, \bar{\mathbf{v}}_p\\)).</p>
<p>Let \\(\mathbf{V}_{\text{top}}^T\\) be a \\(p \times m\\) matrix formed by the first \\(p\\) rows of \\(\mathbf{V}^T\\):</p>
\\[ \mathbf{V}_{\text{top}}^T = \begin{pmatrix} \bar{\mathbf{v}}_1^T \\ \bar{\mathbf{v}}_2^T \\ \vdots \\ \bar{\mathbf{v}}_p^T \end{pmatrix} \\]
<p>The principal component vector \\(\check{\mathbf{x}}_i\\) for the data point \\(\bar{\mathbf{x}}_i\\) is a \\(p \times 1\\) vector given by:</p>
\\[ \check{\mathbf{x}}_i = \mathbf{V}_{\text{top}}^T \bar{\mathbf{x}}_i \\]
<p>This vector \\(\check{\mathbf{x}}_i\\) is the lower-dimensional representation of the original \\(m \times 1\\) vector \\(\bar{\mathbf{x}}_i\\), effectively compressing the data by retaining only its most significant features.</p>

<br>
<b><h3>2. Low-Rank Approximation using SVD</h3></b>

<p>Another powerful application of SVD is finding the best low-rank approximation of a matrix. This is a form of matrix compression, where a large, complex matrix is approximated by a simpler matrix with a lower rank.</p>

<b><h4>Problem Formulation</h4></b>
<p>Given an \\(m \times n\\) matrix \\(\mathbf{H}\\), the goal is to find another \\(m \times n\\) matrix \\(\hat{\mathbf{H}}\\) that satisfies two conditions:</p>
<ol>
    <li><b>Rank Constraint:</b> The rank of \\(\hat{\mathbf{H}}\\) is fixed to a value \\(p\\), where \\(p\\) is less than the rank of \\(\mathbf{H}\\).</li>
    <li><b>Minimization Condition:</b> The matrix \\(\hat{\mathbf{H}}\\) is the "closest" possible rank-\\(\textit{p}\\) matrix to \\(\mathbf{H}\\). Closeness is measured by the Frobenius norm.</li>
</ol>
<p>The optimization problem is:</p>
\\[ \text{Minimize} \quad \|\mathbf{H} - \hat{\mathbf{H}}\|_F^2 \quad \text{subject to} \quad \text{rank}(\hat{\mathbf{H}}) = p \\]

<b><h4>The Frobenius Norm</h4></b>
<p>The Frobenius norm of a matrix \\(\mathbf{A}\\) is the matrix equivalent of the Euclidean vector norm. It is the square root of the sum of the squares of all its elements.</p>
\\[ \|\mathbf{A}\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2} = \sqrt{\text{trace}(\mathbf{A}^T \mathbf{A})} \\]
<p>Therefore, \\(\|\mathbf{A}\|_F^2\\) is simply the sum of the squared magnitudes of all elements in \\(\mathbf{A}\\).</p>
\\[ \|\mathbf{A}\|_F^2 = \sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2 \\]

<b><h4>The SVD-Based Solution</h4></b>
<p>The Eckart-Young-Mirsky theorem states that the best rank-\\(\textit{p}\\) approximation of a matrix \\(\mathbf{H}\\) can be found by truncating its SVD. The procedure is as follows:</p>
<ol>
    <li>
        <b>Compute the SVD of \\(\mathbf{H}\\):</b>
        \\[ \mathbf{H} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T \\]
        The singular values on the diagonal of \\(\boldsymbol{\Sigma}\\) are sorted in descending order: \\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge \dots \ge 0\\).
    </li>
    <li>
        <b>Truncate the SVD matrices:</b> To create a rank-\\(\textit{p}\\) approximation, we keep only the components corresponding to the \\(p\\) largest singular values.
        <ul>
            <li>\\(\tilde{\mathbf{U}}\\): An \\(m \times p\\) matrix containing the first \\(p\\) columns of \\(\mathbf{U}\\) (the left singular vectors for \\(\sigma_1, \dots, \sigma_p\\)).</li>
            <li>\\(\tilde{\boldsymbol{\Sigma}}\\): A \\(p \times p\\) diagonal matrix containing the top \\(p\\) singular values \\(\sigma_1, \dots, \sigma_p\\).</li>
            <li>\\(\tilde{\mathbf{V}}^T\\): A \\(p \times n\\) matrix containing the first \\(p\\) rows of \\(\mathbf{V}^T\\) (the transposes of the right singular vectors for \\(\sigma_1, \dots, \sigma_p\\)).</li>
        </ul>
    </li>
    <li>
        <b>Construct the Approximation \\(\hat{\mathbf{H}}\\):</b> The best rank-\\(\textit{p}\\) approximation is formed by multiplying these truncated matrices.
        \\[ \hat{\mathbf{H}} = \tilde{\mathbf{U}} \tilde{\boldsymbol{\Sigma}} \tilde{\mathbf{V}}^T \\]
    </li>
</ol>
<p>This matrix \\(\hat{\mathbf{H}}\\) is the optimal solution to the minimization problem. It provides a compact representation of the original matrix \\(\mathbf{H}\\), which is highly useful for data compression, storage, and reducing computational complexity in subsequent processing.</p>
</div></div><div class="chapter" id="Lecture 33 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 33 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the system model and foundational concepts for the MUSIC (Multiple Signal Classification) algorithm, as presented in the transcript. The MUSIC algorithm is a powerful technique in signal processing used for Direction of Arrival (DOA) estimation.</p>

<h3>1. Introduction to the MUSIC Algorithm</h3>
<p>The name <b>MUSIC</b> is an acronym for <b>Mu</b>ltiple <b>Si</b>gnal <b>C</b>lassification. It is an algorithm designed to solve the problem of <b>Direction of Arrival (DOA)</b> estimation.</p>
<p>
<b>DOA Estimation:</b> The primary goal is to determine the direction from which one or more signals are arriving at a sensor array. This has significant practical applications, including:
<ul>
  <li>Radar and sonar systems for tracking objects like aircraft, ships, or submarines.</li>
  <li>Wireless communications for locating mobile users.</li>
  <li>Astronomy for identifying the location of celestial radio sources.</li>
</ul>
The process of estimating the location or direction of an object is often referred to as "ranging."
</p>

<h3>2. The System Model: Uniform Linear Array (ULA)</h3>
<p>To estimate the direction of a signal, a single antenna is insufficient. The MUSIC algorithm relies on an array of multiple antennas. The specific configuration discussed is a <b>Uniform Linear Array (ULA)</b>.</p>
<p>
A ULA has two key properties:
<ol>
  <li><b>Linear:</b> The antennas are arranged in a straight line.</li>
  <li><b>Uniform:</b> The spacing between any two adjacent antennas is constant.</li>
</ol>
</p>
<p>The key parameters of this model are:</p>
<ul>
    <li>\\(L\\): The total number of antennas in the array.</li>
    <li>\\(d\\): The uniform separation distance between adjacent antennas.</li>
    <li>\\(\theta\\): The direction of arrival of the incoming signal, measured as an angle relative to the array's axis.</li>
</ul>

<center><img src="https://i.imgur.com/8Qj8j0Y.png" alt="Diagram of a Uniform Linear Array receiving a signal." width="500"></center>
<i><p style="text-align:center;">An incoming plane wave signal arrives at an angle \\(\theta\\) to a Uniform Linear Array with inter-element spacing \\(d\\).</p></i>

<h3>3. Signal Model for a Single Source</h3>
<p>Let's first develop the mathematical model for a single signal arriving from a direction \\(\theta\\).</p>

<h4>A. Signal at the First Antenna</h4>
<p>The signal received at the first antenna (antenna 1) is represented as a complex baseband signal modulated onto a carrier wave, plus some additive noise. The output signal at the first antenna, \\(y_1(t)\\), is given by:</p>
\\[ y_1(t) = x(t) e^{j2\pi f_c t} + n_1(t) \\]
Where:
<ul>
  <li>\\(x(t)\\) is the complex baseband signal being transmitted.</li>
  <li>\\(f_c\\) is the carrier frequency.</li>
  <li>\\(e^{j2\pi f_c t}\\) is the complex representation of the carrier wave.</li>
  <li>\\(n_1(t)\\) is the random noise measured at the first antenna.</li>
</ul>

<h4>B. Signal at Subsequent Antennas and the Concept of Delay</h4>
<p>The core insight for DOA estimation is that the signal arrives at different antennas at slightly different times. The signal arriving at antenna 2 is a <b>delayed version</b> of the signal arriving at antenna 1.</p>
<p>
Using simple trigonometry from the diagram above, the extra distance the signal must travel to reach antenna 2 compared to antenna 1 is \\(d \cos\theta\\). The time delay (\\(\tau\\)) is this extra distance divided by the speed of light (\\(c\\)):
</p>
\\[ \text{Delay } (\tau) = \frac{\text{distance}}{\text{velocity}} = \frac{d \cos\theta}{c} \\]

<p>Therefore, the signal at the second antenna, \\(y_2(t)\\), is the original signal delayed by \\(\tau\\):</p>
\\[ y_2(t) = x(t-\tau) e^{j2\pi f_c (t-\tau)} + n_2(t) \\]
Assuming the baseband signal \\(x(t)\\) changes slowly compared to the carrier (a common narrow-band assumption), we can approximate \\(x(t-\tau) \approx x(t)\\). The delay primarily manifests as a phase shift in the rapidly oscillating carrier term.
\\[ y_2(t) \approx x(t) e^{j2\pi f_c (t - \frac{d \cos\theta}{c})} + n_2(t) \\]
<p>We can separate the exponential term:</p>
\\[ y_2(t) = x(t) e^{j2\pi f_c t} \cdot e^{-j2\pi f_c \frac{d \cos\theta}{c}} + n_2(t) \\]
The phase shift term can be simplified by introducing the wavelength \\(\lambda\\), which is related to frequency \\(f_c\\) and speed \\(c\\) by \\(\lambda = c/f_c\\).
\\[ e^{-j2\pi f_c \frac{d \cos\theta}{c}} = e^{-j \frac{2\pi d}{\lambda} \cos\theta} \\]
For convenience, a constant \\(k\\) is defined as \\(k = \frac{2\pi d}{\lambda}\\). The phase shift becomes \\(e^{-j k \cos\theta}\\). Thus, the signal at the second antenna is:
\\[ y_2(t) = \left( x(t) e^{j2\pi f_c t} \right) e^{-j k \cos\theta} + n_2(t) \\]

<h4>C. Generalizing for the Entire Array</h4>
<p>This logic extends to all \\(L\\) antennas. The signal at the \\(l\\)-th antenna experiences a delay corresponding to an extra path length of \\((l-1)d \cos\theta\\). This results in a phase shift of \\(e^{-j (l-1)k \cos\theta}\\).
<br>For example, at antenna 3, the delay is \\(\frac{2d \cos\theta}{c}\\), and the phase shift is \\(e^{-j 2k \cos\theta}\\).
\\[ y_3(t) = \left( x(t) e^{j2\pi f_c t} \right) e^{-j 2k \cos\theta} + n_3(t) \\]
</p>

<h3>4. Vector Representation and the Array Steering Vector</h3>
<p>We can stack the signals from all \\(L\\) antennas into a single column vector \\(\mathbf{y}(t)\\). This provides a compact representation of the entire system.</p>
\\[ \mathbf{y}(t) = \begin{bmatrix} y_1(t) \\ y_2(t) \\ y_3(t) \\ \vdots \\ y_L(t) \end{bmatrix} = x(t) e^{j2\pi f_c t} \begin{bmatrix} 1 \\ e^{-j k \cos\theta} \\ e^{-j 2k \cos\theta} \\ \vdots \\ e^{-j (L-1) k \cos\theta} \end{bmatrix} + \begin{bmatrix} n_1(t) \\ n_2(t) \\ \vdots \\ n_L(t) \end{bmatrix} \\]
<p>The vector containing the phase shifts is critically important. It depends only on the array geometry (\\(d\\), \\(L\\)) and the direction of arrival (\\(\theta\\)). This vector is known as the <b>Array Steering Vector</b> or <b>Array Response Vector</b>, denoted by \\(\mathbf{a}(\theta)\\).</p>
\\[ \mathbf{a}(\theta) = \begin{bmatrix} 1 \\ e^{-j k \cos\theta} \\ e^{-j 2k \cos\theta} \\ \vdots \\ e^{-j (L-1) k \cos\theta} \end{bmatrix} \\]
<p>
The steering vector \\(\mathbf{a}(\theta)\\) is a unique "spatial signature" for a signal arriving from direction \\(\theta\\). The ability to distinguish between different directions comes from the fact that \\(\mathbf{a}(\theta_1) \neq \mathbf{a}(\theta_2)\\) if \\(\theta_1 \neq \theta_2\\). The entire field of array signal processing is built upon this concept.
</p>
<p>The single-signal model can now be written concisely:</p>
\\[ \mathbf{y}(t) = \mathbf{a}(\theta) \left( x(t) e^{j2\pi f_c t} \right) + \mathbf{n}(t) \\]

<h3>5. The Sampled Digital Model</h3>
<p>In practice, signals are processed digitally. The continuous-time signal \\(\mathbf{y}(t)\\) is sampled at a certain rate, resulting in a discrete-time sequence \\(\mathbf{y}(m)\\), where \\(m\\) is the sample index. The model becomes:</p>
\\[ \mathbf{y}(m) = \mathbf{a}(\theta) x(m) + \mathbf{n}(m) \\]
Here, \\(x(m)\\) now represents the sampled baseband signal value at time index \\(m\\), and \\(\mathbf{n}(m)\\) is the sampled noise vector.</p>

<h3>6. Extension to Multiple Signals (The "Multiple Signal" Model)</h3>
<p>The model can be extended to the more realistic case where \\(P\\) signals from \\(P\\) different targets arrive simultaneously from different directions \\(\theta_1, \theta_2, \dots, \theta_P\\). The received signal at the array is the linear superposition (sum) of the contributions from all \\(P\\) signals.</p>
<p>The received signal vector \\(\mathbf{y}(m)\\) is:</p>
\\[ \mathbf{y}(m) = \mathbf{a}(\theta_1)x_1(m) + \mathbf{a}(\theta_2)x_2(m) + \cdots + \mathbf{a}(\theta_P)x_P(m) + \mathbf{n}(m) \\]
<p>This can be expressed elegantly in matrix form. We define a <b>steering matrix</b> \\(\mathbf{A}(\mathbf{\theta})\\) whose columns are the steering vectors for each of the \\(P\\) directions:</p>
\\[ \mathbf{A}(\mathbf{\theta}) = \begin{bmatrix} \mathbf{a}(\theta_1) & \mathbf{a}(\theta_2) & \cdots & \mathbf{a}(\theta_P) \end{bmatrix} \\]
This is an \\(L \times P\\) matrix. We also define a vector of the \\(P\\) source signals \\(\mathbf{x}(m)\\):
\\[ \mathbf{x}(m) = \begin{bmatrix} x_1(m) \\ x_2(m) \\ \vdots \\ x_P(m) \end{bmatrix} \\]
This is a \\(P \times 1\\) vector.

<p>With these definitions, the final <b>multiple signal model</b> is:</p>
\\[ \mathbf{y}(m) = \mathbf{A}(\mathbf{\theta}) \mathbf{x}(m) + \mathbf{n}(m) \\]
This equation is the foundation of the MUSIC algorithm. The problem is to take measurements of \\(\mathbf{y}(m)\\) over time and use them to estimate the unknown directions of arrival \\(\theta_1, \theta_2, \dots, \theta_P\\) that are embedded in the steering matrix \\(\mathbf{A}(\mathbf{\theta})\\). This estimation or identification of the DOAs is the "classification" part of the MUSIC algorithm.</div></div><div class="chapter" id="Lecture 34 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 34 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts, theories, and formulas related to the <b>MUSIC (Multiple Signal Classification)</b> algorithm, as presented in the transcript. MUSIC is a high-resolution, subspace-based algorithm widely used in array signal processing for estimating the Direction of Arrival (DOA) of multiple signals.</p>

<b>1. The Signal Model</b>
<p>The foundation of the MUSIC algorithm is the mathematical model for the signals received by an antenna array. Consider an array with \\(L\\) antenna elements receiving signals from \\(P\\) distinct targets or sources.</p>
<p>The signal received across all \\(L\\) antennas at a specific time instance (or snapshot) \\(m\\) can be represented by a vector \\(\bar{y}(m)\\). This vector is a composite of the signals from all \\(P\\) sources, combined with additive noise. The model is expressed as:</p>
\\[ \bar{y}(m) = A(\bar{\theta}) \bar{x}(m) + \bar{n}(m) \\]
<p>Let's break down the components of this equation:</p>
<ul>
    <li>\\(\bar{y}(m)\\) is the \\(L \times 1\\) vector of received signals at the \\(L\\) antennas.</li>
    <li>\\(\bar{x}(m)\\) is the \\(P \times 1\\) vector containing the signals transmitted by the \\(P\\) sources at that instant.</li>
    <li>\\(\bar{n}(m)\\) is the \\(L \times 1\\) vector of additive noise at each antenna element.</li>
    <li>\\(A(\bar{\theta})\\) is the \\(L \times P\\) <b>array response matrix</b> (also called the steering matrix). This matrix is central to DOA estimation as it contains the geometric information of the array and the arrival angles. It is composed of \\(P\\) column vectors:
    \\[ A(\bar{\theta}) = [\bar{a}(\theta_1), \bar{a}(\theta_2), \dots, \bar{a}(\theta_P)] \\]
    Each column \\(\bar{a}(\theta_i)\\) is the <b>array response vector</b> (or steering vector) corresponding to the direction of arrival \\(\theta_i\\) of the \\(i\\)-th target. This vector characterizes the phase shifts of the signal across the array elements for a given arrival angle.</li>
</ul>

<b>2. The Output Covariance Matrix</b>
<p>The next step is to analyze the statistical properties of the received signal. This is done by calculating the <b>output covariance matrix</b>, \\(R_y\\), which is the expected value of the outer product of the received signal vector with its Hermitian transpose (conjugate transpose).</p>
\\[ R_y = E[\bar{y}(m) \bar{y}^H(m)] \\]
<p>Substituting the signal model into this definition, and assuming that the source signals \\(\bar{x}(m)\\) and the noise \\(\bar{n}(m)\\) are uncorrelated, we get:</p>
\\[ R_y = A(\bar{\theta}) E[\bar{x}(m)\bar{x}^H(m)] A^H(\bar{\theta}) + E[\bar{n}(m)\bar{n}^H(m)] \\]
<p>This simplifies to:</p>
\\[ R_y = A(\bar{\theta}) R_s A^H(\bar{\theta}) + \sigma^2 I \\]
<p>Here:</p>
<ul>
    <li>\\(R_s = E[\bar{x}(m)\bar{x}^H(m)]\\) is the \\(P \times P\\) source covariance matrix.</li>
    <li>It is assumed that the noise is spatially white, meaning it is uncorrelated between antenna elements and has the same variance \\(\sigma^2\\) at each element. Therefore, the noise covariance matrix is \\(\sigma^2 I\\), where \\(I\\) is the \\(L \times L\\) identity matrix.</li>
</ul>

<b>3. Eigen-decomposition and Subspace Partitioning</b>
<p>The core insight of the MUSIC algorithm comes from the eigenvalue decomposition (EVD) of the covariance matrix \\(R_y\\).</p>
<p>Let's analyze the first term, the signal covariance matrix \\(A(\bar{\theta}) R_s A^H(\bar{\theta})\\). This is an \\(L \times L\\) matrix. Since it is formed from the \\(L \times P\\) matrix \\(A(\bar{\theta})\\), its rank is \\(P\\) (assuming \\(P < L\\) and the source signals are not perfectly correlated). A matrix of rank \\(P\\) has exactly \\(P\\) non-zero eigenvalues.</p>
<p>When we add the noise term \\(\sigma^2 I\\) to the signal covariance matrix, the eigenvectors remain the same, but the eigenvalues are all shifted by \\(\sigma^2\\). Consequently, the EVD of the total output covariance matrix \\(R_y\\) can be written as:</p>
\\[ R_y = U (\Lambda + \sigma^2 I) U^H \\]
<p>where \\(U\\) is a unitary matrix whose columns are the eigenvectors of \\(R_y\\), and \\(\Lambda + \sigma^2 I\\) is a diagonal matrix containing the eigenvalues of \\(R_y\\). These eigenvalues have a distinct structure:</p>
<ul>
    <li><b>P largest eigenvalues:</b> \\(\lambda_1 + \sigma^2, \lambda_2 + \sigma^2, \dots, \lambda_P + \sigma^2\\). These are associated with the signals.</li>
    <li><b>L-P smallest eigenvalues:</b> The remaining \\(L-P\\) eigenvalues are all equal to the noise variance, \\(\sigma^2\\).</li>
</ul>
<p>This structure allows us to partition the vector space spanned by the eigenvectors (columns of \\(U\\)) into two orthogonal subspaces:</p>
<ol>
    <li><b>The Signal Subspace:</b> This space is spanned by the \\(P\\) eigenvectors \\(\{\bar{u}_1, \dots, \bar{u}_P\}\\) corresponding to the \\(P\\) largest eigenvalues. This subspace is identical to the space spanned by the array steering vectors of the actual targets, \\(\{\bar{a}(\theta_1), \dots, \bar{a}(\theta_P)\}\\).</li>
    <li><b>The Noise Subspace:</b> This space is spanned by the \\(L-P\\) eigenvectors \\(\{\bar{u}_{P+1}, \dots, \bar{u}_L\}\\) corresponding to the \\(L-P\\) smallest eigenvalues (all equal to \\(\sigma^2\\)).</li>
</ol>

<b>4. The Orthogonality Principle</b>
<p>The key principle of MUSIC is that the <b>signal subspace</b> and the <b>noise subspace</b> are orthogonal to each other. This means that any vector lying in the signal subspace is orthogonal to any vector lying in the noise subspace.</p>
<p>Since the true array steering vectors \\(\bar{a}(\theta_1), \dots, \bar{a}(\theta_P)\\) lie in the signal subspace, they must be orthogonal to every eigenvector in the noise subspace. Mathematically, this is expressed as:</p>
\\[ \bar{a}^H(\theta_i) \bar{u}_j = 0 \quad \text{for } i \in \{1, \dots, P\} \text{ and } j \in \{P+1, \dots, L\} \\]
<p>This property is a direct consequence of the fact that the noise subspace eigenvectors \\(\{\bar{u}_j\}_{j=P+1}^L\\) belong to the null space of the signal-only covariance matrix \\(A(\bar{\theta}) R_s A^H(\bar{\theta})\\).</p>

<b>5. The MUSIC Pseudospectrum</b>
<p>The orthogonality principle is used to find the unknown DOAs. We can search for angles \\(\theta\\) for which the corresponding steering vector \\(\bar{a}(\theta)\\) is orthogonal to the noise subspace. This is done by defining a function, often called the MUSIC spectrum or pseudospectrum.</p>
<p>The squared norm of the projection of a test steering vector \\(\bar{a}(\theta)\\) onto the noise subspace is given by \\(\sum_{j=P+1}^{L} |\bar{a}^H(\theta) \bar{u}_j|^2\\). According to the orthogonality principle, this sum will be zero (or close to zero in practice) if and only if \\(\theta\\) is one of the true DOAs.</p>
<p>To create a function that has sharp peaks at the DOAs instead of nulls, we take the reciprocal. This gives the MUSIC spectrum formula:</p>
\\[ F(\theta) = \frac{1}{\sum_{j=P+1}^{L} |\bar{a}^H(\theta) \bar{u}_j|^2} \\]
<p>By plotting \\(F(\theta)\\) as a function of the angle \\(\theta\\) over the range of interest, we obtain a spectrum. The locations of the \\(P\\) highest peaks in this spectrum correspond to the estimated DOAs of the targets.</p>
<p>In summary, the MUSIC algorithm elegantly transforms the DOA estimation problem into a search for peaks in a spectrum, which is constructed by leveraging the fundamental orthogonality between the signal and noise subspaces derived from the received signal's covariance matrix.</p>
</div></div><div class="chapter" id="Lecture 35 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 35 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts, theories, and formulas related to <b>Linear Minimum Mean Squared Error (LMMSE) Estimation</b> as presented in the transcript.</p>

<h3>1. Introduction to LMMSE Estimation</h3>
<p>LMMSE stands for <b>Linear Minimum Mean Squared Error</b> estimation. It is a fundamental technique in signal processing and statistics for estimating an unknown random vector based on observations of a related random vector.</p>

<p>The core problem is as follows:</p>
<ul>
    <li>We have an unobserved random vector, let's call it \\(\bar{x}\\), which can be thought of as an input to a system or an underlying parameter we want to know.</li>
    <li>We have an observed random vector, \\(\bar{y}\\), which is the output of the system or some available measurement.</li>
    <li>The vectors \\(\bar{x}\\) and \\(\bar{y}\\) are statistically related, meaning they are <b>correlated</b>. This correlation is key, as it allows us to infer information about \\(\bar{x}\\) from \\(\bar{y}\\).</li>
</ul>
<p>The goal of LMMSE is to find the <i>best linear estimate</i> of \\(\bar{x}\\) using \\(\bar{y}\\). The two defining characteristics of LMMSE are:</p>
<ol>
    <li><b>Linear Estimator:</b> The estimate of \\(\bar{x}\\), denoted \\(\hat{x}\\), is restricted to be a linear transformation of the observation \\(\bar{y}\\). This makes the problem mathematically tractable and computationally efficient. It's important to note that the underlying system relating \\(\bar{x}\\) and \\(\bar{y}\\) does not need to be linear; the constraint is on the structure of the <i>estimator</i> itself.</li>
    <li><b>Minimum Mean Squared Error (MMSE) Criterion:</b> "Best" is defined in the sense of minimizing the average (or "mean") of the squared error between the true vector \\(\bar{x}\\) and its estimate \\(\hat{x}\\).</li>
</ol>

<h3>2. Statistical Preliminaries and Notation</h3>
<p>To formulate the problem mathematically, we define the statistical properties of the random vectors \\(\bar{x}\\) and \\(\bar{y}\\). For simplicity, the analysis begins with the assumption that both vectors are <b>zero-mean</b>:</p>
\\[ E[\bar{x}] = \mathbf{0} \\]
\\[ E[\bar{y}] = \mathbf{0} \\]
<p>The relationships between the vectors are captured by their second-order statistics, specifically their covariance matrices.</p>

<ul>
    <li><b>Autocovariance of \\(\bar{x}\\):</b> This matrix describes the internal structure and variance of the vector \\(\bar{x}\\).
    \\[ \mathbf{R}_{xx} = E[\bar{x}\bar{x}^T] \\]
    </li>
    <li><b>Autocovariance of \\(\bar{y}\\):</b> This matrix describes the internal structure and variance of the vector \\(\bar{y}\\).
    \\[ \mathbf{R}_{yy} = E[\bar{y}\bar{y}^T] \\]
    </li>
    <li><b>Cross-covariance Matrices:</b> These are the most crucial quantities for estimation, as they capture the statistical correlation between \\(\bar{x}\\) and \\(\bar{y}\\). If these matrices are zero, the vectors are uncorrelated, and it's impossible to linearly estimate one from the other (beyond using the mean).
    \\[ \mathbf{R}_{xy} = E[\bar{x}\bar{y}^T] \\]
    \\[ \mathbf{R}_{yx} = E[\bar{y}\bar{x}^T] \\]
    These two matrices are transposes of each other:
    \\[ \mathbf{R}_{xy} = \mathbf{R}_{yx}^T \\]
    </li>
</ul>

<h3>3. The Linear Estimator</h3>
<p>As per the "Linear" constraint in LMMSE, the estimate \\(\hat{x}\\) is formed by applying a linear transformation (a matrix multiplication) to the observation \\(\bar{y}\\). This is expressed as:</p>
\\[ \hat{x} = \mathbf{C}\bar{y} \\]
<p>Here, \\(\mathbf{C}\\) is the matrix that defines the estimator. The entire goal of the LMMSE derivation is to find the optimal matrix \\(\mathbf{C}\\) that minimizes the estimation error.</p>
<p>If \\(\bar{x}\\) is an \\(n \times 1\\) vector and \\(\bar{y}\\) is an \\(m \times 1\\) vector, then the matrix \\(\mathbf{C}\\) must have dimensions \\(n \times m\\) for the matrix multiplication to be valid and produce an estimate \\(\hat{x}\\) of the correct size (\\(n \times 1\\)).</p>

<h3>4. The LMMSE Principle and Objective Function</h3>
<p>The term "Linear Minimum Mean Squared Error" can be broken down to understand the objective:</p>
<ul>
    <li><b>Error:</b> The difference between the true vector and the estimate: \\( \bar{x} - \hat{x} \\).</li>
    <li><b>Squared Error:</b> The magnitude of the error, measured by the squared Euclidean norm: \\( ||\bar{x} - \hat{x}||^2 \\).</li>
    <li><b>Mean Squared Error (MSE):</b> The expected value (or mean) of the squared error. This is the cost function we want to minimize.
    \\[ \text{MSE} = E[||\bar{x} - \hat{x}||^2] \\]
    </li>
    <li><b>Minimum:</b> We seek to find the estimator that makes this MSE as small as possible.</li>
    <li><b>Linear Estimator:</b> The search for the optimal estimator is constrained to the form \\(\hat{x} = \mathbf{C}\bar{y}\\).</li>
</ul>
<p>Combining these, the LMMSE problem is to find the matrix \\(\mathbf{C}\\) that minimizes the MSE:</p>
\\[ \min_{\mathbf{C}} E[||\bar{x} - \mathbf{C}\bar{y}||^2] \\]

<h3>5. Derivation of the Mean Squared Error (MSE) Expression</h3>
<p>The derivation involves expanding the MSE expression to make it a function of the matrix \\(\mathbf{C}\\) and the known covariance matrices. A key technique used is the <b>trace property</b>.</p>

<p><b>Trace Property:</b> For any column vector \\(\mathbf{v}\\), its squared norm can be expressed as the trace of its outer product:</p>
\\[ ||\mathbf{v}||^2 = \mathbf{v}^T\mathbf{v} = \text{trace}(\mathbf{v}\mathbf{v}^T) \\]
<p>The trace of a square matrix is the sum of its diagonal elements. This property allows us to manipulate the matrix expressions more easily.</p>

<p><b>Step-by-step derivation:</b></p>
<p>1. Start with the MSE objective function, using the error vector \\( \hat{x} - \bar{x} \\) (the result is the same as using \\(\bar{x} - \hat{x}\\) due to the square).</p>
\\[ \text{MSE} = E[||\hat{x} - \bar{x}||^2] = E[||\mathbf{C}\bar{y} - \bar{x}||^2] \\]
<p>2. Apply the trace property:</p>
\\[ \text{MSE} = E\left[ \text{trace}\left( (\mathbf{C}\bar{y} - \bar{x})(\mathbf{C}\bar{y} - \bar{x})^T \right) \right] \\]
<p>3. Expand the term inside the trace:</p>
\\[ (\mathbf{C}\bar{y} - \bar{x})(\bar{y}^T\mathbf{C}^T - \bar{x}^T) = \mathbf{C}\bar{y}\bar{y}^T\mathbf{C}^T - \mathbf{C}\bar{y}\bar{x}^T - \bar{x}\bar{y}^T\mathbf{C}^T + \bar{x}\bar{x}^T \\]
<p>4. The trace and expectation operators are both linear, so their order can be swapped. This is a crucial step that allows us to introduce the covariance matrices.</p>
\\[ \text{MSE} = \text{trace}\left( E\left[ \mathbf{C}\bar{y}\bar{y}^T\mathbf{C}^T - \mathbf{C}\bar{y}\bar{x}^T - \bar{x}\bar{y}^T\mathbf{C}^T + \bar{x}\bar{x}^T \right] \right) \\]
<p>5. Apply the expectation to each term individually. Since \\(\mathbf{C}\\) is a constant (non-random) matrix, it can be pulled out of the expectation.</p>
\\[ \text{MSE} = \text{trace}\left( \mathbf{C}E[\bar{y}\bar{y}^T]\mathbf{C}^T - \mathbf{C}E[\bar{y}\bar{x}^T] - E[\bar{x}\bar{y}^T]\mathbf{C}^T + E[\bar{x}\bar{x}^T] \right) \\]
<p>6. Substitute the definitions of the covariance matrices (\\(\mathbf{R}_{xx}, \mathbf{R}_{yy}, \mathbf{R}_{xy}, \mathbf{R}_{yx}\\)):</p>
\\[ \text{MSE}(\mathbf{C}) = \text{trace}(\mathbf{R}_{xx} - \mathbf{C}\mathbf{R}_{yx} - \mathbf{R}_{xy}\mathbf{C}^T + \mathbf{C}\mathbf{R}_{yy}\mathbf{C}^T) \\]

<p>This final expression represents the Mean Squared Error as a function of the estimator matrix \\(\mathbf{C}\\). The next step, which the transcript defers to a future module, is to find the specific matrix \\(\mathbf{C}\\) that minimizes this trace expression. The transcript hints at the technique for this minimization, which involves adding and subtracting a specific term (\\(\mathbf{R}_{xy}\mathbf{R}_{yy}^{-1}\mathbf{R}_{yx}\\)) to algebraically complete the square.</p>
</div></div><div class="chapter" id="Lecture 36 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 36 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the derivation and key concepts of the <b>Linear Minimum Mean Squared Error (LMMSE) estimator</b> as presented in the transcript. The goal of the LMMSE estimator is to find the best linear estimate of a random vector \\(\mathbf{x}\\) given an observation of a correlated random vector \\(\mathbf{y}\\). The "best" estimate is the one that minimizes the mean squared error (MSE).</p>

<b>1. The Mean Squared Error (MSE) Formulation</b>
<p>We want to estimate a random vector \\(\mathbf{x}\\) using a linear function of a related random vector \\(\mathbf{y}\\). For the initial derivation, we assume both vectors are zero-mean (i.e., \\(E[\mathbf{x}] = \mathbf{0}\\) and \\(E[\mathbf{y}] = \mathbf{0}\\)). The linear estimate, denoted as \\(\hat{\mathbf{x}}\\), takes the form:</p>
\\[ \hat{\mathbf{x}} = C\mathbf{y} \\]
<p>where \\(C\\) is a matrix of coefficients that we need to determine. The objective is to find the matrix \\(C\\) that minimizes the mean squared error, which is the expected value of the squared norm of the error vector \\(\mathbf{x} - \hat{\mathbf{x}}\\). This can be expressed using the trace operator:</p>
\\[ \text{MSE} = E\left[\|\mathbf{x} - \hat{\mathbf{x}}\|^2\right] = E\left[\|\mathbf{x} - C\mathbf{y}\|^2\right] \\]
<p>Expanding this expression leads to a formula involving the correlation matrices of the vectors:</p>
\\[ \text{MSE} = \text{Trace}\left(R_{xx} - C R_{yx} - R_{xy} C^T + C R_{yy} C^T\right) \\]
<p>Where:</p>
<ul>
    <li>\\(R_{xx} = E[\mathbf{x}\mathbf{x}^T]\\) is the auto-correlation matrix of \\(\mathbf{x}\\).</li>
    <li>\\(R_{yy} = E[\mathbf{y}\mathbf{y}^T]\\) is the auto-correlation matrix of \\(\mathbf{y}\\).</li>
    <li>\\(R_{xy} = E[\mathbf{x}\mathbf{y}^T]\\) is the cross-correlation matrix between \\(\mathbf{x}\\) and \\(\mathbf{y}\\).</li>
    <li>\\(R_{yx} = E[\mathbf{y}\mathbf{x}^T] = R_{xy}^T\\) is the cross-correlation matrix between \\(\mathbf{y}\\) and \\(\mathbf{x}\\).</li>
</ul>

<b>2. Minimization by Completing the Square</b>
<p>To find the optimal matrix \\(C\\) that minimizes the MSE, the transcript uses a matrix version of the "completing the square" technique. This is done by adding and subtracting the term \\(R_{xy} R_{yy}^{-1} R_{yx}\\) inside the trace.</p>
<p>The MSE expression is manipulated as follows:</p>
\\[ \text{MSE} = \text{Trace}\left(R_{xx} - C R_{yx} - R_{xy} C^T + C R_{yy} C^T + R_{xy}R_{yy}^{-1}R_{yx} - R_{xy}R_{yy}^{-1}R_{yx}\right) \\]
<p>By rearranging terms, this can be factored into a more structured form:</p>
\\[ \text{MSE} = \text{Trace}\left( (C R_{yy} - R_{xy}) R_{yy}^{-1} (C R_{yy} - R_{xy})^T \right) + \text{Trace}\left( R_{xx} - R_{xy} R_{yy}^{-1} R_{yx} \right) \\]
<p>This new expression consists of two main parts. Our goal is to find the matrix \\(C\\) that minimizes this entire quantity.</p>

<b>3. Finding the Optimal Estimator</b>
<p>We analyze the two components of the rewritten MSE expression:</p>
<ol>
    <li><b>The Second Term:</b> \\(\text{Trace}\left( R_{xx} - R_{xy} R_{yy}^{-1} R_{yx} \right)\\)
    <br>This term consists only of correlation matrices and does not depend on our choice of \\(C\\). Therefore, it is a constant with respect to the minimization problem.</li>
    <li><b>The First Term:</b> \\(\text{Trace}\left( (C R_{yy} - R_{xy}) R_{yy}^{-1} (C R_{yy} - R_{xy})^T \right)\\)
    <br>This term is the only part that depends on \\(C\\). To minimize the total MSE, we must minimize this term.</li>
</ol>
<p>The correlation matrix \\(R_{yy}\\) is positive semi-definite (PSD). Assuming it is invertible, its inverse \\(R_{yy}^{-1}\\) is also PSD. The structure of the first term, which is of the form \\(\text{Trace}(A M A^T)\\) where \\(M\\) is a PSD matrix, ensures that the matrix inside the trace is also PSD. The trace of a PSD matrix is always non-negative (greater than or equal to zero).</p>
<p>Therefore, the minimum possible value for this term is <b>zero</b>. This minimum is achieved when the matrix expression inside the trace is the zero matrix:</p>
\\[ C R_{yy} - R_{xy} = \mathbf{0} \\]
<p>Solving for the optimal matrix \\(C_{\text{opt}}\\):</p>
\\[ C_{\text{opt}} R_{yy} = R_{xy} \implies C_{\text{opt}} = R_{xy} R_{yy}^{-1} \\]

<b>4. The LMMSE Estimator and Minimum Error (Zero-Mean Case)</b>
<p>Substituting this optimal matrix \\(C_{\text{opt}}\\) back into our linear estimator equation \\(\hat{\mathbf{x}} = C\mathbf{y}\\), we get the LMMSE estimator for zero-mean vectors:</p>
\\[ \hat{\mathbf{x}}_{\text{LMMSE}} = R_{xy} R_{yy}^{-1} \mathbf{y} \\]
<p>With this optimal estimator, the first term in our MSE expression becomes zero. The minimum MSE is therefore equal to the second, constant term:</p>
\\[ \text{MMSE} = \text{Trace}\left( R_{xx} - R_{xy} R_{yy}^{-1} R_{yx} \right) \\]

<b>5. The Error Covariance Matrix</b>
<p>The matrix inside the trace of the MMSE is known as the <b>error covariance matrix</b>. It describes the covariance of the estimation error vector \\(\mathbf{e} = \mathbf{x} - \hat{\mathbf{x}}\\):</p>
\\[ C_{ee} = E\left[(\mathbf{x} - \hat{\mathbf{x}})(\mathbf{x} - \hat{\mathbf{x}})^T\right] = R_{xx} - R_{xy} R_{yy}^{-1} R_{yx} \\]
<p>The diagonal elements of this matrix represent the mean squared error for each component of the estimated vector \\(\mathbf{x}\\), and the sum of these diagonal elements (the trace) gives the total MMSE.</p>

<b>6. Generalization to Non-Zero Mean Vectors</b>
<p>The derivation can be extended to the more general case where the vectors are not zero-mean, i.e., \\(E[\mathbf{x}] = \boldsymbol{\mu}_x\\) and \\(E[\mathbf{y}] = \boldsymbol{\mu}_y\\). The strategy is to first create zero-mean versions of the vectors:</p>
\\[ \tilde{\mathbf{x}} = \mathbf{x} - \boldsymbol{\mu}_x \\]
\\[ \tilde{\mathbf{y}} = \mathbf{y} - \boldsymbol{\mu}_y \\]
<p>We can apply the zero-mean LMMSE formula to estimate \\(\tilde{\mathbf{x}}\\) from \\(\tilde{\mathbf{y}}\ \):</p>
\\[ \hat{\tilde{\mathbf{x}}} = R_{\tilde{x}\tilde{y}} R_{\tilde{y}\tilde{y}}^{-1} \tilde{\mathbf{y}} \\]
<p>The correlation matrices of these new zero-mean vectors are, by definition, the <i>covariance</i> matrices of the original vectors:</p>
<ul>
    <li>\\(R_{\tilde{y}\tilde{y}} = E[\tilde{\mathbf{y}}\tilde{\mathbf{y}}^T] = E[(\mathbf{y} - \boldsymbol{\mu}_y)(\mathbf{y} - \boldsymbol{\mu}_y)^T] = C_{yy}\\)</li>
    <li>\\(R_{\tilde{x}\tilde{y}} = E[\tilde{\mathbf{x}}\tilde{\mathbf{y}}^T] = E[(\mathbf{x} - \boldsymbol{\mu}_x)(\mathbf{y} - \boldsymbol{\mu}_y)^T] = C_{xy}\\)</li>
</ul>
<p>Substituting these back, along with the definitions of \\(\tilde{\mathbf{x}}\\) and \\(\tilde{\mathbf{y}}\\):</p>
\\[ \hat{\mathbf{x}} - \boldsymbol{\mu}_x = C_{xy} C_{yy}^{-1} (\mathbf{y} - \boldsymbol{\mu}_y) \\]
<p>Finally, solving for \\(\hat{\mathbf{x}}\\) gives the LMMSE estimator for the general (non-zero mean) case:</p>
\\[ \hat{\mathbf{x}}_{\text{LMMSE}} = \boldsymbol{\mu}_x + C_{xy} C_{yy}^{-1} (\mathbf{y} - \boldsymbol{\mu}_y) \\]

<b>7. Key Insights and Interpretations</b>
<ul>
    <li><b>Uncorrelated Vectors:</b> If \\(\mathbf{x}\\) and \\(\mathbf{y}\\) are uncorrelated, their cross-covariance matrix \\(C_{xy}\\) is the zero matrix. In this case, the LMMSE formula simplifies to:
    \\[ \hat{\mathbf{x}} = \boldsymbol{\mu}_x + \mathbf{0} \cdot C_{yy}^{-1} (\mathbf{y} - \boldsymbol{\mu}_y) = \boldsymbol{\mu}_x \\]
    This means that if the observation \\(\mathbf{y}\\) provides no correlated information about \\(\mathbf{x}\\), the best linear estimate for \\(\mathbf{x}\\) is simply its mean. Observing \\(\mathbf{y}\\) does not improve the estimate.
    <br><br>
    </li>
    <li><b>Impact of Correlation on Error:</b> The error covariance matrix for the general case is:
    \\[ C_{ee} = C_{xx} - C_{xy} C_{yy}^{-1} C_{yx} \\]
    The term \\(C_{xy} C_{yy}^{-1} C_{yx}\\) represents the reduction in uncertainty about \\(\mathbf{x}\\) that is gained by observing \\(\mathbf{y}\\). A "stronger" cross-correlation (a larger \\(C_{xy}\\) in a matrix sense) leads to a larger reduction term, and therefore a smaller estimation error. This confirms the intuition that the better \\(\mathbf{y}\\) is correlated with \\(\mathbf{x}\\), the more accurately we can estimate \\(\mathbf{x}\\).</li>
</ul>
</div></div><h2>Weekly Summary</h2><div><h3>MIMO Wireless Communication Optimization using SVD</h3>
<p>This section explores the application of Singular Value Decomposition (SVD) to optimize Multiple-Input Multiple-Output (MIMO) wireless communication systems.</p>
<ul>
    <li><b>Decoupling MIMO Channels:</b> SVD can be used to transform a complex, coupled MIMO system into a set of simpler, independent, parallel channels. This process involves pre-coding at the transmitter and combining at the receiver, which are derived from the SVD of the channel matrix. This enables <b>spatial multiplexing</b>, where multiple data streams (symbols) are transmitted simultaneously over the same frequency.</li>
    <li><b>Sum Rate Maximization:</b> The primary optimization goal is to maximize the total data rate (sum rate) across all these parallel channels, subject to a constraint on the total transmit power. The sum rate is the sum of the individual channel capacities, given by the Shannon formula:
    \\[ R_{sum} = \sum_{i=1}^{t} \log_2 \left( 1 + \frac{\sigma_i^2 P_i}{\sigma^2} \right) \\]
    where \\(\sigma_i\\) is the \\(i\\)-th singular value (channel gain), \\(P_i\\) is the power allocated to the \\(i\\)-th channel, and \\(\sigma^2\\) is the noise power.</li>
    <li><b>Water-filling Algorithm:</b> The optimal power allocation strategy to solve this constrained optimization problem is the water-filling algorithm. This celebrated algorithm allocates more power to channels with higher gains (larger singular values) and less or zero power to weaker channels. The power for the \\(i\\)-th channel is given by:
    \\[ P_i = \left( \frac{1}{\lambda'} - \frac{\sigma^2}{\sigma_i^2} \right)^+ \\]
    where \\(\frac{1}{\lambda'}\\) acts as a "water level" determined by the total power constraint, and the \\( ( \cdot )^+ \\) operator ensures power is non-negative.</li>
</ul>

<h3>Other Applications of SVD</h3>
<p>The discussion covers two other significant applications of SVD in machine learning and data analysis.</p>
<ul>
    <li><b>Principal Component Analysis (PCA):</b> SVD provides a direct and efficient method to perform PCA. Instead of calculating the covariance matrix and then finding its eigenvectors, one can directly apply SVD to the mean-centered data matrix \\(X\\). The <b>right singular vectors</b> of \\(X\\) are the principal components (axes of maximum variance). The singular values indicate the importance of each component.</li>
    <li><b>Low-Rank Approximation:</b> SVD is used to find the best rank-\\(p\\) approximation of a matrix \\(H\\). This is crucial for data compression and creating compact representations of large matrices. The optimal rank-\\(p\\) approximation, \\(\hat{H}\\), is constructed by truncating the SVD of \\(H\\), keeping only the top \\(p\\) singular values and their corresponding left and right singular vectors:
    \\[ \hat{H} = U_p \Sigma_p V_p^T \\]
    This minimizes the Frobenius norm of the error, \\( ||H - \hat{H}||_F \\).</li>
</ul>

<h3>The MUSIC Algorithm for Direction of Arrival (DOA) Estimation</h3>
<p>This topic introduces the MUSIC (Multiple Signal Classification) algorithm, a powerful technique in signal processing used to estimate the angles from which multiple signals arrive at an antenna array.</p>
<ul>
    <li><b>System Model:</b> The algorithm relies on a Uniform Linear Array (ULA) of antennas. A signal arriving at an angle \\(\theta\\) experiences different time delays at each antenna, resulting in a predictable phase shift across the array. This spatial signature is captured by the <b>array steering vector</b>, \\(\mathbf{a}(\theta)\\), which is unique for each angle \\(\theta\\).</li>
    <li><b>Signal and Noise Subspaces:</b> The core of MUSIC is the analysis of the received signal's output covariance matrix, \\(R_y\\). By performing eigenvalue decomposition on \\(R_y\\), the vector space is separated into two orthogonal subspaces:
        <ul>
            <li>The <b>signal subspace</b>, spanned by the eigenvectors corresponding to the largest eigenvalues, which are associated with the incoming signals.</li>
            <li>The <b>noise subspace</b>, spanned by the eigenvectors corresponding to the smallest eigenvalues, which are associated with noise.</li>
        </ul>
    </li>
    <li><b>DOA Estimation:</b> A fundamental property is that the steering vectors of the true signal directions are orthogonal to the noise subspace. The MUSIC algorithm exploits this by creating a "spectrum" function that scans through all possible angles \\(\theta\\). The function is the reciprocal of the magnitude of the projection of the steering vector \\(\mathbf{a}(\theta)\\) onto the noise subspace.
    \\[ f(\theta) = \frac{1}{\sum_{j=p+1}^{L} |\mathbf{a}(\theta)^H \mathbf{u}_j|^2} \\]
    This function exhibits sharp peaks at the angles where \\(\mathbf{a}(\theta)\\) is orthogonal to the noise subspace vectors \\(\mathbf{u}_j\\), thus revealing the Directions of Arrival.</li>
</ul>

<h3>Linear Minimum Mean Squared Error (LMMSE) Estimation</h3>
<p>This final topic introduces the LMMSE estimation principle, a fundamental technique for estimating an unknown random vector based on a correlated, observed random vector.</p>
<ul>
    <li><b>Principle:</b> The goal is to find the best <b>linear estimator</b> \\(\hat{\mathbf{x}} = C\mathbf{y} + \mathbf{d}\\) for an unknown random vector \\(\mathbf{x}\\) given an observation \\(\mathbf{y}\\). The "best" estimator is one that minimizes the Mean Squared Error (MSE), defined as \\(E[||\mathbf{x} - \hat{\mathbf{x}}||^2]\\).</li>
    <li><b>The LMMSE Estimator Formula:</b> The optimal linear estimator is derived using the statistical properties (mean and covariance) of the vectors. The final formula for the LMMSE estimate is:
    \\[ \hat{\mathbf{x}} = \mu_x + R_{xy}R_{yy}^{-1}(\mathbf{y} - \mu_y) \\]
    where \\(\mu_x, \mu_y\\) are the mean vectors, \\(R_{yy}\\) is the covariance matrix of \\(\mathbf{y}\\), and \\(R_{xy}\\) is the cross-covariance matrix between \\(\mathbf{x}\\) and \\(\mathbf{y}\\).</li>
    <li><b>Key Takeaways:</b>
        <ul>
            <li>The estimator heavily relies on the <b>correlation</b> between \\(\mathbf{x}\\) and \\(\mathbf{y}\\), captured by \\(R_{xy}\\).</li>
            <li>If the vectors are uncorrelated (\\(R_{xy} = 0\\)), the observation \\(\mathbf{y}\\) provides no useful information, and the best estimate for \\(\mathbf{x}\\) is simply its mean, \\(\mu_x\\).</li>
            <li>The minimum estimation error (error covariance) decreases as the correlation between the vectors increases, showing that stronger correlation leads to a more accurate estimate.</li>
        </ul>
    </li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<p>Here is an explanation of the questions and answers from the assignment.</p>

<p><b>Question 1: Singular value decomposition (SVD) is defined for</b></p>
<p><b>Answer:</b> Any matrix</p>
<p><b>Explanation:</b> Singular Value Decomposition (SVD) is a fundamental matrix factorization technique in linear algebra. It can be applied to any rectangular matrix \\( A \in \mathbb{C}^{m \times n} \\), regardless of whether it is square, tall, wide, invertible, or singular. The decomposition takes the form \\( A = U\Sigma V^H \\), where \\( U \\) and \\( V \\) are unitary matrices, and \\( \Sigma \\) is a rectangular diagonal matrix containing the non-negative singular values of \\( A \\).</p>
<br>

<p><b>Question 2: Consider the \\( L \times L \\) output covariance matrix \\( R_y \\) for the MUSIC scheme... The MUSIC spectrum as a function of \\( \theta \\) plots</b></p>
<p><b>Answer:</b> \\( \frac{1}{|\sum_{j=P+1}^{L} \bar{a}^H(\theta) \bar{u}_j|^2} \\)</p>
<p><b>Explanation:</b> The MUSIC (Multiple Signal Classification) algorithm estimates the direction of arrival (DOA) of signals. It operates by separating the eigenspace of the data covariance matrix \\(R_y\\) into two orthogonal subspaces: the "signal subspace" and the "noise subspace".
<ul>
    <li>The signal subspace is spanned by the eigenvectors \\(\{\bar{u}_1, \dots, \bar{u}_P\}\\) corresponding to the \\(P\\) largest eigenvalues.</li>
    <li>The noise subspace is spanned by the eigenvectors \\(\{\bar{u}_{P+1}, \dots, \bar{u}_L\}\\) corresponding to the \\(L-P\\) smallest eigenvalues.</li>
</ul>
The array response vector \\(\bar{a}(\theta)\\) for a true signal's DOA is orthogonal to the noise subspace. Therefore, the projection of \\(\bar{a}(\theta)\\) onto the noise subspace will be zero (or close to zero in the presence of noise) for a true DOA. To find these DOAs, the MUSIC algorithm calculates the squared norm of this projection, which is given by \\(\sum_{j=P+1}^{L} |\bar{a}^H(\theta) \bar{u}_j|^2\\). To get sharp peaks at the true DOAs, the spectrum is defined as the reciprocal of this value.
<br><i>Note: The expression in the provided answer \\(|\sum_{j=P+1}^{L} \bar{a}^H(\theta) \bar{u}_j|^2\\) is likely a typo in the question and should be \\(\sum_{j=P+1}^{L} |\bar{a}^H(\theta) \bar{u}_j|^2\\). However, among the given options, this is the correct choice as it correctly identifies the noise subspace eigenvectors (from \\(j=P+1\\) to \\(L\\)).</i></p>
<br>

<p><b>Question 3: Consider the matrix \\( H \\) given below... Its second largest singular value \\( \sigma_2 \\) is given as</b></p>
<p><b>Answer:</b> Accepted Answer is \\(16\sqrt{2}\\), but this appears to be an error in the question.</p>
<p><b>Explanation:</b> The question's formatting is ambiguous, but the most plausible interpretation is that the matrix is \\( H = \begin{bmatrix} 1 & -2 \\ 1 & 2 \\ -1 & 2 \\ -1 & -2 \end{bmatrix} \\).
To find the singular values, we first compute \\( H^T H \\):
\\[ H^T H = \begin{bmatrix} 1 & 1 & -1 & -1 \\ -2 & 2 & 2 & -2 \end{bmatrix} \begin{bmatrix} 1 & -2 \\ 1 & 2 \\ -1 & 2 \\ -1 & -2 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 16 \end{bmatrix} \\]
The eigenvalues of \\( H^T H \\) are \\(\lambda_1 = 16\\) and \\(\lambda_2 = 4\\). The singular values are the square roots of these eigenvalues, conventionally arranged in decreasing order:
\\[ \sigma_1 = \sqrt{16} = 4 \\]
\\[ \sigma_2 = \sqrt{4} = 2 \\]
Thus, the second largest singular value is 2. This result does not match any of the options or the accepted answer of \\(16\sqrt{2}\\). The question as presented is likely flawed.</p>
<br>

<p><b>Question 4: The determinant of a unitary matrix \\(U\\) satisfies the property</b></p>
<p><b>Answer:</b> \\( |\det(U)| = 1 \\)</p>
<p><b>Explanation:</b> A matrix \\(U\\) is unitary if its conjugate transpose is also its inverse, i.e., \\(U^H U = I\\). Using the properties of determinants, we have:
\\[ \det(U^H U) = \det(I) \\]
\\[ \det(U^H)\det(U) = 1 \\]
Since \\(\det(U^H) = \overline{\det(U)}\\) (the complex conjugate of \\(\det(U)\\)), the equation becomes:
\\[ \overline{\det(U)}\det(U) = 1 \\]
\\[ |\det(U)|^2 = 1 \\]
Taking the positive square root, we get \\( |\det(U)| = 1 \\). This means the determinant of a unitary matrix is a complex number with a magnitude of 1 (i.e., it lies on the unit circle in the complex plane).</p>
<br>

<p><b>Question 5: Consider the zero-mean Gaussian random vector \\(\bar{x}\\)... The probability density function of \\(\bar{x}\\) when \\(x_1, x_2, \dots, x_n\\) are independent and identically distributed (i.i.d.) with variance \\(\sigma^2\\) is given as</b></p>
<p><b>Answer:</b> \\( (\frac{1}{2\pi\sigma^2})^{n/2} e^{-\frac{\|\bar{x}\|^2}{2\sigma^2}} \\)</p>
<p><b>Explanation:</b> For a single zero-mean Gaussian random variable \\(x_i\\) with variance \\(\sigma^2\\), the probability density function (PDF) is \\( p(x_i) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-x_i^2 / (2\sigma^2)} \\).
Because the variables \\(x_i\\) are independent, the joint PDF of the vector \\(\bar{x}\\) is the product of the individual PDFs:
\\[ p(\bar{x}) = \prod_{i=1}^{n} p(x_i) = \prod_{i=1}^{n} \left( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x_i^2}{2\sigma^2}} \right) \\]
\\[ p(\bar{x}) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(-\sum_{i=1}^{n} \frac{x_i^2}{2\sigma^2}\right) \\]
\\[ p(\bar{x}) = \left(\frac{1}{2\pi\sigma^2}\right)^{n/2} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^{n} x_i^2\right) \\]
Recognizing that \\(\sum_{i=1}^{n} x_i^2 = \|\bar{x}\|^2\\) (the squared Euclidean norm), we get the final expression.</p>
<br>

<p><b>Question 6: Consider the singular values of a MIMO channel matrix given as \\(\sigma_1 = 2, \sigma_2 = 1\\) with noise power \\(\sigma^2 = 8\\) and total power \\(P = 20\\). The optimal power allocation to maximize the capacity of the MIMO channel is given as</b></p>
<p><b>Answer:</b> 13, 7</p>
<p><b>Explanation:</b> This problem is solved using the water-filling algorithm. The optimal power \\(P_i\\) allocated to the i-th subchannel is given by \\( P_i = (\mu - \frac{\sigma^2}{\sigma_i^2})^+ \\), where \\((x)^+ = \max(0, x)\\) and \\(\mu\\) is the "water level" determined by the total power constraint \\(\sum P_i = P\\).
<ol>
    <li>Calculate the effective noise level for each subchannel:
        <br>For \\(\sigma_1 = 2\\): \\(\frac{\sigma^2}{\sigma_1^2} = \frac{8}{2^2} = 2\\)
        <br>For \\(\sigma_2 = 1\\): \\(\frac{\sigma^2}{\sigma_2^2} = \frac{8}{1^2} = 8\\)</li>
    <li>Set up the total power equation, assuming both channels receive power (i.e., \\(\mu\\) is above both noise levels):
        <br>\\(P_1 = \mu - 2\\)
        <br>\\(P_2 = \mu - 8\\)
        <br>\\(P_{total} = P_1 + P_2 = (\mu - 2) + (\mu - 8) = 2\mu - 10\\)</li>
    <li>Solve for \\(\mu\\) using the total power \\(P=20\\):
        <br>\\(20 = 2\mu - 10 \implies 2\mu = 30 \implies \mu = 15\\)</li>
    <li>Calculate the allocated powers:
        <br>\\(P_1 = 15 - 2 = 13\\)
        <br>\\(P_2 = 15 - 8 = 7\\)</li>
</ol>
The optimal power allocation is (13, 7).</p>
<br>

<p><b>Question 7: Principal Component Analysis (PCA) is used in machine learning for</b></p>
<p><b>Answer:</b> Dimensionality reduction</p>
<p><b>Explanation:</b> PCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It is most commonly used for dimensionality reduction by transforming a large set of variables into a smaller one that still contains most of the information in the large set. It does this by finding a new set of orthogonal axes (principal components) that capture the maximum variance in the data.</p>
<br>

<p><b>Question 8: The singular values \\(\sigma_i\\) of any matrix satisfy the property</b></p>
<p><b>Answer:</b> \\(\sigma_i \ge 0\\)</p>
<p><b>Explanation:</b> Singular values are defined as the square roots of the eigenvalues of the matrix \\(A^H A\\). The matrix \\(A^H A\\) is guaranteed to be positive semi-definite. A fundamental property of positive semi-definite matrices is that their eigenvalues are always real and non-negative. Consequently, the singular values, being the square roots of these non-negative eigenvalues, must also be real and non-negative (\\(\ge 0\\)).</p>
<br>

<p><b>Question 9: The eigenvalues of a symmetric positive semi-definite matrix are</b></p>
<p><b>Answer:</b> Real and non-negative</p>
<p><b>Explanation:</b> This is a core theorem in linear algebra.
<ul>
    <li>For any symmetric (or Hermitian) matrix, all its eigenvalues are real numbers.</li>
    <li>For a positive semi-definite matrix \\(A\\), by definition, \\(\mathbf{v}^H A \mathbf{v} \ge 0\\) for any vector \\(\mathbf{v}\\). If \\(\mathbf{v}\\) is an eigenvector with eigenvalue \\(\lambda\\), then \\(\mathbf{v}^H A \mathbf{v} = \mathbf{v}^H (\lambda\mathbf{v}) = \lambda \|\mathbf{v}\|^2\\). Since \\(\|\mathbf{v}\|^2 > 0\\) and \\(\mathbf{v}^H A \mathbf{v} \ge 0\\), it follows that \\(\lambda \ge 0\\).</li>
</ul>
Combining these two properties, the eigenvalues must be real and non-negative.</p>
<br>

<p><b>Question 10: The singular values \\(\sigma_i\\) of any matrix are arranged in</b></p>
<p><b>Answer:</b> Decreasing order</p>
<p><b>Explanation:</b> By convention, in the Singular Value Decomposition \\(A = U\Sigma V^H\\), the diagonal entries of \\(\Sigma\\) (the singular values) are ordered from largest to smallest: \\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r \ge 0\\), where \\(r\\) is the rank of the matrix. This ordering is crucial for applications like data compression and dimensionality reduction (e.g., PCA), where the largest singular values correspond to the most significant components of the matrix.</p>
</div></div><div class="week" id="week_8"><h1 class="week-title">Week 8</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 37 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 37 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas presented in the transcript, focusing on the application of the Linear Minimum Mean Square Error (LMMSE) estimation principle to a linear input-output system.</p>

<h3>1. The Linear System Model</h3>
<p>The analysis begins by defining a general linear input-output system. This model is widely used in various fields, including wireless communications and machine learning.</p>
<p>The system is described by the equation:</p>
\\[ \bar{y} = H \bar{x} + \bar{n} \\]
<p>Where the components are:</p>
<ul>
    <li>\\(\bar{y}\\): The output vector (e.g., received signals in a wireless system, or the set of explanatory/independent variables in a regression model).</li>
    <li>\\(\bar{x}\\): The input vector, which we want to estimate (e.g., transmitted symbols or the response/dependent variables to be predicted).</li>
    <li>\\(\bar{n}\\): The noise vector, representing random error or interference (e.g., measurement noise).</li>
    <li>\\(H\\): The system matrix that defines the linear transformation between the input and output (e.g., the wireless channel matrix or the design matrix in regression).</li>
</ul>
<p>A key point emphasized is that the LMMSE estimator is, by definition, the <i>best linear estimator</i>. It always produces a linear estimate, regardless of whether the underlying system model is linear or non-linear. This analysis explores the specific case where the system itself is linear.</p>

<h3>2. The LMMSE Estimator</h3>
<p>The goal is to find an estimate, \\(\hat{\bar{x}}\\), of the input \\(\bar{x}\\) using a linear function of the output \\(\bar{y}\\). The form of this linear estimator is:</p>
\\[ \hat{\bar{x}} = C \bar{y} \\]
<p>Here, \\(C\\) is a matrix of coefficients that minimizes the mean squared error between the true input \\(\bar{x}\\) and the estimate \\(\hat{\bar{x}}\\). For the LMMSE principle, this optimal coefficient matrix \\(C\\) is given by the formula:</p>
\\[ C = R_{xy} R_{yy}^{-1} \\]
<p>Where:</p>
<ul>
    <li>\\(R_{xy} = E[\bar{x} \bar{y}^T]\\) is the cross-covariance matrix between the input \\(\bar{x}\\) and the output \\(\bar{y}\\).</li>
    <li>\\(R_{yy} = E[\bar{y} \bar{y}^T]\\) is the auto-covariance matrix of the output \\(\bar{y}\\).</li>
</ul>
<p>To find the specific estimator for our linear system, we must first calculate \\(R_{xy}\\) and \\(R_{yy}\\) based on the model \\(\bar{y} = H \bar{x} + \bar{n}\\).</p>

<h3>3. Derivation of Covariance Matrices</h3>
<p>The derivation relies on a few standard statistical assumptions about the input and noise signals.</p>

<p><b>Assumptions:</b></p>
<ol>
    <li><b>Input Covariance (Signal Power):</b> The components of the input vector \\(\bar{x}\\) are uncorrelated and have the same variance (or power), \\(\gamma\\).
    \\[ R_{xx} = E[\bar{x} \bar{x}^T] = \gamma I \\]
    where \\(I\\) is the identity matrix.</li>
    <li><b>Noise Covariance (Noise Power):</b> The components of the noise vector \\(\bar{n}\\) are uncorrelated and have the same variance (or power), \\(\epsilon\\).
    \\[ R_{nn} = E[\bar{n} \bar{n}^T] = \epsilon I \\]</li>
    <li><b>Uncorrelated Signal and Noise:</b> The input signal \\(\bar{x}\\) and the noise \\(\bar{n}\\) are uncorrelated. This means their cross-covariance is zero.
    \\[ E[\bar{x} \bar{n}^T] = E[\bar{n} \bar{x}^T] = 0 \\]</li>
</ol>

<p><b>Deriving \\(R_{yy}\\) (Output Covariance):</b></p>
<p>We start with the definition of \\(R_{yy}\\) and substitute the linear model for \\(\bar{y}\\).</p>
\\[ R_{yy} = E[\bar{y} \bar{y}^T] = E[(H \bar{x} + \bar{n})(H \bar{x} + \bar{n})^T] \\]
<p>Expanding the expression gives:</p>
\\[ R_{yy} = E[H \bar{x} \bar{x}^T H^T + H \bar{x} \bar{n}^T + \bar{n} \bar{x}^T H^T + \bar{n} \bar{n}^T] \\]
<p>Using the linearity of the expectation operator and the assumption that the signal and noise are uncorrelated (the middle two terms become zero):</p>
\\[ R_{yy} = H E[\bar{x} \bar{x}^T] H^T + E[\bar{n} \bar{n}^T] = H R_{xx} H^T + R_{nn} \\]
<p>Substituting the assumed covariance structures for \\(R_{xx}\\) and \\(R_{nn}\\):</p>
\\[ R_{yy} = \gamma H H^T + \epsilon I \\]

<p><b>Deriving \\(R_{xy}\\) (Cross-Covariance):</b></p>
<p>Similarly, we substitute the linear model into the definition of \\(R_{xy}\\).</p>
\\[ R_{xy} = E[\bar{x} \bar{y}^T] = E[\bar{x}(H \bar{x} + \bar{n})^T] \\]
<p>Expanding this expression:</p>
\\[ R_{xy} = E[\bar{x}(\bar{x}^T H^T + \bar{n}^T)] = E[\bar{x} \bar{x}^T H^T + \bar{x} \bar{n}^T] \\]
<p>Applying the expectation operator and the uncorrelatedness assumption:</p>
\\[ R_{xy} = E[\bar{x} \bar{x}^T] H^T = R_{xx} H^T \\]
<p>Substituting the assumed structure for \\(R_{xx}\\):</p>
\\[ R_{xy} = \gamma H^T \\]

<h3>4. The LMMSE Estimator Formula and Simplification</h3>
<p>Now we can construct the LMMSE estimator by plugging the derived covariance matrices into the formula \\(\hat{\bar{x}} = R_{xy} R_{yy}^{-1} \bar{y}\\).</p>
\\[ \hat{\bar{x}} = (\gamma H^T)(\gamma H H^T + \epsilon I)^{-1} \bar{y} \\]
<p>This formula is correct, but it can be simplified into a more common and computationally efficient form using a matrix identity. The identity (a form of the Woodbury matrix identity, also known as the push-through rule) shown in the transcript is:</p>
\\[ (\gamma H^T H + \epsilon I)^{-1} H^T = H^T (\gamma H H^T + \epsilon I)^{-1} \\]
<p>Using this identity, we can rewrite the estimator. Let's focus on the matrix part of our estimator expression: \\(H^T(\gamma H H^T + \epsilon I)^{-1}\\). Applying the identity, this becomes \\((\gamma H^T H + \epsilon I)^{-1} H^T\\).</p>
<p>Substituting this back into the estimator equation:</p>
\\[ \hat{\bar{x}} = \gamma [(\gamma H^T H + \epsilon I)^{-1} H^T] \bar{y} \\]
<p>We can bring the scalar \\(\gamma\\) inside the inverse term (where it becomes \\(1/\gamma\\)):</p>
\\[ \hat{\bar{x}} = (H^T H + \frac{\epsilon}{\gamma} I)^{-1} H^T \bar{y} \\]
<p>The ratio \\(\gamma / \epsilon\\) represents the ratio of signal power to noise power, which is the <b>Signal-to-Noise Ratio (SNR)</b>. Therefore, \\(\epsilon / \gamma = 1 / \text{SNR}\\).</p>
<p>This gives the final, widely recognized form of the LMMSE estimator for a linear system:</p>
\\[ \hat{\bar{x}} = \left(H^T H + \frac{1}{\text{SNR}} I\right)^{-1} H^T \bar{y} \\]
<p>This is known as the <b>LMMSE receiver</b> in wireless communications or the <b>LMMSE regressor</b> in machine learning (it is mathematically equivalent to Ridge Regression).</p>

<p>For systems involving complex numbers (common in communications), the transpose operation (\\(T\\)) is replaced by the Hermitian (conjugate transpose) operation (\\(H\\)):</p>
\\[ \hat{\bar{x}} = \left(H^H H + \frac{1}{\text{SNR}} I\right)^{-1} H^H \bar{y} \\]

<h3>5. Estimation Error Covariance and Mean Squared Error (MSE)</h3>
<p>The performance of the estimator is measured by its error. The estimation error covariance matrix, \\(R_{ee}\\), is given by the general formula:</p>
\\[ R_{ee} = R_{xx} - R_{xy} R_{yy}^{-1} R_{yx} \\]
<p>where \\(R_{yx} = R_{xy}^T\\). Substituting our derived matrices:</p>
\\[ R_{ee} = \gamma I - (\gamma H^T)(\gamma H H^T + \epsilon I)^{-1}(\gamma H) \\]
<p>Using the same matrix identity to simplify, this becomes:</p>
\\[ R_{ee} = \gamma I - \gamma^2 (\gamma H^T H + \epsilon I)^{-1} H^T H \\]
<p>A mathematical trick is used to simplify this further. We can write \\(\gamma^2 H^T H = \gamma [(\gamma H^T H + \epsilon I) - \epsilon I]\\). Substituting this in:</p>
\\[ R_{ee} = \gamma I - \gamma (\gamma H^T H + \epsilon I)^{-1} [(\gamma H^T H + \epsilon I) - \epsilon I] \\]
\\[ R_{ee} = \gamma I - \gamma \left[I - \epsilon (\gamma H^T H + \epsilon I)^{-1}\right] \\]
\\[ R_{ee} = \gamma I - \gamma I + \gamma \epsilon (\gamma H^T H + \epsilon I)^{-1} \\]
<p>This simplifies to the final expression for the error covariance matrix:</p>
\\[ R_{ee} = \gamma \epsilon (\gamma H^T H + \epsilon I)^{-1} \\]
<p>This can also be expressed as:</p>
\\[ R_{ee} = \epsilon \left(H^T H + \frac{\epsilon}{\gamma} I\right)^{-1} = \epsilon \left(H^T H + \frac{1}{\text{SNR}} I\right)^{-1} \\]

<p>The overall <b>Mean Squared Error (MSE)</b> is the trace of this covariance matrix (the sum of its diagonal elements), as this represents the sum of the variances of the individual error components.</p>
\\[ \text{MSE} = \text{Tr}(R_{ee}) = \text{Tr}\left(\epsilon \left(H^T H + \frac{1}{\text{SNR}} I\right)^{-1}\right) \\]

</div></div><div class="chapter" id="Lecture 38 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 38 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the application of the Linear Minimum Mean Squared Error (LMMSE) principle to channel estimation in a wireless communication system.</p>

<b>1. The MISO System Model</b>
<p>The transcript begins by introducing a specific type of wireless system used for the application: a <b>Multiple-Input Single-Output (MISO)</b> system. This system is characterized by:</p>
<ul>
    <li><b>Multiple Inputs:</b> There are \\(T\\) transmit antennas.</li>
    <li><b>Single Output:</b> There is one receive antenna.</li>
</ul>
<p>The signal travels from each of the \\(T\\) transmit antennas to the single receive antenna. The path from each transmit antenna \\(i\\) to the receiver is characterized by a channel coefficient, \\(h_i\\). The collection of all these coefficients forms the <b>channel vector</b>.</p>
<p>At a specific time instant \\(k\\), the system can be described by the following linear model:</p>
\\[ y(k) = \begin{bmatrix} x_1(k) & x_2(k) & \dots & x_T(k) \end{bmatrix} \begin{bmatrix} h_1 \\ h_2 \\ \vdots \\ h_T \end{bmatrix} + n(k) \\]
<p>This can be written more compactly in vector form:</p>
\\[ y(k) = \mathbf{x}^T(k) \mathbf{h} + n(k) \\]
<p>Where:</p>
<ul>
    <li>\\(y(k)\\) is the scalar symbol received at time \\(k\\).</li>
    <li>\\(\mathbf{x}(k)\\) is the \\(T \times 1\\) vector of symbols transmitted at time \\(k\\). In the context of channel estimation, this is often a known sequence called a "pilot vector".</li>
    <li>\\(\mathbf{h}\\) is the \\(T \times 1\\) channel vector that we want to estimate.</li>
    <li>\\(n(k)\\) is the scalar additive noise sample at time \\(k\\).</li>
</ul>

<b>2. The Channel Estimation Model</b>
<p>To estimate the channel vector \\(\mathbf{h}\\), the transmitter sends a sequence of \\(L\\) known pilot vectors over \\(L\\) consecutive time instants. This generates \\(L\\) corresponding received symbols. We can stack these \\(L\\) equations:</p>
\\[ y_1 = \mathbf{x}_1^T \mathbf{h} + n_1 \\]
\\[ y_2 = \mathbf{x}_2^T \mathbf{h} + n_2 \\]
\\[ \vdots \\]
\\[ y_L = \mathbf{x}_L^T \mathbf{h} + n_L \\]
<p>Using the principles of linear algebra, this system of scalar equations can be elegantly represented in a single matrix-vector equation:</p>
\\[ \mathbf{y} = \mathbf{Xh} + \mathbf{n} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is the \\(L \times 1\\) vector of received symbols: \\( \mathbf{y} = [y_1, y_2, \dots, y_L]^T \\).</li>
    <li>\\(\mathbf{X}\\) is the \\(L \times T\\) <b>pilot matrix</b>, where each row is a transposed pilot vector: \\( \mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_L]^T \\).</li>
    <li>\\(\mathbf{h}\\) is the unknown \\(T \times 1\\) channel vector to be estimated.</li>
    <li>\\(\mathbf{n}\\) is the \\(L \times 1\\) vector of noise samples: \\( \mathbf{n} = [n_1, n_2, \dots, n_L]^T \\).</li>
</ul>
<p>This is the fundamental linear model for channel estimation.</p>

<b>3. Least Squares (LS) vs. LMMSE Estimation</b>
<p>The transcript contrasts two methods for estimating \\(\mathbf{h}\\) from the model \\(\mathbf{y} = \mathbf{Xh} + \mathbf{n}\\).</p>

<p><b>A. The Least Squares (LS) Estimator</b></p>
<p>The LS approach finds an estimate \\(\hat{\mathbf{h}}\\) that minimizes the squared Euclidean distance between the received vector \\(\mathbf{y}\\) and the model's prediction \\(\mathbf{Xh}\\). It solves:</p>
\\[ \min_{\mathbf{h}} ||\mathbf{y} - \mathbf{Xh}||^2 \\]
<p>The solution, which requires no statistical information about \\(\mathbf{h}\\) or \\(\mathbf{n}\\), is:</p>
\\[ \hat{\mathbf{h}}_{LS} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} \\]
<p><i>(Note: If the quantities are complex, the transpose \\((\cdot)^T\\) is replaced by the Hermitian (conjugate transpose) \\((\cdot)^H\\).)</i></p>

<p><b>B. The LMMSE Estimator</b></p>
<p>The LMMSE estimator is more sophisticated because it incorporates <b>prior statistical information</b> about the unknown channel \\(\mathbf{h}\\) and the noise \\(\mathbf{n}\\). The key assumptions made in the transcript are:</p>
<ul>
    <li>The mean of the channel vector is zero: \\( E[\mathbf{h}] = \mathbf{0} \\).</li>
    <li>The channel coefficients are uncorrelated with equal variance \\(\sigma_h^2\\). This gives a covariance matrix: \\( E[\mathbf{h}\mathbf{h}^H] = \sigma_h^2 \mathbf{I} \\).</li>
    <li>The noise samples are uncorrelated with equal variance \\(\sigma^2\\). This gives a covariance matrix: \\( E[\mathbf{n}\mathbf{n}^H] = \sigma^2 \mathbf{I} \\).</li>
</ul>
<p>Under these assumptions, the LMMSE estimate of \\(\mathbf{h}\\) is given by:</p>
\\[ \hat{\mathbf{h}}_{LMMSE} = (\mathbf{X}^T \mathbf{X} + \frac{1}{\text{SNR}}\mathbf{I})^{-1} \mathbf{X}^T \mathbf{y} \\]
<p>Here, the <b>Signal-to-Noise Ratio (SNR)</b> is defined as the ratio of the variance of the parameter being estimated (the signal) to the variance of the measurement noise:</p>
\\[ \text{SNR} = \frac{\sigma_h^2}{\sigma^2} \\]

<b>4. Analysis of the LMMSE Estimator</b>

<p><b>High SNR Behavior (\\(\text{SNR} \to \infty\\))</b></p>
<p>When the SNR is very high, it means the noise power (\\(\sigma^2\\)) is negligible compared to the channel power (\\(\sigma_h^2\\)). In this case:</p>
\\[ \text{As SNR} \to \infty, \quad \frac{1}{\text{SNR}} \to 0 \\]
<p>The LMMSE formula then simplifies:</p>
\\[ \hat{\mathbf{h}}_{LMMSE} \to (\mathbf{X}^T \mathbf{X} + 0 \cdot \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \hat{\mathbf{h}}_{LS} \\]
<p><b>Insight:</b> At high SNR, the LMMSE estimator converges to the LS estimator. This is because the prior information about the channel becomes less important when the observations (data) are very clean and reliable.</p>

<p><b>Low SNR Behavior (\\(\text{SNR} \to 0\\))</b></p>
<p>When the SNR is very low, the noise power dominates. The term \\(\frac{1}{\text{SNR}}\\) becomes very large. The matrix to be inverted, \\(\mathbf{X}^T \mathbf{X} + \frac{1}{\text{SNR}}\mathbf{I}\\), is dominated by the second term. Consequently, the estimate \\(\hat{\mathbf{h}}_{LMMSE}\\) is pulled towards zero.</p>
\\[ \hat{\mathbf{h}}_{LMMSE} \to \mathbf{0} \\]
<p><b>Insight:</b> At low SNR, the observations are swamped by noise and provide no useful information. The LMMSE estimator wisely discards the noisy data and defaults to the best guess based only on prior information, which is the prior mean of the channel (assumed to be \\(\mathbf{0}\\)).</p>

<b>5. Error Covariance of the LMMSE Estimator</b>
<p>The error covariance matrix, \\(\mathbf{R}_{ee} = E[(\hat{\mathbf{h}} - \mathbf{h})(\hat{\mathbf{h}} - \mathbf{h})^H]\\), measures the uncertainty in the estimate. For the LMMSE estimator, it is given by:</p>
\\[ \mathbf{R}_{ee} = \left( \frac{1}{\sigma^2}\mathbf{X}^T\mathbf{X} + \frac{1}{\sigma_h^2}\mathbf{I} \right)^{-1} \\]
<ul>
    <li>At <b>high SNR</b> (\\(\sigma_h^2 \gg \sigma^2\\)), this approaches \\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\\), which is the error covariance of the LS estimator.</li>
    <li>At <b>low SNR</b> (\\(\sigma_h^2 \ll \sigma^2\\)), this approaches \\(\sigma_h^2\mathbf{I}\\), which is the original (prior) covariance of the channel. This confirms that at low SNR, making observations does not reduce our uncertainty about the channel.</li>
</ul>

<b>6. Numerical Example</b>
<p>The transcript provides a concrete example with the following parameters:</p>
<ul>
    <li>Pilot Matrix: \\( \mathbf{X} = \begin{bmatrix} 1 & -1 \\ 1 & -1 \\ -1 & 1 \\ -1 & 1 \end{bmatrix} \\) <i>(Note: There is a discrepancy in the transcript text vs. calculation. The calculation uses \\( \mathbf{X} = \begin{bmatrix} 1 & -1 \\ -1 & -1 \\ 1 & 1 \\ -1 & 1 \end{bmatrix} \\). The explanation below follows the matrix used in the calculation.)</i> Let's assume the matrix used in calculation is correct: \\( \mathbf{X} = \begin{bmatrix} 1 & -1 \\ -1 & -1 \\ 1 & 1 \\ -1 & 1 \end{bmatrix} \\)</li>
    <li>Received Vector: \\( \mathbf{y} = [1, -2, -1, -2]^T \\)</li>
    <li>Channel Variance: \\( \sigma_h^2 = 1 \\)</li>
    <li>Noise Variance: \\( \sigma^2 = 2 \\)</li>
</ul>

<p><b>Step 1: Calculate SNR</b></p>
\\[ \text{SNR} = \frac{\sigma_h^2}{\sigma^2} = \frac{1}{2} \\]

<p><b>Step 2: Compute \\(\mathbf{X}^T\mathbf{X}\\)</b></p>
\\[ \mathbf{X}^T\mathbf{X} = \begin{bmatrix} 1 & -1 & 1 & -1 \\ -1 & -1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & -1 \\ -1 & -1 \\ 1 & 1 \\ -1 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} = 4\mathbf{I} \\]

<p><b>Step 3: Compute the matrix to be inverted</b></p>
\\[ \mathbf{X}^T\mathbf{X} + \frac{1}{\text{SNR}}\mathbf{I} = 4\mathbf{I} + \frac{1}{1/2}\mathbf{I} = 4\mathbf{I} + 2\mathbf{I} = 6\mathbf{I} \\]
<p>The inverse is simply \\((6\mathbf{I})^{-1} = \frac{1}{6}\mathbf{I}\\).</p>

<p><b>Step 4: Compute \\(\mathbf{X}^T\mathbf{y}\\)</b></p>
\\[ \mathbf{X}^T\mathbf{y} = \begin{bmatrix} 1 & -1 & 1 & -1 \\ -1 & -1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ -2 \\ -1 \\ -2 \end{bmatrix} = \begin{bmatrix} 1+2-1+2 \\ -1+2-1-2 \end{bmatrix} = \begin{bmatrix} 4 \\ -2 \end{bmatrix} \\]

<p><b>Step 5: Calculate the LMMSE Estimate</b></p>
\\[ \hat{\mathbf{h}}_{LMMSE} = (\frac{1}{6}\mathbf{I}) (\mathbf{X}^T\mathbf{y}) = \frac{1}{6} \begin{bmatrix} 4 \\ -2 \end{bmatrix} = \begin{bmatrix} 4/6 \\ -2/6 \end{bmatrix} = \begin{bmatrix} 2/3 \\ -1/3 \end{bmatrix} \\]

<p><b>Step 6: Calculate the Error Covariance and MSE</b></p>
\\[ \mathbf{R}_{ee} = \left( \frac{1}{\sigma^2}\mathbf{X}^T\mathbf{X} + \frac{1}{\sigma_h^2}\mathbf{I} \right)^{-1} = \left( \frac{1}{2}(4\mathbf{I}) + \frac{1}{1}\mathbf{I} \right)^{-1} = (2\mathbf{I} + \mathbf{I})^{-1} = (3\mathbf{I})^{-1} = \frac{1}{3}\mathbf{I} \\]
\\[ \mathbf{R}_{ee} = \begin{bmatrix} 1/3 & 0 \\ 0 & 1/3 \end{bmatrix} \\]
<p>The <b>Mean Squared Error (MSE)</b> is the sum of the variances of the estimation errors for each component, which is the trace of the error covariance matrix.</p>
\\[ \text{MSE} = \text{Tr}(\mathbf{R}_{ee}) = \frac{1}{3} + \frac{1}{3} = \frac{2}{3} \\]
</div></div><div class="chapter" id="Lecture 39 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 39 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts of Autoregression, Linear Prediction, and their connection to the Linear Minimum Mean Square Error (LMSE) principle, as presented in the transcript.</p>

<b>1. Introduction to Autoregression (AR)</b>
<p>
Autoregression, often abbreviated as AR, is a statistical method used for modeling time-series data. The name itself breaks down into two parts:
<ul>
<li><b>Auto</b>: This signifies "self," implying that the model uses the variable's own past values.</li>
<li><b>Regression</b>: This refers to the process of prediction or approximation.</li>
</ul>
Therefore, autoregression is a method of <b>self-prediction</b>, where future values of a process are predicted based on a linear combination of its own past values. This falls under the broader theory of <b>Linear Prediction</b>, which has wide-ranging applications in fields like speech processing (e.g., Linear Predictive Coding or LPC) and data compression.
</p>

<b>2. AR Models and Time Series</b>
<p>
Autoregression is particularly well-suited for the analysis of a <b>time series</b>. A time series is a sequence of data points collected at successive, equally spaced points in time. It can be represented as:
\\[ x_0, x_1, x_2, \dots, x_n, \dots \\]
where \\(n\\) is the time index. Examples include daily stock prices, monthly rainfall measurements, or hourly temperature readings.
</p>
<p>
The central goal in time series analysis is often to forecast future values. Autoregression achieves this by modeling the current value of the series, \\(x(n)\\), as a function of its previous values. An <b>L-th order AR model</b> uses the \\(L\\) most recent past samples to make a prediction, \\(\hat{x}(n)\\). The model is expressed as a linear combination:
\\[ \hat{x}(n) = a_1 x(n-1) + a_2 x(n-2) + \dots + a_L x(n-L) \\]
The coefficients \\(a_1, a_2, \dots, a_L\\) are the <b>regression coefficients</b> that define the model. The core problem is to find the optimal values for these coefficients.
</p>

<b>3. Optimal AR Coefficients via LMSE</b>
<p>
To find the best regression coefficients, we use the <b>Linear Minimum Mean Square Error (LMSE)</b> principle. The goal is to choose the coefficients \\(a_k\\) such that the average squared difference between the actual value \\(x(n)\\) and the predicted value \\(\hat{x}(n)\\) is minimized.
</p>
<p>
Following the LMSE framework, we define:
<ul>
    <li>The quantity to be estimated: \\(x(n)\\)</li>
    <li>The vector of known quantities (the past samples): \\(\bar{x} = [x(n-1), x(n-2), \dots, x(n-L)]^T\\)</li>
</ul>
The optimal vector of regression coefficients, denoted \\(\bar{a} = [a_1, a_2, \dots, a_L]^T\\), is given by the formula:
\\[ \bar{a}^T = \mathbf{R}_{x(n)\bar{x}} \mathbf{R}_{\bar{x}\bar{x}}^{-1} \\]
where:
<ul>
    <li>\\(\mathbf{R}_{x(n)\bar{x}}\\) is the cross-correlation vector between the quantity to be estimated, \\(x(n)\\), and the vector of known data, \\(\bar{x}\\).</li>
    <li>\\(\mathbf{R}_{\bar{x}\bar{x}}\\) is the autocorrelation matrix of the known data vector \\(\bar{x}\\).</li>
</ul>
</p>

<b>4. The Wide-Sense Stationary (WSS) Assumption</b>
<p>
To calculate the correlation vector and matrix, a key assumption is made about the time series: it is a <b>Wide-Sense Stationary (WSS)</b> process. A WSS process has statistical properties that do not change over time. Specifically, its autocorrelation function depends only on the time difference (or lag) between two samples, not on their absolute position in time.
</p>
<p>
Mathematically, for any two time indices \\(i\\) and \\(j\\), the correlation is:
\\[ E[x(i)x(j)] = R_{xx}(i-j) \\]
where \\(R_{xx}(k)\\) is the autocorrelation function at lag \\(k\\). This property greatly simplifies the structure of the required correlation matrices.
</p>

<b>5. Deriving the Correlation and Cross-Correlation</b>

<p><b>A. The Autocorrelation Matrix \\(\mathbf{R}_{\bar{x}\bar{x}}\\)</b></p>
<p>
The autocorrelation matrix is defined as \\(\mathbf{R}_{\bar{x}\bar{x}} = E[\bar{x}\bar{x}^T]\\). Using the WSS property, each element \\((i, j)\\) of this matrix is calculated as:
\\[ (\mathbf{R}_{\bar{x}\bar{x}})_{ij} = E[x(n-i)x(n-j)] = R_{xx}((n-i)-(n-j)) = R_{xx}(j-i) \\]
Since \\(R_{xx}(-k) = R_{xx}(k)\\) for a real process, this becomes \\(R_{xx}(|i-j|)\\). This results in a highly structured matrix where all elements along any given diagonal are identical.
<br><br>
For an L-th order model, the matrix is:
\\[ \mathbf{R}_{\bar{x}\bar{x}} = 
\begin{pmatrix}
R_{xx}(0) & R_{xx}(1) & R_{xx}(2) & \dots & R_{xx}(L-1) \\
R_{xx}(1) & R_{xx}(0) & R_{xx}(1) & \dots & R_{xx}(L-2) \\
R_{xx}(2) & R_{xx}(1) & R_{xx}(0) & \dots & R_{xx}(L-3) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
R_{xx}(L-1) & R_{xx}(L-2) & R_{xx}(L-3) & \dots & R_{xx}(0)
\end{pmatrix}
\\]
A matrix with this constant-diagonal structure is known as a <b>Toeplitz matrix</b>.
</p>

<p><b>B. The Cross-Correlation Vector \\(\mathbf{R}_{x(n)\bar{x}}\\)</b></p>
<p>
The cross-correlation vector is defined as \\(\mathbf{R}_{x(n)\bar{x}} = E[x(n)\bar{x}^T]\\). Its elements are calculated using the WSS property:
\\[ E[x(n)x(n-k)] = R_{xx}(n-(n-k)) = R_{xx}(k) \\]
This gives the vector:
\\[ \mathbf{R}_{x(n)\bar{x}} = [R_{xx}(1), R_{xx}(2), \dots, R_{xx}(L)] \\]
</p>

<b>6. The Optimal Autoregression Coefficients</b>
<p>
By substituting the derived Toeplitz matrix and cross-correlation vector into the LMSE formula, we get the expression for the optimal regression coefficients:
\\[ \bar{a}^T = [R_{xx}(1), R_{xx}(2), \dots, R_{xx}(L)] \begin{pmatrix}
R_{xx}(0) & R_{xx}(1) & \dots & R_{xx}(L-1) \\
R_{xx}(1) & R_{xx}(0) & \dots & R_{xx}(L-2) \\
\vdots & \vdots & \ddots & \vdots \\
R_{xx}(L-1) & R_{xx}(L-2) & \dots & R_{xx}(0)
\end{pmatrix}^{-1} \\]
Solving this system of linear equations (known as the Yule-Walker equations) provides the coefficients for the best linear predictor that minimizes the mean squared error.
</p>

<b>7. Characterizing the Prediction Error</b>
<p>
A predictor is only useful if we can quantify its performance. The LMSE principle also provides a formula for the minimum mean squared error (MMSE), which is the variance of the prediction error, \\(\sigma_e^2\\).
\\[ \sigma_e^2 = R_{x(n)x(n)} - \mathbf{R}_{x(n)\bar{x}} \mathbf{R}_{\bar{x}\bar{x}}^{-1} \mathbf{R}_{\bar{x}x(n)} \\]
Recognizing that \\(R_{x(n)x(n)} = R_{xx}(0)\\) and \\(\bar{a}^T = \mathbf{R}_{x(n)\bar{x}} \mathbf{R}_{\bar{x}\bar{x}}^{-1}\\), the formula simplifies to:
\\[ \sigma_e^2 = R_{xx}(0) - \bar{a}^T \mathbf{R}_{\bar{x}x(n)} \\]
Expanding this dot product gives:
\\[ \sigma_e^2 = R_{xx}(0) - [a_1 R_{xx}(1) + a_2 R_{xx}(2) + \dots + a_L R_{xx}(L)] \\]
or
\\[ \sigma_e^2 = R_{xx}(0) - \sum_{k=1}^{L} a_k R_{xx}(k) \\]
This formula shows how the prediction error depends on the signal's variance (\\(R_{xx}(0)\\)) and how well the past values are correlated with the present value.
</p>

<b>8. Example: The First-Order AR(1) Model</b>
<p>
The simplest case is the first-order AR model, denoted AR(1), where \\(L=1\\). The prediction is based on only the single most recent sample:
\\[ \hat{x}(n) = \beta x(n-1) \\]
Here, \\(\beta\\) is the single regression coefficient (\\(a_1\\)).

<p><b>A. Calculating the AR(1) Coefficient</b></p>
<p>
Using the general formulas, the vector and matrix become scalars:
<ul>
    <li>\\(\mathbf{R}_{x(n)\bar{x}} = R_{xx}(1)\\)</li>
    <li>\\(\mathbf{R}_{\bar{x}\bar{x}} = R_{xx}(0)\\)</li>
</ul>
The coefficient \\(\beta\\) is then:
\\[ \beta = R_{xx}(1) \times (R_{xx}(0))^{-1} = \frac{R_{xx}(1)}{R_{xx}(0)} \\]
So, the optimal first-order predictor is:
\\[ \hat{x}(n) = \left(\frac{R_{xx}(1)}{R_{xx}(0)}\right) x(n-1) \\]

<p><b>B. Calculating the AR(1) Regression Error</b></p>
<p>
The regression error for the AR(1) model is found using the general error formula with \\(L=1\\) and \\(a_1 = \beta\\):
\\[ \sigma_e^2 = R_{xx}(0) - a_1 R_{xx}(1) = R_{xx}(0) - \left(\frac{R_{xx}(1)}{R_{xx}(0)}\right) R_{xx}(1) \\]
\\[ \sigma_e^2 = R_{xx}(0) - \frac{R_{xx}(1)^2}{R_{xx}(0)} \\]
This represents the residual variance of the time series after accounting for the information contained in the previous sample.
</p>
</div></div><div class="chapter" id="Lecture 40 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 40 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas discussed in the transcript regarding <b>Recommender Systems</b>, a significant application of linear algebra in data analytics and machine learning.</p>

<h3>1. Introduction to Recommender Systems</h3>
<p>A recommender system is an algorithm designed to suggest relevant items (e.g., products, movies, articles) to users. These systems are a critical component of modern commercial websites like Amazon (for products) and Netflix (for movies and series). The primary goal is to predict a user's preference for an item they have not yet seen or purchased.</p>
<p>The core idea is to analyze historical data, such as:</p>
<ul>
    <li><b>E-commerce:</b> A user's search and purchase history is compared with the histories of other users with similar tastes. The system then recommends products that these similar users have purchased and liked.</li>
    <li><b>Video Streaming:</b> The system analyzes a user's viewing history to recommend movies or series that have been enjoyed by other users with similar viewing patterns.</li>
</ul>
<p>By making relevant suggestions, these systems aim to enhance user experience and increase engagement or sales.</p>

<h3>2. The Matrix Completion Problem</h3>
<p>At its heart, the recommender system problem can be modeled as a <b>matrix completion problem</b>. We can organize the user-item interaction data into a large matrix.</p>
<ul>
    <li>The <b>rows</b> of the matrix represent the users (let's say there are \\(l\\) users).</li>
    <li>The <b>columns</b> represent the items, such as movies (let's say there are \\(k\\) movies).</li>
    <li>The entry in the \\(i\\)-th row and \\(j\\)-th column, denoted as \\(r_{ij}\\), is the rating user \\(i\\) has given to movie \\(j\\).</li>
</ul>
<p>A key characteristic of this matrix is that it is <b>sparse</b>, meaning most of its entries are unknown. This is because a typical user has only seen and rated a very small fraction of the total number of available movies. The task is to predict these missing entries.</p>
<p><b>Example Matrix:</b></p>
<p>
    \\[
    \begin{pmatrix}
     ? & r_{12} & ? & r_{14} & \dots \\
     r_{21} & r_{22} & ? & ? & \dots \\
     ? & ? & r_{33} & ? & \dots \\
     \vdots & \vdots & \vdots & \vdots & \ddots
    \end{pmatrix}
    \\]
</p>
<p>Here, entries like \\(r_{12}\\) and \\(r_{21}\\) are <b>known ratings</b>, while entries marked with '?' are <b>unknown ratings</b> that we need to predict. The process of filling in these blanks is called matrix completion.</p>

<h3>3. A Simple Recommender Algorithm using Linear Algebra</h3>
<p>The transcript outlines a linear model to solve the matrix completion problem. The process involves several steps:</p>

<h4>Step 1: Calculate and Remove the Overall Bias</h4>
<p>First, we compute the average of all known ratings in the matrix. This value, \\(r_a\\), represents the overall bias of the system (e.g., the tendency for all ratings to be around 3.5 on a 5-point scale).</p>
<p>The formula for this overall average rating is:</p>
\\[
r_a = \frac{\sum_{i,j \text{ s.t. } r_{ij} \text{ is known}} r_{ij}}{\text{Number of known ratings}}
\\]
<p>Next, we create a new "unbiased" rating, \\(\tilde{r}_{ij}\\), for each known rating by subtracting this overall average. This centers the data around zero.</p>
\\[
\tilde{r}_{ij} = r_{ij} - r_a
\\]

<h4>Step 2: A First-Order Model for Ratings</h4>
<p>We assume a simple, linear model to explain the unbiased ratings. The model proposes that a user's rating for a movie depends on two main factors:</p>
<ul>
    <li><b>User Bias (\\(u_i\\)):</b> The specific tendency of user \\(i\\). Some users are generally lenient and give high ratings, while others are critical and give low ratings.</li>
    <li><b>Movie Bias (\\(m_j\\)):</b> The inherent quality or characteristic of movie \\(j\\). Some movies are critically acclaimed and tend to receive high ratings from everyone, while others are widely disliked.</li>
</ul>
<p>The model expresses the unbiased rating as the sum of these two biases:</p>
\\[
\tilde{r}_{ij} \approx u_i + m_j
\\]
<p>Our goal is to find the unknown values of \\(u_i\\) for all users and \\(m_j\\) for all movies.</p>

<h4>Step 3: Formulating a System of Linear Equations</h4>
<p>For every known rating \\(r_{ij}\\), we can write an equation based on our model. For example:</p>
<ul>
    <li>\\(\tilde{r}_{12} = u_1 + m_2\\)</li>
    <li>\\(\tilde{r}_{14} = u_1 + m_4\\)</li>
    <li>\\(\tilde{r}_{21} = u_2 + m_1\\)</li>
</ul>
<p>By collecting all such equations for every known rating, we can form a large system of linear equations. This system can be written in matrix form as:</p>
\\[
\mathbf{\tilde{r}} = C \mathbf{b}
\\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{\tilde{r}}\\) is a column vector containing all the known, unbiased ratings \\(\tilde{r}_{ij}\\).</li>
    <li>\\(\mathbf{b}\\) is a column vector containing all the unknown biases we want to find. It is formed by stacking the user biases and movie biases: \\( \mathbf{b} = [u_1, u_2, \dots, u_l, m_1, m_2, \dots, m_k]^T \\). The size of this vector is \\((l+k) \times 1\\).</li>
    <li>\\(C\\) is a large, sparse matrix consisting of only 0s and 1s. Each row of \\(C\\) corresponds to one known rating and has exactly two '1's to pick out the correct user bias \\(u_i\\) and movie bias \\(m_j\\) for that rating's equation.</li>
</ul>

<h4>Step 4: Solving the System with Least Squares</h4>
<p>Typically, the number of known ratings (the number of equations) is much larger than the number of unknowns (\\(l+k\\), the number of users plus movies). This makes the system <b>overdetermined</b>, meaning there is usually no exact solution for \\(\mathbf{b}\\). Instead, we find the "best fit" solution that minimizes the error between our model's predictions (\\(C\mathbf{b}\\)) and the actual data (\\(\mathbf{\tilde{r}}\\)). This is achieved using the <b>least squares method</b>.</p>
<p>The least squares solution for the bias vector \\(\mathbf{b}\\) is given by the formula:</p>
\\[
\mathbf{b} = (C^T C)^{-1} C^T \mathbf{\tilde{r}}
\\]
<p>Solving this equation gives us the estimated values for all user biases \\(u_i\\) and movie biases \\(m_j\\).</p>

<h4>Step 5: Prediction and Recommendation</h4>
<p>Once we have the bias vector \\(\mathbf{b}\\), we can predict any unknown rating. The predicted rating, \\(\hat{r}_{ij}\\), for a movie user \\(i\\) has not seen is calculated by adding the overall bias back to our model's output:</p>
\\[
\hat{r}_{ij} = r_a + u_i + m_j
\\]
<p>With these predicted ratings, the final step is to make a recommendation. For a given user \\(i\\), the system will:</p>
<ol>
    <li>Identify all movies \\(j\\) that user \\(i\\) has <b>not</b> seen.</li>
    <li>Calculate the predicted rating \\(\hat{r}_{ij}\\) for each of these unseen movies.</li>
    <li>Recommend the unseen movie with the <b>highest predicted rating</b>.</li>
</ol>

<h3>4. Relevance and Conclusion</h3>
<p>This method demonstrates a powerful application of linear algebra to a modern, complex problem. While real-world recommender systems employ more sophisticated models (e.g., matrix factorization techniques like Singular Value Decomposition), this first-order bias model illustrates the fundamental principles. The ability to process vast amounts of user data using techniques like least squares is central to the success of today's e-commerce, social media, and streaming platforms.</p>
</div></div><div class="chapter" id="Lecture 41 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 41 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas used in the transcript to build a simple movie recommender system. The method is based on linear algebra and aims to predict a user's rating for a movie they have not yet seen.</p>

<h3>1. Problem Setup: The Ratings Matrix</h3>
<p>The core of a recommender system is a matrix of user ratings. In this example, we have a small system to illustrate the principles.</p>
<ul>
    <li><b>Number of Movies (k):</b> The example sets \\(k=3\\).</li>
    <li><b>Number of Users (l):</b> The example sets \\(l=3\\).</li>
</ul>
<p>The ratings are organized into a matrix, which we can call \\(R\\), where the rows represent users and the columns represent movies. The entry \\(r_{ij}\\) is the rating given by user \\(i\\) to movie \\(j\\).</p>
<p>The sample ratings matrix is:</p>
<table border="1" style="border-collapse: collapse; text-align: center; margin: 1em;">
    <tr>
        <th style="padding: 5px;"></th>
        <th style="padding: 5px;">Movie 1</th>
        <th style="padding: 5px;">Movie 2</th>
        <th style="padding: 5px;">Movie 3</th>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 1</b></td>
        <td style="padding: 5px;">4</td>
        <td style="padding: 5px;">3</td>
        <td style="padding: 5px;">2</td>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 2</b></td>
        <td style="padding: 5px;">2</td>
        <td style="padding: 5px;">?</td>
        <td style="padding: 5px;">3</td>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 3</b></td>
        <td style="padding: 5px;">2</td>
        <td style="padding: 5px;">4</td>
        <td style="padding: 5px;">5</td>
    </tr>
</table>
<p>The goal is to predict the unknown rating, \\(r_{22}\\), which is the rating User 2 would have given to Movie 2. This prediction is denoted as \\(\hat{r}_{22}\\).</p>

<h3>2. Step 1: Calculating the Overall Bias</h3>
<p>The first step is to calculate the overall bias or the average rating across all available entries in the system. This value, denoted \\(r_a\\), serves as a baseline for all predictions.</p>
<p><b>Formula:</b></p>
\\[ r_a = \frac{\text{Sum of all available ratings}}{\text{Total number of available ratings}} \\]
<p>Using the data from the matrix (8 available ratings):</p>
\\[ r_a = \frac{4+3+2+2+3+2+4+5}{8} = \frac{25}{8} = 3.125 \\]
<p>This value represents the average tendency of all users' ratings in this specific system.</p>

<h3>3. Step 2: Normalizing the Ratings by Removing the Bias</h3>
<p>To isolate the specific preferences of users and inherent qualities of movies, the overall bias is subtracted from each known rating. The resulting bias-removed ratings are denoted by \\(\tilde{r}_{ij}\\).</p>
<p><b>Formula:</b></p>
\\[ \tilde{r}_{ij} = r_{ij} - r_a \\]
<p>For example, for User 1's rating of Movie 1:</p>
\\[ \tilde{r}_{11} = r_{11} - r_a = 4 - 3.125 = 0.875 \\]
<p>Applying this to all known ratings gives the following matrix of bias-removed ratings, \\(\tilde{R}\\):</p>
<table border="1" style="border-collapse: collapse; text-align: center; margin: 1em;">
    <tr>
        <th style="padding: 5px;"></th>
        <th style="padding: 5px;">Movie 1</th>
        <th style="padding: 5px;">Movie 2</th>
        <th style="padding: 5px;">Movie 3</th>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 1</b></td>
        <td style="padding: 5px;">0.875</td>
        <td style="padding: 5px;">-0.125</td>
        <td style="padding: 5px;">-1.125</td>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 2</b></td>
        <td style="padding: 5px;">-1.125</td>
        <td style="padding: 5px;">?</td>
        <td style="padding: 5px;">-0.125</td>
    </tr>
    <tr>
        <td style="padding: 5px;"><b>User 3</b></td>
        <td style="padding: 5px;">-1.125</td>
        <td style="padding: 5px;">0.875</td>
        <td style="padding: 5px;">1.875</td>
    </tr>
</table>

<h3>4. Step 3: The Linear Model for Biases</h3>
<p>The core assumption of this model is that a user's deviation from the average rating for a movie can be broken down into two components: the user's personal bias and the movie's inherent bias.</p>
<p><b>Formula:</b></p>
\\[ \tilde{r}_{ij} = u_i + m_j \\]
<ul>
    <li>\\(u_i\\): The bias for user \\(i\\). This captures if a user is generally a harsh critic (negative \\(u_i\\)) or a lenient one (positive \\(u_i\\)).</li>
    <li>\\(m_j\\): The bias for movie \\(j\\). This reflects the movie's overall quality or appeal relative to the average. A popular, highly-rated movie would have a positive \\(m_j\\).</li>
</ul>
<p>For each known rating, we can write an equation. For example, for \\(\tilde{r}_{11}\\):</p>
\\[ 0.875 = u_1 + m_1 \\]

<h3>5. Step 4: Forming a System of Linear Equations</h3>
<p>We can express all the relationships from the previous step as a single matrix equation. This system relates the known bias-removed ratings to the unknown user and movie biases.</p>
<p><b>Matrix Equation:</b></p>
\\[ \tilde{\mathbf{r}} = C \mathbf{\bar{b}} \\]
<p>Where:</p>
<ul>
    <li>\\(\tilde{\mathbf{r}}\\) is a column vector of all 8 known bias-removed ratings.</li>
    <li>\\(\mathbf{\bar{b}}\\) is a column vector of all the unknown biases: \\(u_1, u_2, u_3, m_1, m_2, m_3\\).</li>
    <li>\\(C\\) is a coefficient matrix that maps the biases to the ratings. Each row in \\(C\\) has exactly two '1's, corresponding to the user and movie for that rating.</li>
</ul>
<p>The complete system is:</p>
\\[
\begin{bmatrix}
0.875 \\ 
-0.125 \\ 
-1.125 \\ 
-1.125 \\ 
-0.125 \\ 
-1.125 \\ 
0.875 \\ 
1.875
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
u_1 \\ u_2 \\ u_3 \\ m_1 \\ m_2 \\ m_3
\end{bmatrix}
\\]
<p>This is an <b>overdetermined system</b> because there are more equations (8 ratings) than unknowns (6 biases). This is typical for recommender systems, as the number of ratings is usually much larger than the sum of users and movies.</p>

<h3>6. Step 5: Solving for the Biases using the Pseudo-Inverse</h3>
<p>Since the system is overdetermined, an exact solution that satisfies all equations simultaneously is unlikely. Instead, we find the best-fit solution that minimizes the error (the least-squares solution). This is achieved using the <b>pseudo-inverse</b> of matrix \\(C\\), denoted \\(C^\dagger\\).</p>
<p><b>Formula for the solution:</b></p>
\\[ \mathbf{\bar{b}} = C^\dagger \tilde{\mathbf{r}} \\]
<p>For a tall matrix \\(C\\) with linearly independent columns, the pseudo-inverse is calculated as:</p>
\\[ C^\dagger = (C^T C)^{-1} C^T \\]
<p>Solving this system yields the values for the user and movie biases:</p>
\\[ \mathbf{\bar{b}} = 
\begin{bmatrix}
u_1 \\ u_2 \\ u_3 \\ m_1 \\ m_2 \\ m_3
\end{bmatrix}
=
\begin{bmatrix}
-0.1042 \\
-0.5208 \\
0.5625 \\
-0.4375 \\
0.1458 \\
0.2292
\end{bmatrix}
\\]

<h3>7. Step 6: Predicting the Unknown Rating</h3>
<p>Now that the user and movie biases are known, we can predict the missing rating \\(r_{22}\\). The prediction, \\(\hat{r}_{22}\\), is made by first applying the linear model to find the bias-removed prediction and then adding the overall bias \\(r_a\\) back.</p>
<p><b>Formula:</b></p>
\\[ \hat{r}_{22} = r_a + u_2 + m_2 \\]
<p>Substituting the calculated values:</p>
\\[ \hat{r}_{22} = 3.125 + (-0.5208) + 0.1458 \\]
\\[ \hat{r}_{22} = 2.75 \\]
<p>The predicted rating for User 2 on Movie 2 is <b>2.75</b>.</p>

<h3>Conclusion</h3>
<p>This example demonstrates a foundational method for collaborative filtering. By modeling ratings as a combination of an overall average, user-specific biases, and item-specific biases, we can formulate a system of linear equations. Even when there are more ratings than biases (an overdetermined system), we can find the best-fit solution using the pseudo-inverse to estimate these biases. These estimates are then used to predict ratings for user-item pairs that have not yet occurred.</p>
<p>In a full-scale recommender system, this process would be performed for all unseen movies for a given user. The system would then recommend the movie with the highest predicted rating.</p>
</div></div><h2>Weekly Summary</h2><div>
<p>This week's lectures explore the application of the Linear Minimum Mean Square Error (LMMSE) principle across various domains, demonstrating its power and versatility in estimation and prediction problems. The topics progress from the foundational theory for linear systems to specific applications in wireless communications, time series analysis, and data analytics.</p>

<p><b>1. LMMSE Estimation for Linear Systems</b></p>
<p>This module introduces the application of the LMMSE principle to a general linear input-output model, often encountered in communications and machine learning. The system is modeled as \\(\bar{y} = H\bar{x} + \bar{n}\\), where \\(\bar{x}\\) is the input vector to be estimated, \\(\bar{y}\\) is the observed output, \\(H\\) is the system matrix, and \\(\bar{n}\\) is noise.</p>
<ul>
    <li><b>Key Takeaway:</b> The LMMSE estimate for \\(\bar{x}\\) is derived, resulting in a famous and elegant formula known as the LMMSE (or MMSE) receiver. The estimate incorporates prior statistical knowledge of the signal and noise powers (variances) through the Signal-to-Noise Ratio (SNR).</li>
    <li><b>Key Formula (LMMSE Estimate):</b> The estimate \\(\hat{x}\\) is given by:
    \\[ \hat{x} = \left(H^T H + \frac{1}{\text{SNR}} I\right)^{-1} H^T \bar{y} \\]
    where \\(\text{SNR} = \gamma / \epsilon\\) is the ratio of signal power (\\(\gamma\\)) to noise power (\\(\epsilon\\)). For complex-valued systems, the transpose (\\(H^T\\)) is replaced by the Hermitian transpose (\\(H^H\\)).</li>
    <li><b>Key Formula (Error Covariance):</b> The covariance of the estimation error is also derived, providing a way to quantify the performance of the estimator.</li>
</ul>

<p><b>2. Application: LMMSE for MISO Channel Estimation</b></p>
<p>This module provides a practical application of the LMMSE framework to estimate the wireless channel in a Multiple-Input Single-Output (MISO) communication system. The problem is formulated as a linear model \\(\bar{y} = X\bar{h} + \bar{n}\\), where the goal is to estimate the unknown channel vector \\(\bar{h}\\).</p>
<ul>
    <li><b>Key Takeaway:</b> The LMMSE estimator serves as a regularized version of the classical Least Squares (LS) estimator. At high SNR, the LMMSE estimate converges to the LS estimate, as the data from observations is trusted more. At low SNR, the LMMSE estimate wisely defaults towards the prior mean of the channel (assumed to be zero), as the noisy observations provide little useful information.</li>
    <li><b>Comparison to LS:</b> Unlike the LS estimate, \\(\hat{h}_{LS} = (X^T X)^{-1} X^T \bar{y}\\), the LMMSE estimate avoids issues with ill-conditioned matrices and provides more robust performance, especially in noisy conditions, by leveraging prior statistics of the channel and noise.</li>
</ul>

<p><b>3. Application: Autoregression and Linear Prediction</b></p>
<p>The LMMSE principle is applied to time series analysis for the problem of autoregression (AR), or "self-prediction." The goal is to predict the next value in a time series as a linear combination of its own past values.</p>
<ul>
    <li><b>Key Takeaway:</b> By assuming the time series is Wide-Sense Stationary (WSS), the covariance matrix of the past samples exhibits a special, highly structured form known as a <b>Toeplitz matrix</b> (all elements on a given diagonal are identical). This structure is fundamental to the theory of linear prediction.</li>
    <li><b>AR(1) Model Example:</b> For a simple first-order model \\(\hat{x}_n = \beta x_{n-1}\\), the optimal prediction coefficient is derived using the LMMSE principle as the ratio of the autocorrelation at lag 1 to the autocorrelation at lag 0 (the signal power): \\(\beta = r_{xx}(1) / r_{xx}(0)\\).</li>
</ul>

<p><b>4. Application: Recommender Systems and Matrix Completion</b></p>
<p>The final topic shifts to a modern data analytics problem: building a recommender system (e.g., for movies or products). This is framed as a <b>matrix completion</b> problem, where the goal is to predict unknown user ratings in a very large, sparse user-item rating matrix.</p>
<ul>
    <li><b>Key Takeaway:</b> A simple but powerful model for a rating is introduced, where a rating is a sum of an overall average rating (global bias), a user-specific bias, and an item-specific bias. Determining these unknown biases from the known ratings leads to a large, overdetermined system of linear equations.</li>
    <li><b>Least Squares Solution:</b> This linear system is solved using the method of least squares to find the best-fit values for all user and item biases. Once found, these biases can be used to predict any unknown rating.</li>
    <li><b>Recommendation Process:</b> After predicting the ratings for items a user has not yet seen, the system recommends the item with the highest predicted rating. This demonstrates a direct and practical application of linear algebra and least squares to a core problem in e-commerce and media streaming.</li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<h3>Question 1: Regression Error in a Time-Series</h3>
<p><b>Question:</b> Consider a zero-mean wide sense stationary time-series \\(x(n)\\) with auto-correlation \\(r_{xx}(n) = 0.9^n\\). The regression error for the best prediction of \\(x(n)\\) based on \\(x(n-1)\\) is given as...</p>
<p><b>Explanation:</b> This question asks for the minimum mean squared error (MMSE) when predicting the current value of a time-series, \\(x(n)\\), using only its previous value, \\(x(n-1)\\). This is a first-order auto-regressive (AR(1)) model.</p>
<p>1. <b>The Predictor:</b> The best linear predictor has the form \\(\hat{x}(n) = a \cdot x(n-1)\\). The optimal coefficient \\(a\\) is found using the Yule-Walker equation, which for this simple case is:
\\[ a = \frac{r_{xx}(1)}{r_{xx}(0)} \\]
</p>
<p>2. <b>Calculate Auto-correlation Values:</b> From the given auto-correlation function \\(r_{xx}(n) = 0.9^n\\):
<ul>
    <li>\\(r_{xx}(0) = 0.9^0 = 1\\) (This is the variance of the signal).</li>
    <li>\\(r_{xx}(1) = 0.9^1 = 0.9\\).</li>
</ul>
</p>
<p>3. <b>Calculate the Coefficient:</b>
\\[ a = \frac{0.9}{1} = 0.9 \\]
So the best prediction is \\(\hat{x}(n) = 0.9 \cdot x(n-1)\\).</p>
<p>4. <b>Calculate the Regression Error (MMSE):</b> The formula for the minimum prediction error variance is:
\\[ E_{min} = r_{xx}(0) - a \cdot r_{xx}(1) \\]
Plugging in the values:
\\[ E_{min} = 1 - (0.9)(0.9) = 1 - 0.81 = 0.19 \\]
Thus, the regression error is <b>0.19</b>.</p>

<hr>

<h3>Question 2: Projection Matrix</h3>
<p><b>Question:</b> Consider the matrix A defined as... The projection matrix \\(P_A\\) for the subspace spanned by the columns of A is...</p>
<p><b>Explanation:</b> The formula for the projection matrix \\(P_A\\) onto the column space of a matrix \\(A\\) is:
\\[ P_A = A(A^T A)^{-1} A^T \\]
</p>
<p><i>Note: There appears to be a typo in the question's matrix A. Calculating the projection matrix for the given A does not result in any of the options. However, the accepted answer is the correct projection matrix for a different, related matrix. We will show the calculation for the matrix that correctly produces the answer.</i></p>
<p>Let's assume the columns of \\(A\\) were intended to be the orthogonal vectors \\(v_1 = [1, -1, 0, 0]^T\\) and \\(v_2 = [0, 0, 1, -1]^T\\).
So, let's use the matrix:
\\[ A = \begin{bmatrix} 1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix} \\]
</p>
<p>1. <b>Calculate \\(A^T A\\):</b>
\\[ A^T A = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 0 & 1 & -1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} (1+1) & 0 \\ 0 & (1+1) \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \\]
</p>
<p>2. <b>Calculate \\((A^T A)^{-1}\\):</b>
\\[ (A^T A)^{-1} = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}^{-1} = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/2 \end{bmatrix} = \frac{1}{2}I \\]
</p>
<p>3. <b>Calculate \\(P_A = A(A^T A)^{-1} A^T\\):</b>
\\[ P_A = \begin{bmatrix} 1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix} \left( \frac{1}{2}I \right) \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 0 & 1 & -1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 0 & 1 & -1 \end{bmatrix} \\]
\\[ P_A = \frac{1}{2} \begin{bmatrix} 1 & -1 & 0 & 0 \\ -1 & 1 & 0 & 0 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & -1 & 1 \end{bmatrix} \\]
This matches the accepted answer.</p>

<hr>

<h3>Question 3: MUSIC Algorithm</h3>
<p><b>Question:</b> MUSIC (Multiple Signal Classification) algorithm is used for...</p>
<p><b>Explanation:</b> MUSIC is a high-resolution, subspace-based signal processing algorithm. Its primary and most famous application is in sensor array processing for determining the <b>Direction of Arrival (DOA)</b> of multiple signals impinging on the array. It works by separating the observation space into a "signal subspace" and a "noise subspace" using an eigendecomposition of the data's covariance matrix. It then searches for directions where the array's steering vectors are orthogonal to the noise subspace, which correspond to the DOAs of the incoming signals.</p>

<hr>

<h3>Question 4: Minimum Mean Square Error (MSE) for LMMSE</h3>
<p><b>Question:</b> Consider the vectors \\(\bar{x}, \bar{y}\\) with respective means \\(\bar{\mu}_x, \bar{\mu}_y\\) and covariance matrices \\(R_{xx}, R_{yy}\\). Let the cross-covariance matrix be given as \\(E\{\bar{x}\bar{y}^H\} = R_{xy}\\). The minimum mean square error (MSE) corresponding to the Linear Minimum Mean Square Error (LMMSE) estimate of \\(\bar{x}\\), is given as...</p>
<p><b>Explanation:</b> This question asks for the formula for the total MSE when estimating a random vector \\(\bar{x}\\) from a random vector \\(\bar{y}\\).</p>
<p>1. <b>Error Covariance Matrix:</b> The LMMSE estimate, \\(\hat{\bar{x}}\\), is designed to minimize the mean squared error. The covariance matrix of the estimation error \\(\bar{e} = \bar{x} - \hat{\bar{x}}\\) is given by the formula:
\\[ R_{ee} = R_{xx} - R_{xy}R_{yy}^{-1}R_{yx} \\]
where \\(R_{yx} = R_{xy}^H\\).</p>
<p>2. <b>Total MSE:</b> The total Mean Square Error (MSE) is the sum of the variances of the error in each component of the vector. This is equivalent to the trace of the error covariance matrix.
\\[ \text{MSE} = E\{||\bar{e}||^2\} = \text{Tr}(R_{ee}) \\]
Substituting the expression for \\(R_{ee}\\):
\\[ \text{MSE} = \text{Tr}(R_{xx} - R_{xy}R_{yy}^{-1}R_{yx}) \\]
This matches the accepted answer.</p>

<hr>

<h3>Question 5: Best Prediction in a Time-Series</h3>
<p><b>Question:</b> Consider a zero-mean wide sense stationary time-series \\(x(n)\\) with auto-correlation \\(r_{xx}(n) = 0.75^n\\). The best prediction of \\(x(n)\\) based on \\(x(n-1)\\) is given as...</p>
<p><b>Explanation:</b> This is similar to Question 1, but instead of asking for the error, it asks for the prediction itself. The best linear prediction of \\(x(n)\\) from \\(x(n-1)\\) is \\(\hat{x}(n) = a \cdot x(n-1)\\).</p>
<p>1. <b>Find the coefficient \\(a\\):</b>
\\[ a = \frac{r_{xx}(1)}{r_{xx}(0)} \\]
</p>
<p>2. <b>Calculate Auto-correlation Values:</b>
<ul>
    <li>\\(r_{xx}(0) = 0.75^0 = 1\\)</li>
    <li>\\(r_{xx}(1) = 0.75^1 = 0.75\\)</li>
</ul>
</p>
<p>3. <b>Calculate the Coefficient:</b>
\\[ a = \frac{0.75}{1} = 0.75 \\]
</p>
<p>4. <b>Form the Predictor:</b> The best prediction is therefore:
\\[ \hat{x}(n) = 0.75 \cdot x(n-1) \\]
This matches the accepted answer.</p>

<hr>

<h3>Question 6: LMMSE Estimate Formula</h3>
<p><b>Question:</b> Consider vectors \\(\bar{x}, \bar{y}\\) with respective means \\(\bar{\mu}_x, \bar{\mu}_y\\) and covariance matrices \\(R_{xx}, R_{yy}\\). Let the cross-covariance matrix be given as \\(E\{\bar{x}\bar{y}^H\} = R_{xy}\\). The Linear Minimum Mean Square Error (LMMSE) estimate of \\(\bar{x}\\) is given as...</p>
<p><b>Explanation:</b> This question asks for the general formula for the LMMSE estimator. The estimator is an affine transformation of the observation vector \\(\bar{y}\\), of the form \\(\hat{\bar{x}} = W\bar{y} + \bar{b}\\). The optimal \\(W\\) (the Wiener filter) and \\(\bar{b}\\) are chosen to minimize the MSE. The resulting formula is:
\\[ \hat{\bar{x}} = \bar{\mu}_x + R_{xy}R_{yy}^{-1}(\bar{y} - \bar{\mu}_y) \\]
This formula intuitively adjusts the estimate based on the observed deviation of \\(\bar{y}\\) from its mean \\((\bar{y} - \bar{\mu}_y)\\), scaled by the optimal Wiener filter \\(R_{xy}R_{yy}^{-1}\\), and then adds back the mean of \\(\bar{x}\\).</p>

<hr>

<h3>Question 7: Circulant Channel Matrix</h3>
<p><b>Question:</b> Consider the Inter Symbol Interference (ISI) channel with channel taps \\(h(0) = 1, h(1) = -3, h(2) = -1, h(3) = 2\\). The circulant matrix corresponding to this channel for N = 4 subcarriers is given as...</p>
<p><b>Explanation:</b> In systems like OFDM, a circular convolution between the input signal and the channel response can be represented by multiplication with a circulant matrix. An \\(N \times N\\) circulant matrix is defined by its first column. Each subsequent column is a downward cyclic shift of the previous column.</p>
<p>1. <b>Define the first column:</b> The first column is the vector of channel taps, \\(\bar{h} = [h(0), h(1), h(2), h(3)]^T\\).
\\[ \text{col}_1 = \begin{bmatrix} 1 \\ -3 \\ -1 \\ 2 \end{bmatrix} \\]
</p>
<p>2. <b>Generate subsequent columns:</b>
<ul>
    <li><b>Column 2:</b> Cyclically shift column 1 down. The last element (2) moves to the top.
    \\[ \text{col}_2 = \begin{bmatrix} 2 \\ 1 \\ -3 \\ -1 \end{bmatrix} \\]
    </li>
    <li><b>Column 3:</b> Cyclically shift column 2 down. The last element (-1) moves to the top.
    \\[ \text{col}_3 = \begin{bmatrix} -1 \\ 2 \\ 1 \\ -3 \end{bmatrix} \\]
    </li>
    <li><b>Column 4:</b> Cyclically shift column 3 down. The last element (-3) moves to the top.
    \\[ \text{col}_4 = \begin{bmatrix} -3 \\ -1 \\ 2 \\ 1 \end{bmatrix} \\]
    </li>
</ul>
</p>
<p>3. <b>Assemble the matrix:</b>
\\[ H = \begin{bmatrix} 1 & 2 & -1 & -3 \\ -3 & 1 & 2 & -1 \\ -1 & -3 & 1 & 2 \\ 2 & -1 & -3 & 1 \end{bmatrix} \\]
This matches the accepted answer.</p>

<hr>

<h3>Question 8: Eigenvalue Decomposition</h3>
<p><b>Question:</b> The eigenvalue decomposition of an arbitrary \\(n \times n\\) square matrix A is given as...</p>
<p><b>Explanation:</b> The eigenvalue decomposition factorizes a matrix \\(A\\) into a product of three matrices related to its eigenvectors and eigenvalues. For this decomposition to exist, the matrix \\(A\\) must be diagonalizable (i.e., it must have \\(n\\) linearly independent eigenvectors).</p>
<p>The general form of the decomposition is:
\\[ A = U\Lambda U^{-1} \\]
where:
<ul>
    <li>\\(U\\) is a square matrix whose columns are the eigenvectors of \\(A\\).</li>
    <li>\\(\Lambda\\) is a diagonal matrix whose diagonal entries are the corresponding eigenvalues of \\(A\\).</li>
    <li>\\(U^{-1}\\) is the inverse of the matrix \\(U\\).</li>
</ul>
The other options are special cases: \\(A = U\Lambda U^T\\) is for real symmetric matrices (where \\(U\\) is orthogonal, so \\(U^{-1}=U^T\\)), and \\(A = U\Lambda U^H\\) is for Hermitian matrices (where \\(U\\) is unitary, so \\(U^{-1}=U^H\\)). The most general form for an arbitrary diagonalizable matrix is \\(A = U\Lambda U^{-1}\\).</p>

<hr>

<h3>Question 9: Affine Transformation of a Gaussian Vector</h3>
<p><b>Question:</b> Consider a 2D random vector \\(\bar{x}\\) that has a multi-variate Gaussian distribution with mean \\(\bar{\mu}\\) and covariance \\(\Sigma\\)... Consider the vector \\(\bar{y} = A\bar{x} + \bar{b}\\)... is Gaussian with mean \\(\bar{\mu}_y\\) and covariance matrix \\(\Sigma_y\\).</p>
<p><b>Explanation:</b> An affine transformation of a Gaussian random vector results in another Gaussian random vector. If \\(\bar{x} \sim \mathcal{N}(\bar{\mu}_x, \Sigma_x)\\) and \\(\bar{y} = A\bar{x} + \bar{b}\\), the new mean and covariance are given by:
\\[ \bar{\mu}_y = A\bar{\mu}_x + \bar{b} \\]
\\[ \Sigma_y = A \Sigma_x A^T \\]
</p>
<p>Given:
\\[ \bar{\mu}_x = \begin{bmatrix} -3 \\ 2 \end{bmatrix}, \quad \Sigma_x = \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix}, \quad A = \begin{bmatrix} -2 & 1 \\ -3 & 2 \end{bmatrix}, \quad \bar{b} = \begin{bmatrix} 1 \\ -2 \end{bmatrix} \\]
</p>
<p>1. <b>Calculate the new mean \\(\bar{\mu}_y\\):</b>
\\[ \bar{\mu}_y = \begin{bmatrix} -2 & 1 \\ -3 & 2 \end{bmatrix} \begin{bmatrix} -3 \\ 2 \end{bmatrix} + \begin{bmatrix} 1 \\ -2 \end{bmatrix} = \begin{bmatrix} (-2)(-3) + (1)(2) \\ (-3)(-3) + (2)(2) \end{bmatrix} + \begin{bmatrix} 1 \\ -2 \end{bmatrix} = \begin{bmatrix} 8 \\ 13 \end{bmatrix} + \begin{bmatrix} 1 \\ -2 \end{bmatrix} = \begin{bmatrix} 9 \\ 11 \end{bmatrix} \\]
</p>
<p>2. <b>Calculate the new covariance \\(\Sigma_y\\):</b>
\\[ \Sigma_y = \begin{bmatrix} -2 & 1 \\ -3 & 2 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} -2 & -3 \\ 1 & 2 \end{bmatrix} \\]
First multiply \\(A\Sigma_x\\):
\\[ A\Sigma_x = \begin{bmatrix} -4 & 4 \\ -6 & 8 \end{bmatrix} \\]
Then multiply by \\(A^T\\):
\\[ \Sigma_y = \begin{bmatrix} -4 & 4 \\ -6 & 8 \end{bmatrix} \begin{bmatrix} -2 & -3 \\ 1 & 2 \end{bmatrix} = \begin{bmatrix} (-4)(-2)+(4)(1) & (-4)(-3)+(4)(2) \\ (-6)(-2)+(8)(1) & (-6)(-3)+(8)(2) \end{bmatrix} = \begin{bmatrix} 12 & 20 \\ 20 & 34 \end{bmatrix} \\]
The correct answer is \\(\bar{\mu}_y = [9, 11]^T\\) and \\(\Sigma_y = \begin{bmatrix} 12 & 20 \\ 20 & 34 \end{bmatrix}\\).</p>

<hr>

<h3>Question 10: LMMSE Estimate with SNR</h3>
<p><b>Question:</b> Consider the linear model \\(\bar{y} = H\bar{x} + \bar{n}\\), where \\(E\{\bar{x}\bar{x}^T\} = R_{xx} = \gamma I\\) and noise covariance \\(E\{\bar{n}\bar{n}^T\} = \epsilon I\\). The SNR is \\(\frac{\gamma}{\epsilon}\\). The Linear Minimum Mean Square Error (LMMSE) estimate of \\(\bar{x}\\) for this system becomes...</p>
<p><b>Explanation:</b> We need to find the LMMSE estimate \\(\hat{\bar{x}}\\) for the given linear model. A common form for this estimator, also known as the regularized least squares or Wiener filter solution, is:
\\[ \hat{\bar{x}} = (H^T R_{nn}^{-1} H + R_{xx}^{-1})^{-1} H^T R_{nn}^{-1} \bar{y} \\]
</p>
<p>1. <b>Find the inverse covariance matrices:</b>
<ul>
    <li>\\(R_{xx} = \gamma I \implies R_{xx}^{-1} = \frac{1}{\gamma}I\\)</li>
    <li>\\(R_{nn} = \epsilon I \implies R_{nn}^{-1} = \frac{1}{\epsilon}I\\)</li>
</ul>
</p>
<p>2. <b>Substitute into the formula:</b>
\\[ \hat{\bar{x}} = \left(H^T \left(\frac{1}{\epsilon}I\right) H + \frac{1}{\gamma}I\right)^{-1} H^T \left(\frac{1}{\epsilon}I\right) \bar{y} \\]
\\[ \hat{\bar{x}} = \left(\frac{1}{\epsilon}H^T H + \frac{1}{\gamma}I\right)^{-1} \frac{1}{\epsilon}H^T \bar{y} \\]
</p>
<p>3. <b>Simplify the expression:</b> Factor \\(\frac{1}{\epsilon}\\) out of the term in the inverse.
\\[ \hat{\bar{x}} = \left[ \frac{1}{\epsilon} \left(H^T H + \frac{\epsilon}{\gamma}I\right) \right]^{-1} \frac{1}{\epsilon}H^T \bar{y} \\]
Using the property \\((cA)^{-1} = c^{-1}A^{-1}\\):
\\[ \hat{\bar{x}} = \epsilon \left(H^T H + \frac{\epsilon}{\gamma}I\right)^{-1} \frac{1}{\epsilon}H^T \bar{y} \\]
The \\(\epsilon\\) and \\(\frac{1}{\epsilon}\\) terms cancel out:
\\[ \hat{\bar{x}} = \left(H^T H + \frac{\epsilon}{\gamma}I\right)^{-1} H^T \bar{y} \\]
</p>
<p>4. <b>Substitute SNR:</b> We are given that \\(SNR = \frac{\gamma}{\epsilon}\\), so \\(\frac{\epsilon}{\gamma} = \frac{1}{SNR}\\).
\\[ \hat{\bar{x}} = \left(H^T H + \frac{1}{SNR}I\right)^{-1} H^T \bar{y} \\]
This matches the accepted answer.</p>
</div></div><div class="week" id="week_9"><h1 class="week-title">Week 9</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 42 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 42 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript regarding the application of linear algebra to Fourier analysis, specifically the Fast Fourier Transform (FFT) and its inverse (IFFT).</p>

<b>1. Introduction to FFT and DFT</b>
<p>The transcript introduces the <b>Fast Fourier Transform (FFT)</b> and the <b>Inverse Fast Fourier Transform (IFFT)</b> as fundamental linear operations in signal and system analysis. It clarifies that:</p>
<ul>
    <li><b>FFT</b> is a computationally efficient algorithm for calculating the <b>Discrete Fourier Transform (DFT)</b>.</li>
    <li><b>IFFT</b> is a fast algorithm for calculating the <b>Inverse Discrete Fourier Transform (IDFT)</b>.</li>
</ul>
<p>The primary purpose of these transforms is to convert a signal from its representation in one domain (like time or space) to the frequency domain. This process, known as <b>Fourier analysis</b> or <b>harmonic analysis</b>, provides valuable insights into the frequency content of a signal, such as identifying its high-frequency and low-frequency components.</p>

<b>2. The Discrete Fourier Transform (DFT)</b>
<p>The DFT is defined for a finite sequence of N points. If we have an N-point time-domain sequence denoted by \\(x[n]\\) for \\(n = 0, 1, \dots, N-1\\), its DFT, denoted by \\(X[k]\\), is given by the formula:</p>
\\[ X[k] = \sum_{n=0}^{N-1} x[n] e^{-j \frac{2\pi kn}{N}} \\]
<p>Here:</p>
<ul>
    <li>\\(X[k]\\) is the k-th point in the frequency domain, often called the k-th <b>frequency bin</b>.</li>
    <li>The index \\(k\\) also ranges from 0 to N-1, resulting in an N-point output sequence in the frequency domain.</li>
    <li>The term \\(k/N\\) represents the <b>normalized frequency</b>.</li>
    <li>The collection of all \\(X[k]\\) values forms the <b>frequency spectrum</b> of the signal. The magnitude, \\(|X[k]|\\), is known as the <b>magnitude spectrum</b>.</li>
</ul>

<b>3. The Inverse Discrete Fourier Transform (IDFT)</b>
<p>The IDFT performs the reverse operation, transforming the frequency-domain sequence \\(X[k]\\) back into the time-domain sequence \\(x[n]\\). The formula is:</p>
\\[ x[n] = \frac{1}{N} \sum_{k=0}^{N-1} X[k] e^{j \frac{2\pi kn}{N}} \\]
<p>This transformation recovers the original N-point signal \\(x[0], x[1], \dots, x[N-1]\\). The key differences from the DFT formula are the positive sign in the exponent (\\(e^{j \dots}\\) instead of \\(e^{-j \dots}\\)) and the scaling factor of \\(1/N\\) applied to the entire sum.</p>

<b>4. Matrix Representation of the FFT</b>
<p>The DFT is a linear transformation, which means it can be represented as a matrix-vector multiplication. If we represent the time-domain signal as a vector \\(\mathbf{x} = [x[0], x[1], \dots, x[N-1]]^T\\) and the frequency-domain signal as a vector \\(\mathbf{X} = [X[0], X[1], \dots, X[N-1]]^T\\), the transformation is:</p>
<p>\\(\mathbf{X} = \mathbf{F}_{FFT} \mathbf{x}\\)</p>
<p>The N×N matrix \\(\mathbf{F}_{FFT}\\) is the FFT matrix. Its entries are based on the complex exponential term, which is simplified by defining a fundamental complex number \\(W\\), known as the Nth root of unity:</p>
\\[ W = e^{-j \frac{2\pi}{N}} \\]
<p>Using this, the FFT matrix is constructed as follows, where the entry in the (k+1)-th row and (n+1)-th column is \\(W^{kn}\\):</p>
\\[ \mathbf{F}_{FFT} =
\begin{pmatrix}
1 & 1 & 1 & \dots & 1 \\
1 & W & W^2 & \dots & W^{N-1} \\
1 & W^2 & W^4 & \dots & W^{2(N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & W^{N-1} & W^{2(N-1)} & \dots & W^{(N-1)^2}
\end{pmatrix}
\\]
<p>An important property of this matrix is that it is <b>transpose symmetric</b> (\\(F_{ij} = F_{ji}\\)), but it is <b>not</b> Hermitian symmetric.</p>

<b>Examples of FFT Matrices:</b>
<ul>
    <li>For <b>N = 2</b>: \\(W = e^{-j 2\pi / 2} = e^{-j\pi} = -1\\). The FFT matrix is:
    \\[ \mathbf{F}_{FFT} = \begin{pmatrix} 1 & 1 \\ 1 & W \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \\]
    </li>
    <li>For <b>N = 4</b>: \\(W = e^{-j 2\pi / 4} = e^{-j\pi/2} = -j\\). The powers of W are \\(W^2 = -1, W^3 = j, W^4 = 1\\). The 4x4 FFT matrix is:
    \\[ \mathbf{F}_{FFT} = \begin{pmatrix}
    1 & 1 & 1 & 1 \\
    1 & W & W^2 & W^3 \\
    1 & W^2 & W^4 & W^6 \\
    1 & W^3 & W^6 & W^9
    \end{pmatrix} = \begin{pmatrix}
    1 & 1 & 1 & 1 \\
    1 & -j & -1 & j \\
    1 & -1 & 1 & -1 \\
    1 & j & -1 & -j
    \end{pmatrix} \\]
    </li>
</ul>

<b>5. Matrix Representation of the IFFT</b>
<p>Similarly, the IFFT can be represented by a matrix transformation:</p>
<p>\\(\mathbf{x} = \mathbf{F}_{IFFT} \mathbf{X}\\)</p>
<p>The IFFT matrix, \\(\mathbf{F}_{IFFT}\\), is closely related to the FFT matrix. It is constructed by replacing \\(W\\) with \\(W^{-1} = e^{j \frac{2\pi}{N}}\\) and scaling the entire matrix by \\(1/N\\).</p>
\\[ \mathbf{F}_{IFFT} = \frac{1}{N}
\begin{pmatrix}
1 & 1 & 1 & \dots & 1 \\
1 & W^{-1} & W^{-2} & \dots & W^{-(N-1)} \\
1 & W^{-2} & W^{-4} & \dots & W^{-2(N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & W^{-(N-1)} & W^{-2(N-1)} & \dots & W^{-(N-1)^2}
\end{pmatrix}
\\]

<b>6. Relationship Between FFT and IFFT Matrices</b>
<p>The relationship between the FFT and IFFT matrices is fundamental. Since \\(W^{-1}\\) is the complex conjugate of \\(W\\) (i.e., \\(W^{-1} = W^*\\)), the IFFT matrix can be expressed in terms of the conjugate transpose (Hermitian) of the FFT matrix:</p>
\\[ \mathbf{F}_{IFFT} = \frac{1}{N} \mathbf{F}_{FFT}^* = \frac{1}{N} \mathbf{F}_{FFT}^H \\]
<p>(Note: Since \\(\mathbf{F}_{FFT}\\) is symmetric, its transpose is itself, so its conjugate transpose is the same as its conjugate).</p>
<p>Because the IFFT is the inverse of the FFT, their matrix product yields the identity matrix \\(\mathbf{I}\\):</p>
\\[ \mathbf{F}_{IFFT} \mathbf{F}_{FFT} = \mathbf{I} \\]
<p>Substituting the relationship above, we get:</p>
\\[ \left(\frac{1}{N} \mathbf{F}_{FFT}^H\right) \mathbf{F}_{FFT} = \mathbf{I} \implies \mathbf{F}_{FFT}^H \mathbf{F}_{FFT} = N\mathbf{I} \\]
<p>This property means the FFT matrix is nearly a <b>unitary matrix</b>. A matrix \\(U\\) is unitary if \\(U^H U = \mathbf{I}\\). The FFT matrix differs only by a scaling factor of \\(N\\).</p>

<b>Examples of IFFT Matrices:</b>
<ul>
    <li>For <b>N = 2</b>:
    \\[ \mathbf{F}_{IFFT} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \\]
    Verification:
    \\[ \mathbf{F}_{IFFT} \mathbf{F}_{FFT} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \mathbf{I} \\]
    </li>
    <li>For <b>N = 4</b>:
    \\[ \mathbf{F}_{IFFT} = \frac{1}{4} \begin{pmatrix}
    1 & 1 & 1 & 1 \\
    1 & j & -1 & -j \\
    1 & -1 & 1 & -1 \\
    1 & -j & -1 & j
    \end{pmatrix} \\]
    This matrix is equal to \\(\frac{1}{4}\mathbf{F}_{FFT}^H\\), and multiplying it by \\(\mathbf{F}_{FFT}\\) results in the 4x4 identity matrix.
    </li>
</ul>
</div></div><div class="chapter" id="Lecture 43 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 43 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains the fundamental principles of Orthogonal Frequency Division Multiplexing (OFDM), a crucial technology in modern high-speed wireless communication systems like 4G, 5G, and Wi-Fi. It details the core problem that OFDM solves—Inter-Symbol Interference (ISI)—and introduces the mathematical framework that shows how the IFFT and FFT operations are central to its implementation.</p>

<b>1. Introduction to OFDM and the Problem of High Data Rates</b>
<p>OFDM stands for <b>Orthogonal Frequency Division Multiplexing</b>. It is the dominant signaling strategy (physical layer technology) used in modern wireless systems to achieve ultra-high data rates.</p>
<p>The central challenge in high data rate (HDR) wireless transmission is a phenomenon called <b>Inter-Symbol Interference (ISI)</b>. The relationship can be understood through the following chain of logic:</p>
<ul>
    <li><b>High Data Rate:</b> To send more data in the same amount of time, a wider range of frequencies (high bandwidth) is needed.</li>
    <li><b>High Bandwidth:</b> A consequence of high bandwidth is that the duration of each individual data symbol becomes extremely short (small symbol time).</li>
    <li><b>Small Symbol Time:</b> In a wireless environment, signals travel along multiple paths and arrive at the receiver at slightly different times (a phenomenon called multipath propagation). When the symbol time is very small, the delayed copies of a symbol can overlap with and distort the subsequent symbols. This overlap is known as Inter-Symbol Interference (ISI).</li>
</ul>
<p>ISI causes distortion, making it difficult for the receiver to reliably decode the transmitted symbols, which in turn increases the error rate. OFDM is a clever technique designed to enable high data rate transmission while completely eliminating ISI.</p>

<b>2. The Wireless Channel Model with ISI</b>
<p>To understand how OFDM works, we first need a mathematical model for a channel that causes ISI. The relationship between the transmitted symbols (input) and the received symbols (output) can be described by a linear equation. The received symbol at time \\(m\\), denoted by \\(y_m\\), is a weighted sum of the current input symbol \\(x_m\\) and several past input symbols.</p>
<p>The formula for the channel output is:</p>
\\[ y_m = h_0 x_m + h_1 x_{m-1} + \dots + h_{N-1} x_{m-(N-1)} \\]
<p>Here:</p>
<ul>
    <li>\\(y_m\\) is the output symbol received at time instance \\(m\\).</li>
    <li>\\(x_m, x_{m-1}, \dots\\) are the input symbols transmitted at time \\(m\\), \\(m-1\\), and so on.</li>
    <li>\\(h_0, h_1, \dots, h_{N-1}\\) are the <b>channel taps</b>. These coefficients represent the different signal paths in the wireless channel.</li>
</ul>
<p>The crucial observation here is that \\(y_m\\) depends not only on the desired symbol \\(x_m\\) but also on past symbols (\\(x_{m-1}, x_{m-2}, \dots\\)). This interference from past symbols is the mathematical representation of ISI.</p>

<b>3. The OFDM Principle: From Single-Carrier to Multi-Carrier</b>
<p>OFDM overcomes ISI by fundamentally changing the transmission strategy. Instead of sending symbols one after another in a single high-speed stream (a single-carrier system), OFDM splits the high-speed stream into many parallel, lower-speed streams. Each of these streams is then modulated onto a separate, closely spaced carrier frequency, called a <b>sub-carrier</b>. This is why OFDM is a form of <b>Multi-Carrier Modulation (MCM)</b>.</p>

<p>The implementation of this idea involves the following key steps at the transmitter:</p>
<ol>
    <li><b>Start in the Frequency Domain:</b> Unlike conventional systems, an OFDM transmitter begins with a block of \\(N\\) symbols, \\(X_0, X_1, \dots, X_{N-1}\\), which are considered to be in the frequency domain. Each symbol \\(X_k\\) is intended for one of the \\(N\\) sub-carriers.</li>
    <li><b>Perform an IFFT:</b> An Inverse Fast Fourier Transform (IFFT) is performed on this block of frequency-domain symbols to generate a block of \\(N\\) time-domain samples, \\(x_0, x_1, \dots, x_{n-1}\\). This is a unique aspect of OFDM, where the IFFT is used at the transmitter and the FFT is used at the receiver.</li>
    <li><b>Add a Cyclic Prefix (CP):</b> This is the critical step that eliminates ISI. Before transmitting the time-domain samples, a copy of the last few samples from the end of the block is prepended to the beginning of the block. This prefix is called the <b>Cyclic Prefix (CP)</b>. This makes the transmitted block appear as if it is one period of a periodic sequence.</li>
</ol>

<b>4. The Role of the Cyclic Prefix and Circular Convolution</b>
<p>The addition of the Cyclic Prefix transforms the effect of the channel. A standard channel performs a <i>linear convolution</i> on the input signal. However, because the transmitted block with the CP looks periodic, the linear convolution performed by the channel effectively becomes a <b>circular convolution</b> from the receiver's perspective (after the CP is removed).</p>
<p>Let's examine the received samples. For \\(y_0\\), the input symbols are \\(x_0, x_{n-1}, x_{n-2}, \dots, x_1\\). The symbols \\(x_{n-1}, x_{n-2}, \dots\\) are the "past" symbols, but due to the CP, they are simply the last symbols of the <i>current</i> block that have been wrapped around.</p>
<p>The first two received samples can be written as:</p>
\\[ y_0 = h_0 x_0 + h_1 x_{n-1} + h_2 x_{n-2} + \dots + h_{n-1} x_1 + w_0 \\]
\\[ y_1 = h_0 x_1 + h_1 x_0 + h_2 x_{n-1} + \dots + h_{n-1} x_2 + w_1 \\]
<p>Observing the pattern, we can see that the sequence of input symbols \\((x_0, x_1, \dots, x_{n-1})\\) used to calculate \\(y_1\\) is a <b>circular shift</b> of the sequence used to calculate \\(y_0\\). This circular property is a direct result of the Cyclic Prefix and is the key to simplifying the channel's effect.</p>

<b>5. Matrix Representation and the Circulant Matrix</b>
<p>This system of linear equations describing the circular convolution can be elegantly represented in matrix form:</p>
\\[ \mathbf{y} = \mathbf{H}_c \mathbf{x} + \mathbf{w} \\]
<p>where:</p>
<ul>
    <li>\\(\mathbf{y} = [y_0, y_1, \dots, y_{n-1}]^T\\) is the vector of received time-domain samples.</li>
    <li>\\(\mathbf{x} = [x_0, x_1, \dots, x_{n-1}]^T\\) is the vector of transmitted time-domain samples (after IFFT).</li>
    <li>\\(\mathbf{w} = [w_0, w_1, \dots, w_{n-1}]^T\\) is the noise vector.</li>
    <li>\\(\mathbf{H}_c\\) is the \\(N \times N\\) channel matrix.</li>
</ul>
<p>Due to the circular shift property induced by the Cyclic Prefix, the matrix \\(\mathbf{H}_c\\) has a very special structure. It is a <b>circulant matrix</b>.</p>
\\[
\mathbf{H}_c = \begin{pmatrix}
h_0 & h_{n-1} & h_{n-2} & \dots & h_1 \\
h_1 & h_0 & h_{n-1} & \dots & h_2 \\
h_2 & h_1 & h_0 & \dots & h_3 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
h_{n-1} & h_{n-2} & h_{n-3} & \dots & h_0
\end{pmatrix}
\\]
<p>A circulant matrix is defined by its first row (or column). Each subsequent row is a circular right-shift of the row above it. This structure is not a coincidence; it is a direct mathematical consequence of using a Cyclic Prefix to turn a linear convolution problem into a circular one.</p>
<p>This "cyclic" or "periodic" nature of the channel matrix is a profound result. It strongly hints that Fourier analysis (specifically the FFT and IFFT) will be instrumental in diagonalizing this matrix, which simplifies the equalization process at the receiver and ultimately allows for the recovery of the original data symbols without any ISI.</p>
</div></div><div class="chapter" id="Lecture 44 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 44 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This transcript explains the mathematical properties of the channel matrix in an Orthogonal Frequency-Division Multiplexing (OFDM) system, particularly when dealing with an Inter-Symbol Interference (ISI) channel. The core concept is that the use of a cyclic prefix in OFDM transforms the channel's effect into a circular convolution, which is represented by a special type of matrix called a circulant matrix. The key insight is that this circulant matrix has a very elegant structure that can be diagonalized by the Fourier transform matrices (FFT and IFFT), which simplifies the analysis of the entire system.</p>

<b>1. The Channel Matrix in OFDM</b>
<p>In an OFDM system operating over a channel with ISI, the channel is characterized by a set of discrete-time channel taps, denoted as \\(h_0, h_1, \dots, h_{N-1}\\), where \\(N\\) is the number of subcarriers. After the cyclic prefix is removed at the receiver, the linear convolution of the transmitted signal with the channel response becomes a circular convolution. This relationship can be expressed in matrix form, \\(\mathbf{y} = \mathbf{H}_c \mathbf{x}\\), where the channel matrix \\(\mathbf{H}_c\\) has a specific structure.</p>

<p>This matrix, \\(\mathbf{H}_c\\), is a <b>circulant matrix</b>. Its structure is defined as follows:</p>
\\[
\mathbf{H}_c =
\begin{pmatrix}
h_0 & h_{N-1} & h_{N-2} & \dots & h_1 \\
h_1 & h_0 & h_{N-1} & \dots & h_2 \\
h_2 & h_1 & h_0 & \dots & h_3 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
h_{N-1} & h_{N-2} & h_{N-3} & \dots & h_0
\end{pmatrix}
\\]
<p>The defining characteristic of a circulant matrix is that each row is a circular shift of the row above it. Equivalently, each column is a circular shift of the column to its left. The first column of this matrix consists of the channel taps in order.</p>

<b>2. Key Property: Eigenvectors and Eigenvalues of a Circulant Matrix</b>
<p>The most important property of a circulant matrix, and the central point of the lecture, is related to its eigenvalue decomposition. The transcript explains that any circulant matrix has a special set of eigenvectors and eigenvalues.</p>
<ul>
    <li><b>Eigenvectors:</b> The eigenvectors of the circulant channel matrix \\(\mathbf{H}_c\\) are the columns of the Inverse Fast Fourier Transform (IFFT) matrix.</li>
    <li><b>Eigenvalues:</b> The corresponding eigenvalues are the Discrete Fourier Transform (DFT) coefficients of the first column of the matrix (which are the channel taps \\(h_0, \dots, h_{N-1}\\)).</li>
</ul>

<p>The \\(k^{th}\\) eigenvector, denoted as \\(\bar{\mathbf{f}}_k\\), is given by:</p>
\\[
\bar{\mathbf{f}}_k = \frac{1}{\sqrt{N}} \begin{pmatrix} 1 \\ W^{-k} \\ W^{-2k} \\ \vdots \\ W^{-(N-1)k} \end{pmatrix}
\\]
<p><i>(Note: The transcript uses a normalization factor of \\(1/N\\), but \\(1/\sqrt{N}\\) is more standard for a unitary DFT matrix. The core result remains the same.)</i></p>
<p>Here, \\(W\\) is the twiddle factor used in DFT/FFT calculations, defined as:</p>
\\[ W = e^{-j\frac{2\pi}{N}} \\]
<p>These vectors \\(\bar{\mathbf{f}}_k\\) are precisely the columns of the IFFT matrix.</p>

<b>3. Proof of the Eigenvector-Eigenvalue Relationship</b>
<p>The transcript provides a proof to demonstrate that \\(\bar{\mathbf{f}}_k\\) is an eigenvector of \\(\mathbf{H}_c\\). The proof involves showing that the product \\(\mathbf{H}_c \bar{\mathbf{f}}_k\\) is a scaled version of \\(\bar{\mathbf{f}}_k\\). This is done by analyzing the \\(r^{th}\\) element of the resulting vector.</p>

<p><b>Step 1: Define the \\(r^{th}\\) row of \\(\mathbf{H}_c\\)</b><br>
The element in the \\(r^{th}\\) row and \\(n^{th}\\) column of \\(\mathbf{H}_c\\) can be written as \\(h_{(r-n) \pmod N}\\). So, the \\(r^{th}\\) element of the product \\(\mathbf{H}_c \bar{\mathbf{f}}_k\\) is the inner product of the \\(r^{th}\\) row of \\(\mathbf{H}_c\\) and the vector \\(\bar{\mathbf{f}}_k\\).</p>
\\[
[\mathbf{H}_c \bar{\mathbf{f}}_k]_r = \frac{1}{N} \sum_{n=0}^{N-1} h_{(r-n) \pmod N} W^{-nk}
\\]

<p><b>Step 2: Change of Variables</b><br>
A change of variables is introduced in the summation: let \\(m = (r-n) \pmod N\\). As \\(n\\) goes from \\(0\\) to \\(N-1\\), \\(m\\) also covers the same range, just in a different order. From this substitution, we get \\(n = (r-m) \pmod N\\).</p>
\\[
[\mathbf{H}_c \bar{\mathbf{f}}_k]_r = \frac{1}{N} \sum_{m=0}^{N-1} h_m W^{-(r-m)k}
\\]

<p><b>Step 3: Rearrange the Expression</b><br>
The exponential term can be split:</p>
\\[
[\mathbf{H}_c \bar{\mathbf{f}}_k]_r = \frac{1}{N} \left( \sum_{m=0}^{N-1} h_m W^{mk} \right) W^{-rk}
\\]
<p>Let's look at the term in the parenthesis. Substituting \\(W = e^{-j2\pi/N}\\), we get:</p>
\\[
\sum_{m=0}^{N-1} h_m W^{mk} = \sum_{m=0}^{N-1} h_m (e^{-j\frac{2\pi}{N}})^{-mk} = \sum_{m=0}^{N-1} h_m e^{-j\frac{2\pi mk}{N}}
\\]
<p>This expression is, by definition, the \\(k^{th}\\) DFT coefficient of the sequence of channel taps \\(h_0, \dots, h_{N-1}\\). Let's call this \\(H(k)\\).</p>

<p><b>Step 4: Final Result</b><br>
Substituting this back, the \\(r^{th}\\) element of the product vector becomes:</p>
\\[
[\mathbf{H}_c \bar{\mathbf{f}}_k]_r = \frac{1}{N} H(k) W^{-rk} = H(k) \cdot \left( \frac{1}{N} W^{-rk} \right)
\\]
<p>The term \\(\frac{1}{N} W^{-rk}\\) is precisely the \\(r^{th}\\) element of the original vector \\(\bar{\mathbf{f}}_k\\). Therefore, for the entire vector, we have:</p>
\\[
\mathbf{H}_c \bar{\mathbf{f}}_k = H(k) \bar{\mathbf{f}}_k
\\]
<p>This is the standard form of the eigenvalue equation, \\(\mathbf{A}\mathbf{x} = \lambda\mathbf{x}\\), which proves that:</p>
<ul>
    <li>\\(\bar{\mathbf{f}}_k\\) is an eigenvector of \\(\mathbf{H}_c\\).</li>
    <li>\\(H(k)\\), the \\(k^{th}\\) DFT coefficient of the channel taps, is the corresponding eigenvalue.</li>
</ul>
<p>The \\(k^{th}\\) eigenvalue, \\(\lambda_k\\), is given by:</p>
\\[
\lambda_k = H(k) = \sum_{n=0}^{N-1} h_n e^{-j\frac{2\pi nk}{N}}
\\]

<b>4. Eigenvalue Decomposition (EVD) of the Channel Matrix</b>
<p>The eigenvalue decomposition of a square matrix \\(\mathbf{A}\\) is given by \\(\mathbf{A} = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^{-1}\\), where \\(\mathbf{V}\\) is the matrix whose columns are the eigenvectors of \\(\mathbf{A}\\), and \\(\mathbf{\Lambda}\\) is a diagonal matrix containing the corresponding eigenvalues.</p>

<p>For the circulant channel matrix \\(\mathbf{H}_c\\):</p>
<ul>
    <li>The <b>eigenvector matrix</b> is the matrix whose columns are \\(\bar{\mathbf{f}}_k\\) for \\(k=0, \dots, N-1\\). This is exactly the <b>IFFT matrix</b>, denoted \\(\mathbf{F}_{IFFT}\\) or \\(\mathbf{F}^H\\).</li>
    <li>The <b>eigenvalue matrix</b> \\(\mathbf{\Lambda}\\) is a diagonal matrix where the diagonal elements are the eigenvalues \\(H(0), H(1), \dots, H(N-1)\\).</li>
    <li>The inverse of the eigenvector matrix \\((\mathbf{F}_{IFFT})^{-1}\\) is the <b>FFT matrix</b>, denoted \\(\mathbf{F}_{FFT}\\) or \\(\mathbf{F}\\).</li>
</ul>

<p>Therefore, the eigenvalue decomposition of \\(\mathbf{H}_c\\) is:</p>
\\[
\mathbf{H}_c = \mathbf{F}_{IFFT} \mathbf{\Lambda} \mathbf{F}_{FFT}
\\]
<p>where \\(\mathbf{\Lambda}\\) is:</p>
\\[
\mathbf{\Lambda} =
\begin{pmatrix}
H(0) & 0 & \dots & 0 \\
0 & H(1) & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & H(N-1)
\end{pmatrix}
\\]

<p>This decomposition is the central result. It shows that the complex operation of a circulant channel matrix can be broken down into three simpler steps: an FFT, a simple element-wise multiplication (by the channel's frequency response), and an IFFT. This is the fundamental reason why equalization is so simple in OFDM systems.</p>
</div></div><div class="chapter" id="Lecture 45 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 45 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on how linear algebra is used to model and analyze an Orthogonal Frequency Division Multiplexing (OFDM) communication system.</p>

<b>1. The Linear System Model for OFDM</b>
<p>The starting point is a linear algebraic model that describes the relationship between the transmitted signal, the communication channel, and the received signal. The system is represented by the equation:</p>
\\[ \bar{y} = H_c \bar{x} + \bar{w} \\]
<ul>
    <li>\\( \bar{x} \\) is the input vector of transmitted time-domain samples: \\( \bar{x} = [x_0, x_1, \dots, x_{N-1}]^T \\).</li>
    <li>\\( \bar{y} \\) is the output vector of received time-domain samples: \\( \bar{y} = [y_0, y_1, \dots, y_{N-1}]^T \\).</li>
    <li>\\( \bar{w} \\) is the additive noise vector: \\( \bar{w} = [w_0, w_1, \dots, w_{N-1}]^T \\), which is an inherent part of any communication system.</li>
    <li>\\( H_c \\) is a special square matrix that represents the communication channel. In OFDM systems, due to the use of a cyclic prefix (a concept that makes the channel behave this way), this matrix has a <b>circulant</b> structure. A circulant matrix is one where each row is a cyclic shift of the row above it, and it is completely defined by its first row, which consists of the channel's impulse response taps.</li>
</ul>

<b>2. Eigenvalue Decomposition of the Circulant Channel Matrix</b>
<p>A key insight from linear algebra is that any circulant matrix can be diagonalized using the Discrete Fourier Transform (DFT) matrices. This is known as its eigenvalue decomposition. The matrix \\( H_c \\) can be expressed as:</p>
\\[ H_c = F_{IFFT} \Lambda F_{FFT} \\]
<ul>
    <li>\\( F_{FFT} \\) is the Fast Fourier Transform (FFT) matrix. It is a matrix that performs the DFT operation.</li>
    <li>\\( F_{IFFT} \\) is the Inverse Fast Fourier Transform (IFFT) matrix. The columns of this matrix are the eigenvectors of \\( H_c \\).</li>
    <li>\\( \Lambda \\) is a <b>diagonal matrix</b> whose diagonal entries are the eigenvalues of \\( H_c \\).</li>
</ul>
<p>Crucially, the eigenvalues of the circulant channel matrix \\( H_c \\) are simply the DFT coefficients of the channel's impulse response (the channel taps \\( h_n \\)). The k-th eigenvalue, denoted as \\( H_k \\), is given by:</p>
\\[ H(k) = \sum_{n=0}^{N-1} h_n e^{-j \frac{2\pi kn}{N}} \\]
<p>This decomposition is the mathematical foundation that makes OFDM work efficiently.</p>

<b>3. System Analysis: Combining the Model and Decomposition</b>
<p>By substituting the eigenvalue decomposition of \\( H_c \\) into the system model, we can begin to simplify and understand the system's behavior.</p>

<p><b>Step 1: Receiver Processing (Post-processing)</b></p>
<p>In an OFDM receiver, the first step is to perform an FFT on the received time-domain samples \\( \bar{y} \\). In matrix form, this is equivalent to pre-multiplying the system equation by the \\( F_{FFT} \\) matrix:</p>
\\[ F_{FFT} \bar{y} = F_{FFT} (F_{IFFT} \Lambda F_{FFT} \bar{x} + \bar{w}) \\]
<p>Using the distributive property, this becomes:</p>
\\[ F_{FFT} \bar{y} = (F_{FFT} F_{IFFT}) \Lambda F_{FFT} \bar{x} + F_{FFT} \bar{w} \\]
<p>A fundamental property of the FFT and IFFT matrices is that they are inverses of each other, meaning their product is the identity matrix (\\( I \\)):</p>
\\[ F_{FFT} \cdot F_{IFFT} = I \\]
<p>Applying this property simplifies the equation to:</p>
\\[ \bar{Y} = \Lambda (F_{FFT} \bar{x}) + \bar{W} \\]
<p>Here, we introduce new notation for the frequency-domain signals after the FFT operation: \\( \bar{Y} = F_{FFT} \bar{y} \\) and \\( \bar{W} = F_{FFT} \bar{w} \\).</p>

<p><b>Step 2: Transmitter Processing (Pre-processing)</b></p>
<p>The signal that is actually transmitted, \\( \bar{x} \\), is not the original data. In OFDM, the original data symbols (which exist in the frequency domain, denoted by \\( \bar{X} \\)) are first processed by an IFFT before transmission. This is the pre-processing step.</p>
\\[ \bar{x} = F_{IFFT} \bar{X} \\]
<p>Here, \\( \bar{X} = [X_0, X_1, \dots, X_{N-1}]^T \\) is the vector of the actual communication symbols (e.g., QPSK or QAM symbols) we want to send.</p>

<b>4. The "Wonderful" Decoupled OFDM Model</b>
<p>Now we substitute the expression for the pre-processed signal \\( \bar{x} \\) into our post-processed system equation:</p>
\\[ \bar{Y} = \Lambda F_{FFT} (F_{IFFT} \bar{X}) + \bar{W} \\]
<p>Again, using the property that \\( F_{FFT} \cdot F_{IFFT} = I \\), the equation simplifies dramatically:</p>
\\[ \bar{Y} = \Lambda \bar{X} + \bar{W} \\]
<p>This is the final, simplified model for an OFDM system. Its "wonderful" nature comes from the fact that \\( \Lambda \\) is a diagonal matrix. Let's expand this equation:</p>
\\[
\begin{pmatrix} Y_0 \\ Y_1 \\ \vdots \\ Y_{N-1} \end{pmatrix} = 
\begin{pmatrix} H_0 & 0 & \cdots & 0 \\ 0 & H_1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & H_{N-1} \end{pmatrix} 
\begin{pmatrix} X_0 \\ X_1 \\ \vdots \\ X_{N-1} \end{pmatrix} + 
\begin{pmatrix} W_0 \\ W_1 \\ \vdots \\ W_{N-1} \end{pmatrix}
\\]
<p>Because the channel matrix \\( \Lambda \\) is diagonal, the system is broken down into a set of N independent, parallel scalar equations:</p>
\\[ Y_k = H_k X_k + W_k \quad \text{for } k = 0, 1, \dots, N-1 \\]

<b>5. The Benefits of Decoupling</b>
<ul>
    <li><b>Transformation of the Channel:</b> OFDM effectively converts a complex, wideband channel that causes inter-symbol interference (ISI) into \\( N \\) parallel, independent, and flat (non-dispersive) narrowband channels. Each of these narrowband channels is called a <b>subcarrier</b>.</li>
    <li><b>Elimination of Interference:</b> In the final model, the received symbol on subcarrier \\( k \\) (\\( Y_k \\)) depends only on the transmitted symbol on that same subcarrier (\\( X_k \\)). There is no interference from symbols on other subcarriers (e.g., \\( X_{k-1} \\) or \\( X_{k+1} \\)). This completely eliminates ISI.</li>
    <li><b>Low-Complexity Decoding (Equalization):</b> With the interference gone, recovering the transmitted symbol \\( X_k \\) becomes trivial. The receiver simply divides the received symbol \\( Y_k \\) by the channel coefficient for that subcarrier, \\( H_k \\) (which is assumed to be known). This process is called equalization.</li>
</ul>
<p>The estimate of the transmitted symbol, \\( \hat{X}_k \\), is found by:</p>
\\[ \hat{X}_k = \frac{Y_k}{H_k} = \frac{H_k X_k + W_k}{H_k} = X_k + \frac{W_k}{H_k} \\]
<p>This simple division is known as a <b>single-tap equalizer</b>, which is vastly simpler than the complex equalizers required for single-carrier systems suffering from ISI.</p>

<p>In summary, the application of linear algebra—specifically the eigenvalue decomposition of circulant matrices via FFT/IFFT operations—provides a powerful framework to model and understand how OFDM transforms a difficult communication problem into a set of much simpler, parallel problems. This transformation is what enables the high-speed data transmission we rely on in modern wireless technologies like 4G, 5G, and Wi-Fi.</p>
</div></div><div class="chapter" id="Lecture 46 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 46 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts, theories, and formulas related to <b>Single Carrier Frequency Division Multiple Access (SC-FDMA)</b>, as presented in the transcript. The focus is on its mathematical modeling using linear algebra and its comparison to Orthogonal Frequency Division Multiplexing (OFDM).</p>

<h3>1. Introduction to SC-FDMA</h3>
<p>SC-FDMA, which stands for <b>Single Carrier Frequency Division Multiple Access</b>, is a crucial transmission technology used in modern wireless communication systems, particularly 4G (LTE) and 5G. While it shares some principles with OFDM, it has a distinct application and structure.</p>
<ul>
    <li><b>OFDM:</b> Primarily used in the <b>downlink</b> (from the base station to the mobile device).</li>
    <li><b>SC-FDMA:</b> Primarily used in the <b>uplink</b> (from the mobile device to the base station).</li>
</ul>
<p>The reason for this distinction lies in a key limitation of OFDM known as the Peak-to-Average Power Ratio (PAPR).</p>

<h3>2. The Peak-to-Average Power Ratio (PAPR) Problem in OFDM</h3>
<p>OFDM signals are generated by performing an Inverse Fast Fourier Transform (IFFT) on the data symbols. This process is equivalent to summing up a large number of sinusoidal signals (subcarriers). When these sinusoids align in phase, they can produce a very high peak amplitude relative to the average signal power. This high ratio is called PAPR.</p>
<p>A high PAPR requires the power amplifier in the transmitter to have a large linear operating range to avoid signal distortion. Such amplifiers are inefficient and consume significant power. While this is manageable for a base station (downlink), it is highly undesirable for a power-constrained mobile device (uplink).</p>
<p>SC-FDMA is designed to overcome this issue. Since it is a <b>single-carrier</b> technique, it does not involve an IFFT at the transmitter, resulting in a much lower PAPR. This makes it more power-efficient and suitable for the uplink.</p>

<h3>3. SC-FDMA System Model</h3>
<p>Unlike OFDM where IFFT samples are transmitted, in SC-FDMA, the modulated data symbols themselves are transmitted directly in the time domain.</p>

<p><b>A. Transmission Process</b></p>
<ol>
    <li>A block of \\(N\\) modulated symbols \\(x_0, x_1, \dots, x_{N-1}\\) is formed. These are the actual data symbols (e.g., QPSK symbols), not IFFT samples.</li>
    <li>A <b>Cyclic Prefix (CP)</b> is added to this block, similar to OFDM. The purpose of the CP is to transform the effect of the channel from a linear convolution into a <b>circular convolution</b>. This mathematical property is essential for enabling simple equalization at the receiver.</li>
</ol>

<p><b>B. Channel Model and Circular Convolution</b></p>
<p>After the signal passes through the wireless channel, the received signal at each time instance \\(m\\) can be modeled as the circular convolution of the transmitted symbols \\(x_m\\) and the channel's impulse response \\(h_m\\), plus additive noise \\(w_m\\).</p>
\\[ y_m = (h \circledast x)_m + w_m \\]
<p>Here, \\(\circledast\\) denotes circular convolution. The channel is characterized by \\(N\\) channel taps, \\(h_0, h_1, \dots, h_{N-1}\\), which represent inter-symbol interference (ISI).</p>

<p><b>C. Linear Algebra Representation</b></p>
<p>The entire block of received signals can be represented in a compact vector-matrix form:</p>
\\[ \mathbf{y} = \mathbf{H}_c \mathbf{x} + \mathbf{w} \\]
where:
<ul>
    <li>\\(\mathbf{y} = [y_0, y_1, \dots, y_{N-1}]^T\\) is the \\(N \times 1\\) received signal vector.</li>
    <li>\\(\mathbf{x} = [x_0, x_1, \dots, x_{N-1}]^T\\) is the \\(N \times 1\\) transmitted symbol vector.</li>
    <li>\\(\mathbf{w} = [w_0, w_1, \dots, w_{N-1}]^T\\) is the \\(N \times 1\\) additive noise vector.</li>
    <li>\\(\mathbf{H}_c\\) is the \\(N \times N\\) <b>circulant channel matrix</b> that represents the circular convolution.</li>
</ul>

<p>The circulant matrix \\(\mathbf{H}_c\\) has a special structure where each column is a circularly shifted version of the preceding column. Its first column is formed by the channel taps:</p>
\\[
\mathbf{H}_c =
\begin{pmatrix}
h_0 & h_{N-1} & \cdots & h_1 \\
h_1 & h_0 & \cdots & h_2 \\
\vdots & \vdots & \ddots & \vdots \\
h_{N-1} & h_{N-2} & \cdots & h_0
\end{pmatrix}
\\]

<h3>4. Receiver Processing: Frequency Domain Equalization (FDE)</h3>
<p>The brilliance of this model is that circulant matrices can be diagonalized by the Fourier Transform matrices. This allows for a very efficient equalization process in the frequency domain.</p>

<p><b>A. Eigenvalue Decomposition of the Circulant Matrix</b></p>
<p>The circulant matrix \\(\mathbf{H}_c\\) can be expressed using its eigenvalue decomposition:</p>
\\[ \mathbf{H}_c = \mathbf{F}_{IFFT} \mathbf{\Lambda} \mathbf{F}_{FFT} \\]
where:
<ul>
    <li>\\(\mathbf{F}_{FFT}\\) is the \\(N \times N\\) Fast Fourier Transform (FFT) matrix.</li>
    <li>\\(\mathbf{F}_{IFFT}\\) is the \\(N \times N\\) Inverse Fast Fourier Transform (IFFT) matrix. It is the inverse of \\(\mathbf{F}_{FFT}\\).</li>
    <li>\\(\mathbf{\Lambda}\\) is a diagonal matrix containing the eigenvalues of \\(\mathbf{H}_c\\). These eigenvalues are precisely the DFT coefficients of the channel taps \\(h_n\\).</li>
</ul>
<p>The diagonal elements of \\(\mathbf{\Lambda}\\) are \\(H_0, H_1, \dots, H_{N-1}\\), where \\(H_k\\) is the \\(k\\)-th DFT coefficient of the channel:</p>
\\[ H_k = \sum_{n=0}^{N-1} h_n e^{-j \frac{2\pi nk}{N}} \\]
The matrix \\(\mathbf{\Lambda}\\) looks like this:
\\[
\mathbf{\Lambda} =
\begin{pmatrix}
H_0 & 0 & \cdots & 0 \\
0 & H_1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & H_{N-1}
\end{pmatrix}
\\]

<p><b>B. Step-by-Step Receiver Operations</b></p>
<p>The receiver performs a three-step process to recover the transmitted symbols \\(\mathbf{x}\\):</p>
<p><b>Step 1: Apply FFT</b><br>
Substitute the decomposition of \\(\mathbf{H}_c\\) into the system equation:</p>
\\[ \mathbf{y} = (\mathbf{F}_{IFFT} \mathbf{\Lambda} \mathbf{F}_{FFT}) \mathbf{x} + \mathbf{w} \\]
<p>The receiver first transforms the received signal \\(\mathbf{y}\\) to the frequency domain by applying an FFT (multiplying by \\(\mathbf{F}_{FFT}\\)):</p>
\\[ \mathbf{F}_{FFT} \mathbf{y} = \mathbf{F}_{FFT} (\mathbf{F}_{IFFT} \mathbf{\Lambda} \mathbf{F}_{FFT} \mathbf{x} + \mathbf{w}) \\]
<p>Using the property that \\(\mathbf{F}_{FFT} \mathbf{F}_{IFFT} = \mathbf{I}\\) (the identity matrix), we get:</p>
\\[ \mathbf{Y} = \mathbf{\Lambda} (\mathbf{F}_{FFT} \mathbf{x}) + \mathbf{W} \\]
<p>where \\(\mathbf{Y} = \mathbf{F}_{FFT} \mathbf{y}\\) is the received signal in the frequency domain, and \\(\mathbf{W} = \mathbf{F}_{FFT} \mathbf{w}\\) is the noise in the frequency domain.</p>

<p><b>Step 2: Single-Tap Equalization</b><br>
The effect of the channel is now represented by the simple diagonal matrix \\(\mathbf{\Lambda}\\). To remove this effect (i.e., equalize the channel), we multiply by its inverse, \\(\mathbf{\Lambda}^{-1}\\). Since \\(\mathbf{\Lambda}\\) is diagonal, its inverse is also a diagonal matrix with elements \\(1/H_k\\).</p>
\\[ \mathbf{\Lambda}^{-1} \mathbf{Y} = \mathbf{\Lambda}^{-1} \mathbf{\Lambda} (\mathbf{F}_{FFT} \mathbf{x}) + \mathbf{\Lambda}^{-1} \mathbf{W} \\]
\\[ \mathbf{\Lambda}^{-1} \mathbf{Y} = \mathbf{F}_{FFT} \mathbf{x} + \mathbf{\Lambda}^{-1} \mathbf{W} \\]
<p>This operation is known as a <b>single-tap equalizer</b> because for each subcarrier \\(k\\), the equalization is a simple complex division: \\(Y_k / H_k\\).</p>

<p><b>Step 3: Apply IFFT</b><br>
The result from the previous step is \\(\mathbf{F}_{FFT} \mathbf{x}\\), which is the DFT of the original symbols. To recover the symbols \\(\mathbf{x}\\), we apply an IFFT (multiply by \\(\mathbf{F}_{IFFT}\\)):</p>
\\[ \mathbf{F}_{IFFT} (\mathbf{\Lambda}^{-1} \mathbf{Y}) = \mathbf{F}_{IFFT}(\mathbf{F}_{FFT} \mathbf{x} + \mathbf{\Lambda}^{-1} \mathbf{W}) \\]
\\[ \tilde{\mathbf{y}} = (\mathbf{F}_{IFFT} \mathbf{F}_{FFT}) \mathbf{x} + \mathbf{F}_{IFFT} \mathbf{\Lambda}^{-1} \mathbf{W} \\]
\\[ \tilde{\mathbf{y}} = \mathbf{x} + \tilde{\mathbf{w}} \\]
<p>where \\(\tilde{\mathbf{y}}\\) is the final estimated symbol vector and \\(\tilde{\mathbf{w}}\\) is the processed noise vector. This final equation shows that we have successfully recovered the transmitted symbols, corrupted only by noise.</p>
<p>In component form, the model becomes decoupled:</p>
\\[ \tilde{y}_i = x_i + \tilde{w}_i \quad \text{for } i = 0, 1, \dots, N-1 \\]

<h3>5. Conclusion and Key Advantages of SC-FDMA</h3>
<p>SC-FDMA cleverly combines the benefits of single-carrier and multi-carrier systems.</p>
<ul>
    <li><b>Low PAPR:</b> By avoiding the IFFT at the transmitter, it maintains the low PAPR characteristics of a single-carrier system, making it ideal for the uplink.</li>
    <li><b>Low Complexity Equalization:</b> By using a cyclic prefix and performing equalization in the frequency domain (FFT -> single-tap division -> IFFT), it avoids the high computational complexity of traditional time-domain equalizers (which require \\(O(N^3)\\) matrix inversion). The FDE process has a complexity of \\(O(N \log N)\\), which is highly efficient.</li>
</ul>
<p>The entire SC-FDMA system, from transmission to reception, can be elegantly modeled and analyzed using linear algebra. The concepts of circulant matrices and their eigenvalue decomposition are central to understanding why frequency domain equalization works and provides a powerful, compact framework for describing these advanced communication systems.</p>
</div></div><div class="chapter" id="Lecture 47 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 47 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the lecture on Linear Dynamical Systems (LDS). The focus is on defining these systems, understanding their mathematical representation, and solving them using the concept of the matrix exponential.</p>

<b>1. Introduction to Linear Dynamical Systems (LDS)</b>
<p>A dynamical system is a system whose state evolves over time. The term "dynamic" is the opposite of "static," implying constant change and evolution. A <b>Linear Dynamical System (LDS)</b> is a specific type of dynamical system where the rules governing this evolution are linear.</p>
<p>The lecture focuses on <b>autonomous</b> linear dynamical systems, which are systems that evolve without any external input. The fundamental equation describing such a system is a first-order vector differential equation:</p>
\\[ \dot{\bar{v}}(t) = H \bar{v}(t) \\]
<p>Let's break down the components of this equation:</p>
<ul>
    <li><b>\\(\bar{v}(t)\\)</b>: This is the <b>state vector</b> of the system at time \\(t\\). It is an \\(n \times 1\\) column vector whose elements, \\(v_1(t), v_2(t), \dots, v_n(t)\\), represent the key variables needed to describe the state of the system.</li>
    <li><b>\\(\dot{\bar{v}}(t)\\)</b>: This is the time derivative of the state vector, often written as \\(\frac{d\bar{v}}{dt}\\). It represents the rate of change of each state variable:
    \\[ \dot{\bar{v}} = \frac{d\bar{v}}{dt} = \begin{bmatrix} \frac{dv_1}{dt} \\ \frac{dv_2}{dt} \\ \vdots \\ \frac{dv_n}{dt} \end{bmatrix} \\]
    This vector describes how the system is changing at any instant.
    </li>
    <li><b>\\(H\\)</b>: This is an \\(n \times n\\) square matrix, often called the <b>system matrix</b> or state matrix. It defines the linear relationship between the current state of the system \\(\bar{v}(t)\\) and its rate of change \\(\dot{\bar{v}}(t)\\). The "linear" part of LDS comes from this matrix multiplication, which is a linear transformation.</li>
</ul>
<p>The system is called "autonomous" because the rate of change \\(\dot{\bar{v}}\\) depends only on the current state \\(\bar{v}\\) and not on any external forcing function or input. A non-autonomous system would have an additional input term, for example: \\(\dot{\bar{v}} = H\bar{v} + Q\bar{u}\\), where \\(\bar{u}\\) is an input vector.</p>

<b>2. Example: An RC Circuit as an LDS</b>
<p>A physical circuit containing energy storage elements like capacitors and inductors is a classic example of a dynamical system. The lecture uses a two-node RC circuit to demonstrate how to model a physical system as an LDS.</p>

<p>The state variables are chosen to be the voltages across the capacitors, \\(v_1(t)\\) and \\(v_2(t)\\). The state vector is therefore:</p>
\\[ \bar{v} = \begin{bmatrix} v_1(t) \\ v_2(t) \end{bmatrix} \\]

<p>The model is derived using two fundamental principles:</p>
<ol>
    <li><b>Capacitor Law</b>: The current \\(i\\) through a capacitor \\(C\\) is proportional to the rate of change of voltage \\(v\\) across it: \\(i = C \frac{dv}{dt}\\).</li>
    <li><b>Kirchhoff's Current Law (KCL)</b>: The net sum of currents flowing out of any node in a circuit is zero.</li>
</ol>

<p>By applying KCL at each of the two nodes, we derive a system of two differential equations:</p>

<ul>
    <li><b>At Node 1:</b> The sum of currents leaving the node through \\(R_1\\), \\(C_1\\), and \\(R_2\\) is zero.
    \\[ \frac{v_1}{R_1} + C_1 \frac{dv_1}{dt} + \frac{v_1 - v_2}{R_2} = 0 \\]
    Rearranging this to solve for \\(\dot{v}_1 = \frac{dv_1}{dt}\\), we get:
    \\[ \dot{v}_1 = -\frac{1}{C_1}\left(\frac{1}{R_1} + \frac{1}{R_2}\right)v_1 + \frac{1}{C_1 R_2}v_2 \\]
    </li>
    <li><b>At Node 2:</b> The current flowing in from Node 1 via \\(R_2\\) equals the current flowing out through \\(C_2\\).
    \\[ \frac{v_1 - v_2}{R_2} = C_2 \frac{dv_2}{dt} \\]
    Rearranging for \\(\dot{v}_2 = \frac{dv_2}{dt}\\):
    \\[ \dot{v}_2 = \frac{1}{C_2 R_2}v_1 - \frac{1}{C_2 R_2}v_2 \\]
    </li>
</ul>

<p>These two linear differential equations can be combined into the standard LDS matrix form \\(\dot{\bar{v}} = H\bar{v}\\):</p>
\\[ \begin{bmatrix} \dot{v}_1 \\ \dot{v}_2 \end{bmatrix} = \begin{bmatrix} -\frac{1}{C_1}(\frac{1}{R_1} + \frac{1}{R_2}) & \frac{1}{C_1 R_2} \\ \frac{1}{C_2 R_2} & -\frac{1}{C_2 R_2} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} \\]
<p>This demonstrates how a real-world system can be modeled by the LDS equation, with the matrix \\(H\\) containing the physical parameters of the circuit.</p>

<b>3. The Solution to an LDS: The Matrix Exponential</b>
<p>The solution to the autonomous LDS equation \\(\dot{\bar{v}} = H\bar{v}\\) describes the state of the system at any future time \\(t\\), given its initial state \\(\bar{v}(0)\\) at \\(t=0\\). The solution is remarkably compact:</p>
\\[ \bar{v}(t) = e^{tH} \bar{v}(0) \\]

<p>The most important and novel concept here is \\(e^{tH}\\), known as the <b>matrix exponential</b>. It is the matrix analogue of the scalar exponential function \\(e^x\\). Just as \\(e^x\\) can be defined by its power series expansion, the matrix exponential is defined similarly:</p>
\\[ e^{tH} = I + tH + \frac{(tH)^2}{2!} + \frac{(tH)^3}{3!} + \dots = \sum_{k=0}^{\infty} \frac{t^k H^k}{k!} \\]
<p>Here, \\(I\\) is the identity matrix and \\(H^k\\) is the matrix \\(H\\) multiplied by itself \\(k\\) times. This infinite series converges for any square matrix \\(H\\), providing a well-defined way to "exponentiate" a matrix.</p>

<b>4. Examples of Computing the Matrix Exponential</b>

<p>The lecture provides several examples to illustrate how to compute \\(e^{tH}\\) for different matrices \\(H\\).</p>

<p><b>Example 1: Identity Matrix</b></p>
<p>Let \\(H = I\\) (the identity matrix). Using the property that \\(I^k = I\\) for any \\(k \ge 1\\):</p>
\\[ e^{tI} = I + tI + \frac{t^2 I^2}{2!} + \frac{t^3 I^3}{3!} + \dots \\]
\\[ e^{tI} = I \left(1 + t + \frac{t^2}{2!} + \frac{t^3}{3!} + \dots \right) \\]
<p>The scalar series in the parenthesis is the power series for \\(e^t\\). Therefore:</p>
\\[ e^{tI} = e^t I \\]

<p><b>Example 2: A Nilpotent Matrix</b></p>
<p>A matrix \\(H\\) is called nilpotent if some power of it is the zero matrix. Consider:</p>
\\[ H = \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix} \\]
<p>Computing its square gives:</p>
\\[ H^2 = \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \\]
<p>Since \\(H^2 = 0\\), all higher powers (\\(H^3, H^4, \dots\\)) will also be the zero matrix. This causes the infinite power series for \\(e^{tH}\\) to truncate after the second term:</p>
\\[ e^{tH} = I + tH + \frac{t^2 H^2}{2!} + \frac{t^3 H^3}{3!} + \dots = I + tH + 0 + 0 + \dots \\]
\\[ e^{tH} = I + tH = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + t \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & -t \\ 0 & 1 \end{bmatrix} \\]

<p><b>Example 3: A Skew-Symmetric Matrix (Rotation)</b></p>
<p>Consider a matrix that generates rotation:</p>
\\[ H = \begin{bmatrix} 0 & \omega \\ -\omega & 0 \end{bmatrix} = \omega \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} = \omega \tilde{I} \\]
<p>The powers of \\(H\\) follow a repeating pattern:</p>
<ul>
    <li>\\(H^2 = \omega^2 \tilde{I}^2 = \omega^2 \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} = -\omega^2 I\\)</li>
    <li>\\(H^3 = H^2 H = (-\omega^2 I)(\omega \tilde{I}) = -\omega^3 \tilde{I}\\)</li>
    <li>\\(H^4 = H^2 H^2 = (-\omega^2 I)(-\omega^2 I) = \omega^4 I\\)</li>
</ul>
<p>Substituting these into the power series and grouping terms with \\(I\\) and \\(\tilde{I}\\):</p>
\\[ e^{tH} = \left(I - \frac{t^2\omega^2}{2!}I + \frac{t^4\omega^4}{4!}I - \dots\right) + \left(t\omega\tilde{I} - \frac{t^3\omega^3}{3!}\tilde{I} + \frac{t^5\omega^5}{5!}\tilde{I} - \dots\right) \\]
<p>Factoring out the matrices \\(I\\) and \\(\tilde{I}\\):</p>
\\[ e^{tH} = I \left(1 - \frac{(\omega t)^2}{2!} + \frac{(\omega t)^4}{4!} - \dots\right) + \tilde{I} \left((\omega t) - \frac{(\omega t)^3}{3!} + \frac{(\omega t)^5}{5!} - \dots\right) \\]
<p>We recognize the scalar power series for \\(\cos(\omega t)\\) and \\(\sin(\omega t)\\):</p>
\\[ e^{tH} = I \cos(\omega t) + \tilde{I} \sin(\omega t) \\]
<p>Substituting the matrix forms for \\(I\\) and \\(\tilde{I}\\) yields the final result:</p>
\\[ e^{tH} = \begin{bmatrix} \cos(\omega t) & 0 \\ 0 & \cos(\omega t) \end{bmatrix} + \begin{bmatrix} 0 & \sin(\omega t) \\ -\sin(\omega t) & 0 \end{bmatrix} = \begin{bmatrix} \cos(\omega t) & \sin(\omega t) \\ -\sin(\omega t) & \cos(\omega t) \end{bmatrix} \\]
<p>This is a <b>rotation matrix</b>. The solution \\(\bar{v}(t) = e^{tH}\bar{v}(0)\\) means that the initial state vector \\(\bar{v}(0)\\) is continuously rotated over time with an angular velocity of \\(\omega\\), resulting in periodic, oscillatory behavior.</p>

</div></div><div class="chapter" id="Lecture 48 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 48 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts presented in the transcript, focusing on the application of the Eigenvalue Decomposition (EVD) to analyze Autonomous Linear Dynamical Systems (LDS).</p>

<b>1. Autonomous Linear Dynamical Systems (LDS) and The Matrix Exponential</b>
<p>An autonomous linear dynamical system is a system whose state evolves over time according to a linear rule, without any external input. Its behavior is governed by a first-order differential equation.</p>
<p>The fundamental equation for such a system is:</p>
\\[ \dot{\bar{v}} = H \bar{v} \\]
<p>where:</p>
<ul>
    <li>\\(\bar{v}\\) is the state vector of the system.</li>
    <li>\\(\dot{\bar{v}}\\), which is \\(\frac{d\bar{v}}{dt}\\), represents the rate of change of the state vector over time \\(t\\).</li>
    <li>\\(H\\) is a square matrix that defines the dynamics of the system.</li>
</ul>
<p>The solution to this differential equation, which describes the state of the system \\(\bar{v}(t)\\) at any time \\(t\\), given an initial state \\(\bar{v}(0)\\), is given by:</p>
\\[ \bar{v}(t) = e^{tH} \bar{v}(0) \\]
<p>The term \\(e^{tH}\\) is called the <b>Matrix Exponential</b>. It is a matrix function analogous to the scalar exponential function \\(e^x\\). It's defined by its Taylor series expansion:</p>
\\[ e^{tH} = I + tH + \frac{(tH)^2}{2!} + \frac{(tH)^3}{3!} + \dots = \sum_{n=0}^{\infty} \frac{(tH)^n}{n!} \\]
<p>While this definition is fundamental, calculating it directly can be very complex. The Eigenvalue Decomposition provides a much more practical method for its computation.</p>

<b>2. Eigenvalue Decomposition (EVD)</b>
<p>The Eigenvalue Decomposition is a factorization of a matrix into a product of three matrices related to its eigenvalues and eigenvectors. For a square matrix \\(H\\) (assuming it is diagonalizable), the EVD is:</p>
\\[ H = U \Lambda U^{-1} \\]
<p>The components of this decomposition are:</p>
<ul>
    <li><b>\\(\Lambda\\)</b>: A diagonal matrix containing the eigenvalues (\\(\lambda_1, \lambda_2, \dots, \lambda_m\\)) of \\(H\\).
    \\[ \Lambda = \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_m \end{pmatrix} \\]
    </li>
    <li><b>\\(U\\)</b>: A matrix whose columns are the corresponding eigenvectors (\\(\bar{u}_1, \bar{u}_2, \dots, \bar{u}_m\\)) of \\(H\\). These are also known as the <i>right eigenvectors</i>.
    \\[ U = \begin{pmatrix} | & | & & | \\ \bar{u}_1 & \bar{u}_2 & \dots & \bar{u}_m \\ | & | & & | \end{pmatrix} \\]
    </li>
    <li><b>\\(U^{-1}\\)</b>: The inverse of the eigenvector matrix \\(U\\).</li>
</ul>
<p>This decomposition is derived from the fundamental property of eigenvalues and eigenvectors: \\(H\bar{u}_i = \lambda_i\bar{u}_i\\), which can be written in matrix form as \\(HU = U\Lambda\\).</p>

<b>3. Powers of a Matrix using EVD</b>
<p>The EVD simplifies the computation of matrix powers. To calculate \\(H^n\\), we can use its decomposed form:</p>
\\[ H^n = (U \Lambda U^{-1})^n = (U \Lambda U^{-1})(U \Lambda U^{-1}) \dots (U \Lambda U^{-1}) \quad (n \text{ times}) \\]
<p>In this product, the adjacent \\(U^{-1}U\\) terms cancel out to become the identity matrix (\\(I\\)), leaving:</p>
\\[ H^n = U (\Lambda \cdot \Lambda \cdot \dots \cdot \Lambda) U^{-1} = U \Lambda^n U^{-1} \\]
<p>Since \\(\Lambda\\) is a diagonal matrix, raising it to the power \\(n\\) is straightforward; we simply raise each diagonal element (eigenvalue) to the power \\(n\\):</p>
\\[ \Lambda^n = \begin{pmatrix} \lambda_1^n & 0 & \dots & 0 \\ 0 & \lambda_2^n & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_m^n \end{pmatrix} \\]

<b>4. Calculating the Matrix Exponential using EVD</b>
<p>We can now substitute the EVD expression for \\(H^n\\) into the Taylor series for the matrix exponential:</p>
\\[ e^{tH} = \sum_{n=0}^{\infty} \frac{t^n H^n}{n!} = \sum_{n=0}^{\infty} \frac{t^n (U \Lambda^n U^{-1})}{n!} \\]
<p>We can factor out \\(U\\) and \\(U^{-1}\\) from the summation:</p>
\\[ e^{tH} = U \left( \sum_{n=0}^{\infty} \frac{t^n \Lambda^n}{n!} \right) U^{-1} = U \left( \sum_{n=0}^{\infty} \frac{(t\Lambda)^n}{n!} \right) U^{-1} \\]
<p>The expression inside the parentheses is the Taylor series for \\(e^{t\Lambda}\\). Since \\(\Lambda\\) is diagonal, this exponential is also a diagonal matrix where each element is the scalar exponential of the corresponding eigenvalue:</p>
\\[ e^{t\Lambda} = \begin{pmatrix} e^{\lambda_1 t} & 0 & \dots & 0 \\ 0 & e^{\lambda_2 t} & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & e^{\lambda_m t} \end{pmatrix} \\]
<p>This leads to the final, simplified formula for the matrix exponential:</p>
\\[ e^{tH} = U e^{t\Lambda} U^{-1} \\]
<p>This is a powerful result because it reduces the complex problem of computing a matrix exponential to the much simpler task of computing scalar exponentials of its eigenvalues.</p>

<b>5. Left Eigenvectors</b>
<p>The EVD equation \\(H = U \Lambda U^{-1}\\) can be rearranged by left-multiplying by \\(U^{-1}\\) to get:</p>
\\[ U^{-1}H = \Lambda U^{-1} \\]
<p>If we denote the rows of the matrix \\(U^{-1}\\) as \\(\bar{w}_k^T\\):</p>
\\[ U^{-1} = \begin{pmatrix} \rule[0.5ex]{0.8em}{0.4pt} & \bar{w}_1^T & \rule[0.5ex]{0.8em}{0.4pt} \\ \rule[0.5ex]{0.8em}{0.4pt} & \bar{w}_2^T & \rule[0.5ex]{0.8em}{0.4pt} \\ & \vdots & \\ \rule[0.5ex]{0.8em}{0.4pt} & \bar{w}_m^T & \rule[0.5ex]{0.8em}{0.4pt} \end{pmatrix} \\]
<p>Then the equation implies that \\(\bar{w}_k^T H = \lambda_k \bar{w}_k^T\\). These row vectors \\(\bar{w}_k^T\\) are called the <b>left eigenvectors</b> of \\(H\\). By default, the term "eigenvector" refers to the right eigenvectors (columns of \\(U\\)).</p>

<b>6. The Solution of LDS in terms of Modes</b>
<p>By substituting the EVD form of the matrix exponential into the LDS solution, we gain deeper insight into the system's behavior. The solution \\(\bar{v}(t)\\) becomes:</p>
\\[ \bar{v}(t) = (U e^{t\Lambda} U^{-1}) \bar{v}(0) \\]
<p>This can be expanded into a summation form, which represents the overall system behavior as a sum of independent "modes":</p>
\\[ \bar{v}(t) = \sum_{k=1}^{m} \bar{u}_k e^{\lambda_k t} (\bar{w}_k^T \bar{v}(0)) \\]
<p>Each term in this sum is a <b>mode</b> of the system, and its behavior can be broken down into three parts:</p>
<ol>
    <li><b>Projection:</b> The term \\(\bar{w}_k^T \bar{v}(0)\\) is a scalar coefficient. It represents the projection of the initial state \\(\bar{v}(0)\\) onto the k-th left eigenvector. This determines the initial "strength" or contribution of that mode.</li>
    <li><b>Growth/Decay:</b> The term \\(e^{\lambda_k t}\\) is a scalar function of time that dictates how the mode evolves. If the real part of \\(\lambda_k\\) is positive, the mode grows exponentially. If it's negative, it decays exponentially. If it's zero, it oscillates or remains constant.</li>
    <li><b>Direction:</b> The vector \\(\bar{u}_k\\) is the k-th (right) eigenvector. It defines the fixed direction in the state space along which the k-th mode evolves.</li>
</ol>
<p>In summary, the EVD decomposes the complex dynamics of the system into a sum of simple, independent behaviors (modes), each evolving along an eigenvector direction at a rate determined by its corresponding eigenvalue.</p>

<b>7. System Stability</b>
<p>The stability of the autonomous LDS is determined by its long-term behavior. The system is considered stable if its state returns to the origin over time, i.e., \\(\bar{v}(t) \to 0\\) as \\(t \to \infty\\).</p>
<p>From the modal decomposition, for the state \\(\bar{v}(t)\\) to approach zero, every mode must decay to zero. This requires that the growth factor for each mode goes to zero:</p>
\\[ e^{\lambda_k t} \to 0 \quad \text{as} \quad t \to \infty \\]
<p>This condition holds true if and only if the real part of every eigenvalue \\(\lambda_k\\) is less than zero. (The transcript simplifies this to \\(\lambda_k < 0\\), which is sufficient for real eigenvalues). Therefore, the stability of the entire system is determined by the eigenvalues of the matrix \\(H\\).</p>
</div></div><h2>Weekly Summary</h2><div>
<p>This week's modules focus on advanced applications of linear algebra, specifically in signal processing for wireless communications and in the analysis of dynamic systems. The key theme is how matrix representations and properties, particularly eigenvalue decomposition, can be used to model, analyze, and simplify complex systems.</p>

<b>1. Fourier Analysis and Matrix Representation</b>
<p>The week begins with an introduction to Fourier analysis, framing the Discrete Fourier Transform (DFT) and its inverse (IDFT) as linear transformations that can be represented by matrices.</p>
<ul>
 <li><b>Key Concepts:</b>
  <ul>
   <li>The <b>Fast Fourier Transform (FFT)</b> and <b>Inverse Fast Fourier Transform (IFFT)</b> are computationally efficient algorithms for the DFT and IDFT, respectively. These transforms are fundamental for converting signals between the time domain and the frequency domain.</li>
   <li>The N-point DFT is defined as: \\( X_k = \sum_{n=0}^{N-1} x_n e^{-j\frac{2\pi kn}{N}} \\)</li>
   <li>The N-point IDFT is defined as: \\( x_n = \frac{1}{N} \sum_{k=0}^{N-1} X_k e^{j\frac{2\pi kn}{N}} \\)</li>
  </ul>
 </li>
 <li><b>Key Takeaways:</b>
  <ul>
   <li>Both the FFT and IFFT operations can be represented as multiplication by an \\(N \times N\\) matrix. The <b>FFT matrix</b> (\\(F_{FFT}\\)) is constructed using powers of the N-th root of unity, \\( w = e^{-j\frac{2\pi}{N}} \\).</li>
   <li>The FFT matrix is transpose-symmetric, but not Hermitian symmetric.</li>
   <li>The IFFT matrix is closely related to the FFT matrix: \\( F_{IFFT} = \frac{1}{N} F_{FFT}^H \\).</li>
   <li>A crucial property is that they are inverses of each other (up to a scaling factor): \\( F_{FFT} \cdot F_{IFFT} = I \\), where \\(I\\) is the identity matrix. This shows the FFT matrix is a scaled unitary matrix.</li>
  </ul>
 </li>
</ul>

<b>2. Applications in Wireless Communications: OFDM and SC-FDMA</b>
<p>The concepts of FFT/IFFT matrices are applied to understand two dominant technologies in modern wireless systems like 4G and 5G: Orthogonal Frequency Division Multiplexing (OFDM) and Single-Carrier Frequency-Division Multiple Access (SC-FDMA).</p>
<ul>
 <li><b>OFDM System Model:</b>
  <ul>
   <li>OFDM is a multi-carrier technique used to combat <b>Inter-Symbol Interference (ISI)</b> on high-speed wireless channels.</li>
   <li>At the transmitter, an <b>IFFT</b> is performed on the data symbols, and a <b>Cyclic Prefix (CP)</b> is added. The CP makes the linear channel convolution behave like a circular convolution.</li>
   <li>This circular convolution can be modeled as \\( \mathbf{y} = H_c \mathbf{x} + \mathbf{w} \\), where \\(H_c\\) is a special type of matrix known as a <b>circulant matrix</b>.</li>
  </ul>
 </li>
 <li><b>Circulant Matrices and Eigenvalue Decomposition (EVD):</b>
  <ul>
   <li>A key insight is that any circulant matrix is diagonalized by the FFT and IFFT matrices. The EVD of \\(H_c\\) is \\( H_c = F_{IFFT} \Lambda F_{FFT} \\).</li>
   <li>The <b>eigenvectors</b> of \\(H_c\\) are the columns of the IFFT matrix.</li>
   <li>The <b>eigenvalues</b> (the diagonal entries of \\(\Lambda\\)) are the DFT of the channel taps (i.e., the channel's frequency response).</li>
  </ul>
 </li>
 <li><b>Key Takeaways:</b>
  <ul>
   <li>By applying an <b>FFT</b> at the OFDM receiver, the complex ISI channel is transformed into a set of simple, parallel, non-interfering sub-channels. The final model is \\( Y_k = H_k X_k + W_k \\), where \\(H_k\\) is the k-th eigenvalue. This makes equalization trivial (single-tap equalization).</li>
   <li><b>SC-FDMA</b> is a single-carrier alternative to OFDM used in the uplink (mobile to base station) due to its lower Peak-to-Average Power Ratio (PAPR).</li>
   <li>In SC-FDMA, no IFFT is performed at the transmitter. At the receiver, an FFT is applied, equalization is performed in the frequency domain, and then an IFFT is used to recover the original symbols. This is known as <b>Frequency Domain Equalization (FDE)</b>.</li>
  </ul>
 </li>
</ul>

<b>3. Linear Dynamical Systems (LDS)</b>
<p>The final topic covers the analysis of systems that evolve over time, known as Linear Dynamical Systems, again leveraging matrix properties.</p>
<ul>
 <li><b>Key Concepts:</b>
  <ul>
   <li>An autonomous LDS is described by the differential equation \\( \dot{\mathbf{v}} = H \mathbf{v} \\), where \\(\mathbf{v}(t)\\) is the state vector and \\(H\\) is a constant matrix.</li>
   <li>The solution is given by \\( \mathbf{v}(t) = e^{tH} \mathbf{v}(0) \\), involving a <b>matrix exponential</b>, \\(e^{tH}\\).</li>
   <li>The matrix exponential is defined via a power series: \\( e^{A} = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \dots \\).</li>
  </ul>
 </li>
 <li><b>Key Takeaways:</b>
  <ul>
   <li>Eigenvalue decomposition provides a powerful way to compute the matrix exponential. If \\( H = U \Lambda U^{-1} \\), then \\( e^{tH} = U e^{t\Lambda} U^{-1} \\). The term \\(e^{t\Lambda}\\) is a simple diagonal matrix with entries \\(e^{\lambda_k t}\\), where \\(\lambda_k\\) are the eigenvalues of \\(H\\).</li>
   <li>The system's behavior can be understood as a sum of independent "modes," each associated with an eigenvalue-eigenvector pair. The evolution of the k-th mode is governed by the term \\(e^{\lambda_k t}\\).</li>
   <li>The stability of the system depends on the eigenvalues. The system is stable (i.e., \\(\mathbf{v}(t) \to 0\\) as \\(t \to \infty\\)) if all eigenvalues \\(\lambda_k\\) have a negative real part.</li>
  </ul>
 </li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<p><b>Question 1: A recommender system can be used for</b></p>
<p><b>Explanation:</b> This question asks about the applications of recommender systems. Recommender systems are algorithms designed to suggest relevant items to users. This is a core feature in many modern online platforms. For example, e-commerce sites like Amazon recommend products, streaming services like Netflix recommend movies and series, and social media platforms like TikTok or Instagram recommend posts and videos. Since all the given options are valid applications, the correct answer is "All of these".</p>
<br>
<p><b>Question 2: Consider a movie recommender system with the user biases \\(u_i\\), movie biases \\(m_j\\), and average bias \\(r_a\\). The quantity \\(r_{ij}\\), which denotes the rating given by user \\(i\\) to movie \\(j\\), can be modeled as</b></p>
<p><b>Explanation:</b> This question describes a common baseline model for recommender systems based on biases. The rating \\(r_{ij}\\) is predicted by considering three components:
<ul>
<li>\\(r_a\\): The average rating across all users and all movies. This is the global baseline.</li>
<li>\\(u_i\\): The bias of user \\(i\\). A positive \\(u_i\\) means the user tends to give higher ratings than average, while a negative \\(u_i\\) means they tend to give lower ratings.</li>
<li>\\(m_j\\): The bias of movie \\(j\\). A positive \\(m_j\\) means the movie tends to receive higher ratings than average, and a negative \\(m_j\\) means it tends to receive lower ratings.</li>
</ul>
The simplest way to combine these is by adding them together. Therefore, the rating is modeled as the sum of the average bias, the user bias, and the movie bias: \\(r_{ij} = r_a + u_i + m_j\\).</p>
<br>
<p><b>Question 3: The \\(4 \times 4\\) FFT matrix is given as</b></p>
<p><b>Explanation:</b> The \\(N \times N\\) Discrete Fourier Transform (DFT) matrix, often used in FFT algorithms, is defined by its elements \\(F_{k,n} = \omega_N^{kn}\\), where \\(\omega_N = e^{-j2\pi/N}\\) is a primitive N-th root of unity and \\(k, n\\) range from 0 to \\(N-1\\). For \\(N=4\\), we have \\(\omega_4 = e^{-j2\pi/4} = e^{-j\pi/2} = -j\\).<br>The matrix elements are \\(F_{k,n} = (-j)^{kn}\\).
<ul>
<li>Row 1 (k=0): \\([(-j)^0, (-j)^0, (-j)^0, (-j)^0] = [1, 1, 1, 1]\\)</li>
<li>Row 2 (k=1): \\([(-j)^0, (-j)^1, (-j)^2, (-j)^3] = [1, -j, -1, j]\\)</li>
<li>Row 3 (k=2): \\([(-j)^0, (-j)^2, (-j)^4, (-j)^6] = [1, -1, 1, -1]\\)</li>
<li>Row 4 (k=3): \\([(-j)^0, (-j)^3, (-j)^6, (-j)^9] = [1, j, -1, -j]\\)</li>
</ul>
Assembling these rows gives the matrix shown in the third option, which is the correct answer.</p>
<br>
<p><b>Question 4: Consider a multiple antenna uniform linear array with L = 2 antennas... The array response vector \\(\bar{a}(\theta)\\), corresponding to angle of arrival \\(\theta = \pi/3\\) is</b></p>
<p><b>Explanation:</b> The array response vector (or steering vector) for a Uniform Linear Array (ULA) with \\(L\\) antennas is given by:
\\[ \bar{a}(\theta) = \begin{bmatrix} 1 \\ e^{-j\phi} \\ e^{-j2\phi} \\ \vdots \\ e^{-j(L-1)\phi} \end{bmatrix} \\]
where \\(\phi = \frac{2\pi d}{\lambda}\cos\theta\\) is the phase shift between adjacent antennas. The angle \\(\theta\\) is often defined as the angle of arrival with respect to the array axis.<br>
Given the parameters:
<ul>
<li>Number of antennas, \\(L=2\\)</li>
<li>Antenna spacing, \\(d = \lambda/2\\)</li>
<li>Angle of arrival, \\(\theta = \pi/3\\)</li>
</ul>
First, we calculate the phase shift \\(\phi\\):
\\[ \phi = \frac{2\pi}{\lambda} \left(\frac{\lambda}{2}\right) \cos\left(\frac{\pi}{3}\right) = \pi \cdot \frac{1}{2} = \frac{\pi}{2} \\]
Now, we construct the array response vector for \\(L=2\\):
\\[ \bar{a}(\theta) = \begin{bmatrix} 1 \\ e^{-j\phi} \end{bmatrix} = \begin{bmatrix} 1 \\ e^{-j\pi/2} \end{bmatrix} \\]
Since \\(e^{-j\pi/2} = \cos(-\pi/2) + j\sin(-\pi/2) = 0 - j = -j\\), the vector is:
\\[ \bar{a}(\theta) = \begin{bmatrix} 1 \\ -j \end{bmatrix} \\]
This matches the selected answer.</p>
<br>
<p><b>Question 5: Consider a zero-mean wide sense stationary time-series \\(x(n)\\)... The covariance matrix \\(E\{\bar{x}\bar{x}^T\}\\) has the following structure</b></p>
<p><b>Explanation:</b> A time-series \\(x(n)\\) is Wide-Sense Stationary (WSS) if its mean is constant and its auto-correlation function \\(R_x(t_1, t_2) = E\{x(t_1)x(t_2)\}\\) depends only on the time lag \\(\tau = t_1 - t_2\\).
The given vector is \\(\bar{x} = [x(n-1), x(n-2), \dots, x(n-L)]^T\\).
The covariance matrix is \\(C = E\{\bar{x}\bar{x}^T\}\\). The element at the \\(i\\)-th row and \\(j\\)-th column of this matrix is \\(C_{ij} = E\{x(n-i)x(n-j)\}\\).
Because the process is WSS, this value depends only on the difference of the time indices: \\((n-i) - (n-j) = j-i\\). So, \\(C_{ij} = R_x(j-i)\\).
A matrix where the entry \\(C_{ij}\\) is a function of \\(j-i\\) is a <b>Toeplitz matrix</b>. In a Toeplitz matrix, all the elements along any diagonal are constant. Thus, the correct structure is Toeplitz.</p>
<br>
<p><b>Question 6: Consider the linear model \\(\bar{y} = H\bar{x} + \bar{n}\\)... The error covariance of the Linear Minimum Mean Square Error (LMMSE) estimate of \\(\bar{x}\\) for this system is given as</b></p>
<p><b>Explanation:</b> The error covariance matrix for the LMMSE estimate of \\(\bar{x}\\) can be computed using the formula \\(C_e = (R_{xx}^{-1} + H^T R_{nn}^{-1} H)^{-1}\\), where \\(R_{xx}\\) is the covariance of the signal \\(\bar{x}\\) and \\(R_{nn}\\) is the covariance of the noise \\(\bar{n}\\).
From the problem statement, we have:
<ul>
<li>\\(R_{xx} = E\{\bar{x}\bar{x}^T\} = \frac{1}{4}I\\)</li>
<li>\\(R_{nn} = E\{\bar{n}\bar{n}^T\} = I\\)</li>
<li>\\(H = \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ 1 & -1 \\ 1 & -1 \end{bmatrix}\\)</li>
</ul>
First, we find the inverses of the covariance matrices:
\\[ R_{xx}^{-1} = \left(\frac{1}{4}I\right)^{-1} = 4I \\]
\\[ R_{nn}^{-1} = I^{-1} = I \\]
Next, we compute \\(H^T H\\):
\\[ H^T H = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & -1 & -1 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ 1 & -1 \\ 1 & -1 \end{bmatrix} = \begin{bmatrix} 1+1+1+1 & 1+1-1-1 \\ 1+1-1-1 & 1+1+1+1 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} = 4I \\]
Now, substitute these into the formula for \\(C_e^{-1}\\):
\\[ C_e^{-1} = R_{xx}^{-1} + H^T R_{nn}^{-1} H = 4I + H^T(I)H = 4I + H^T H = 4I + 4I = 8I \\]
Finally, we find \\(C_e\\) by taking the inverse:
\\[ C_e = (8I)^{-1} = \frac{1}{8}I = \frac{1}{8}\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \\]
This matches the correct answer.</p>
<br>
<p><b>Question 7: Consider a zero-mean wide sense stationary time-series \\(x(n)\\). For such a process, the auto-correlation \\(E\{x(i)x(j)\}\\)</b></p>
<p><b>Explanation:</b> This question is about the definition of a Wide-Sense Stationary (WSS) process. A random process \\(x(n)\\) is WSS if its mean is constant and its auto-correlation function, \\(R_x(i,j) = E\{x(i)x(j)\}\\), depends only on the time lag \\(\tau = i-j\\). It does not depend on the absolute time indices \\(i\\) and \\(j\\), but only on how far apart they are. Therefore, the auto-correlation "Depends only on the time-difference \\(i-j\\)".</p>
<br>
<p><b>Question 8: Consider the matrix \\(H\\) given below... Its largest singular value \\(\sigma_1\\) is given as</b></p>
<p><b>Explanation:</b> The largest singular value \\(\sigma_1\\) of a matrix \\(H\\) is the square root of the largest eigenvalue of \\(H^T H\\). The matrix \\(H\\) is given as a product of three matrices, resembling the Singular Value Decomposition (SVD) form \\(H = U\Sigma V^H\\). If this were a valid SVD, the singular values would be the diagonal entries of the middle matrix \\(\Sigma\\).
In this question, the middle matrix is \\(\begin{bmatrix} 2 & 0 \\ 0 & 3/2 \end{bmatrix}\\). The diagonal entries are 2 and 1.5. The largest of these is 2.
However, the accepted answer is \\(\sqrt{3}\\). This suggests there is a typo in the question as presented in the image, because neither a full calculation of \\(H^T H\\) nor a direct interpretation of the provided numbers yields \\(\sqrt{3}\\). The intended question likely had \\(\sqrt{3}\\) as one of the diagonal entries in the \\(\Sigma\\) matrix.</p>
<br>
<p><b>Question 9: Consider the matrix \\(H\\) given below... Its largest singular value \\(\sigma_1\\) is given as</b></p>
<p><b>Explanation:</b> Similar to the previous question, we are asked for the largest singular value of a matrix \\(H\\) which is presented as a product of three matrices. If we interpret this as an SVD, \\(H = U\Sigma V^H\\), the singular values are the diagonal entries of the middle matrix \\(\Sigma\\).
The middle matrix shown is \\(\begin{bmatrix} 3 & 0 \\ 0 & 6 \end{bmatrix}\\). The diagonal entries are 3 and 6. The largest of these is 6.
However, the accepted answer is \\(\sqrt{48}\\), which is approximately 6.928. Since the largest value from the presumed \\(\Sigma\\) matrix (6) does not match the answer (\\(\sqrt{48}\\)), we can conclude there is a significant typo in the question as presented in the image. A direct multiplication and calculation of eigenvalues would be extremely complex and is unlikely to be the intended method. The intended question likely contained different numerical values.</p>
<br>
<p><b>Question 10: Consider the singular value decomposition of a tall matrix \\(H\\) given as \\(H = U\Sigma V^H\\). The matrix \\(V\\) is a</b></p>
<p><b>Explanation:</b> This question is about the definition of the Singular Value Decomposition (SVD). The SVD of any matrix \\(H\\) is a factorization of the form \\(H = U\Sigma V^H\\), where:
<ul>
<li>\\(U\\) is a unitary matrix containing the left-singular vectors.</li>
<li>\\(\Sigma\\) is a rectangular diagonal matrix containing the non-negative singular values.</li>
<li>\\(V^H\\) is the conjugate transpose of \\(V\\), and \\(V\\) is a <b>unitary matrix</b> containing the right-singular vectors.</li>
</ul>
A unitary matrix is a square matrix whose columns (and rows) form an orthonormal basis, such that \\(V^H V = I\\). Therefore, \\(V\\) is a unitary matrix.</p>
</div></div><div class="week" id="week_10"><h1 class="week-title">Week 10</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 49 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 49 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts behind Support Vector Machines (SVMs) as presented in the transcript. The focus is on the foundational principles of SVM as a linear, binary classifier.</p>

<h3>1. Introduction to Support Vector Machines (SVM)</h3>
<p>A Support Vector Machine, abbreviated as <b>SVM</b>, is a powerful and widely used algorithm in machine learning for classification tasks. The transcript introduces SVM as a <b>binary classifier</b>. This means its primary function is to take a set of data and partition it into two distinct groups or classes.</p>

<p>The goal is to find an optimal boundary that separates the data points of one class from the data points of the other. SVM is a highly effective method due to its strong theoretical foundation and excellent performance in many real-world applications.</p>

<h3>2. The Binary Classification Problem</h3>
<p>Binary classification is the task of categorizing data into one of two predefined classes. The transcript labels these classes as "Class 0" and "Class 1", which can also be represented by numerical values like {0, 1} or, as we'll see later, {+1, -1}. The output of a binary classifier is discrete; it provides a definitive label (e.g., "yes" or "no") rather than a continuous value.</p>

<p>The transcript provides two practical examples:</p>
<ul>
    <li><b>Medical Diagnosis:</b> Analyzing medical data (like blood tests or scans) to determine the <i>presence</i> or <i>absence</i> of a disease. The output is a clear "yes" or "no", making it a binary classification problem.</li>
    <li><b>Quality Control:</b> In manufacturing, data from various tests on a product (e.g., a car) is used to determine if it <i>meets</i> the required quality standards or <i>fails</i> to meet them. Again, the outcome falls into one of two categories.</li>
</ul>

<h3>3. Linear Classifiers and the Hyperplane</h3>
<p>While many types of classifiers exist, SVM in its basic form is a <b>linear classifier</b>. Linear classifiers are preferred because they are generally simpler to determine and analyze compared to their non-linear counterparts. They work by creating a linear decision boundary to separate the classes.</p>

<h4>The Hyperplane</h4>
<p>In an N-dimensional space, this linear boundary is called a <b>hyperplane</b>. A hyperplane is the generalization of familiar geometric concepts:</p>
<ul>
    <li>In a 2-dimensional space (N=2), a hyperplane is simply a <b>line</b>.</li>
    <li>In a 3-dimensional space (N=3), a hyperplane is a flat <b>plane</b>.</li>
</ul>
<p>For a space with more than three dimensions (N > 3), the boundary is still referred to as a hyperplane.</p>

<h4>The Equation of a Hyperplane</h4>
<p>A hyperplane in an N-dimensional space is defined by a linear equation. For a point with coordinates \\((X_1, X_2, \dots, X_n)\\), the equation is:</p>
\\[ A_1 X_1 + A_2 X_2 + \dots + A_n X_n + B = 0 \\]
<p>Using the principles of linear algebra, this equation can be expressed more compactly in vector form. Let:</p>
<ul>
    <li>\\(\bar{a}\\) be the <b>weight vector</b> (or coefficient vector): \\( \bar{a} = \begin{bmatrix} A_1 \\ A_2 \\ \vdots \\ A_n \end{bmatrix} \\)</li>
    <li>\\(\bar{x}\\) be the <b>variable vector</b>: \\( \bar{x} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} \\)</li>
    <li>\\(B\\) be a scalar constant, often called the bias.</li>
</ul>
<p>The equation of the hyperplane is then written as a dot product:</p>
\\[ \bar{a}^T \bar{x} + B = 0 \\]

<h3>4. Half-Spaces: Dividing the Space</h3>
<p>A key property of a hyperplane is that it divides the entire N-dimensional space into two distinct regions, known as <b>half-spaces</b>. These two regions are defined by the following inequalities:</p>
<ol>
    <li>The first half-space: \\( \bar{a}^T \bar{x} + B \ge 0 \\)</li>
    <li>The second half-space: \\( \bar{a}^T \bar{x} + B \le 0 \\)</li>
</ol>
<p>The hyperplane itself (\\( \bar{a}^T \bar{x} + B = 0 \\)) is the boundary between these two half-spaces. The fundamental idea of a linear classifier is to find a hyperplane such that all data points of Class 0 fall into one half-space, and all data points of Class 1 fall into the other.</p>

<h3>5. Formulating the SVM Problem</h3>
<p>To find the optimal hyperplane, we need to mathematically formulate the classification task. We start with a set of <i>M</i> training data points, where each point \\(\bar{x}_k\\) has an associated class label \\(y_k\\).</p>
<p>A crucial step in the SVM formulation is the choice of class labels. Instead of {0, 1}, SVM uses <b>{+1, -1}</b>. For instance:</p>
<ul>
    <li>Class 0 is assigned the label \\( y_k = +1 \\)</li>
    <li>Class 1 is assigned the label \\( y_k = -1 \\)</li>
</ul>
<p>This labeling convention allows us to create a single, elegant mathematical constraint for all data points.</p>

<h4>The Combined Classification Constraint</h4>
<p>Based on the half-space concept:</p>
<ul>
    <li>For a point \\(\bar{x}_k\\) in Class 0 (\\(y_k = +1\\)), we require it to be in the positive half-space: \\(\bar{a}^T \bar{x}_k + B \ge 0\\).</li>
    <li>For a point \\(\bar{x}_k\\) in Class 1 (\\(y_k = -1\\)), we require it to be in the negative half-space: \\(\bar{a}^T \bar{x}_k + B \le 0\\).</li>
</ul>
<p>By multiplying the second inequality by \\(y_k = -1\\), the inequality sign flips, giving \\(y_k(\bar{a}^T \bar{x}_k + B) \ge 0\\). Notice that this same equation also holds for the first case (where \\(y_k = +1\\)). Therefore, we can combine both conditions into a single constraint that must be true for all data points:</p>
\\[ y_k (\bar{a}^T \bar{x}_k + B) \ge 0 \\]

<h3>6. The Margin and the "Slab" Concept</h3>
<p>The constraint \\( y_k (\bar{a}^T \bar{x}_k + B) \ge 0 \\) has a flaw: it is satisfied by the <b>trivial solution</b> where \\(\bar{a} = \mathbf{0}\\) and \\(B=0\\). To avoid this and to create a more robust classifier, the constraint is strengthened:</p>
\\[ y_k (\bar{a}^T \bar{x}_k + B) \ge 1 \\]
<p>This seemingly small change has a profound geometric meaning. It no longer defines a single separating hyperplane. Instead, it defines two parallel hyperplanes that act as boundaries for each class:</p>
<ul>
    <li>For Class 0 (\\(y_k = +1\\)): \\( \bar{a}^T \bar{x}_k + B \ge 1 \\)</li>
    <li>For Class 1 (\\(y_k = -1\\)): \\( \bar{a}^T \bar{x}_k + B \le -1 \\)</li>
</ul>
<p>These two hyperplanes, \\( \bar{a}^T \bar{x} + B = 1 \\) and \\( \bar{a}^T \bar{x} + B = -1 \\), form a margin or a "slab" between the two classes. The data points of each class must lie on or outside their respective boundary plane, ensuring there is a clear separation between them.</p>

<p>The region between these two hyperplanes is a "no man's land" where no data points should fall. The width of this region is known as the <b>margin</b>.</p>

<h3>7. The Goal of SVM: Maximizing the Margin</h3>
<p>The central idea of the Support Vector Machine is not just to find any hyperplane that separates the data, but to find the hyperplane that creates the <b>maximum possible margin</b> (i.e., the thickest possible slab).</p>
<p><b>Why is a wider margin better?</b></p>
<p>A classifier with a larger margin is considered more <b>robust</b>. It is less sensitive to the specific location of the training data points and is more likely to classify new, unseen data correctly. A thin margin, by contrast, suggests a fragile classifier that might easily misclassify points that are slightly different from the training data. The goal of SVM is, therefore, to design a classifier that maximizes this slab width, thereby minimizing the potential for classification error.</p>
</div></div><div class="chapter" id="Lecture 50 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 50 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and mathematical derivations presented in the transcript, focusing on the formulation of the Support Vector Machine (SVM) as a geometric optimization problem.</p>

<h3>1. The Core Idea of Support Vector Machines (SVM)</h3>
<p>A Support Vector Machine (SVM) is a machine learning model used for classification tasks. The primary goal is to find an optimal separator for two distinct sets of data points in an n-dimensional space. The central philosophy of SVM is not just to find any separating boundary, but to find the <b>best</b> one.</p>
<p>This "best" boundary is defined as the one that maximizes the margin or separation between the two classes. Geometrically, this is visualized as fitting the "thickest possible slab" between the two sets of points. This slab is defined by two parallel hyperplanes, with each hyperplane touching the closest points of one class. Maximizing the thickness of this slab leads to a more robust and generalizable classifier.</p>

<h3>2. The Hyperplane: Equation and Normal Vector</h3>
<p>The fundamental building block of the SVM classifier is the hyperplane.</p>
<p><b>Equation of a Hyperplane:</b><br>
In an n-dimensional space, a hyperplane is a flat, (n-1)-dimensional subspace. Its equation is given by a linear equation:</p>
\\[ \bar{a}^T \bar{x} = c \\]
<p>Where:</p>
<ul>
    <li>\\(\bar{x}\\) is a vector representing any point \\((x_1, x_2, \dots, x_n)\\) on the hyperplane.</li>
    <li>\\(\bar{a}\\) is a constant vector \\((a_1, a_2, \dots, a_n)\\) that defines the orientation of the hyperplane.</li>
    <li>\\(c\\) is a constant scalar that determines the position of the hyperplane relative to the origin.</li>
    <li>\\(\bar{a}^T \bar{x}\\) represents the dot product (or inner product) of the vectors \\(\bar{a}\\) and \\(\bar{x}\\).</li>
</ul>

<p><b>The Normal Vector to the Hyperplane:</b><br>
An important property is that the vector \\(\bar{a}\\) is always perpendicular (or normal) to the hyperplane. The transcript provides a clear proof of this fact:</p>
<ol>
    <li>Consider any two distinct points, \\(\bar{x}_1\\) and \\(\bar{x}_2\\), that lie on the hyperplane.</li>
    <li>Since both points are on the hyperplane, they must both satisfy its equation:
    \\[ \bar{a}^T \bar{x}_1 = c \\]
    \\[ \bar{a}^T \bar{x}_2 = c \\]
    </li>
    <li>Subtracting the second equation from the first gives:
    \\[ \bar{a}^T \bar{x}_1 - \bar{a}^T \bar{x}_2 = c - c \\]
    \\[ \bar{a}^T (\bar{x}_1 - \bar{x}_2) = 0 \\]
    </li>
    <li>The vector \\(\bar{x}_1 - \bar{x}_2\\) is a vector that lies entirely <i>within</i> the hyperplane (it connects two points on the hyperplane). Let's call this vector \\(\bar{x}_{\text{tilde}} = \bar{x}_1 - \bar{x}_2\\).</li>
    <li>The equation \\(\bar{a}^T \bar{x}_{\text{tilde}} = 0\\) shows that the dot product of \\(\bar{a}\\) and any vector \\(\bar{x}_{\text{tilde}}\\) lying on the hyperplane is zero. By definition, this means \\(\bar{a}\\) is orthogonal, or normal, to the hyperplane.</li>
</ol>

<h3>3. Distance of a Hyperplane from the Origin</h3>
<p>The constant \\(c\\) in the hyperplane equation is directly related to the perpendicular distance of the hyperplane from the origin. This relationship can be derived using the geometric definition of the dot product.</p>
<ol>
    <li>The dot product \\(\bar{a}^T \bar{x}\\) can be expressed as:
    \\[ \bar{a}^T \bar{x} = \|\bar{a}\| \|\bar{x}\| \cos(\theta) \\]
    where \\(\|\bar{a}\|\\) and \\(\|\bar{x}\|\\) are the magnitudes (norms) of the vectors, and \\(\theta\\) is the angle between them.</li>
    <li>In our geometric setup, \\(\bar{a}\\) is the normal vector. The quantity \\(\|\bar{x}\| \cos(\theta)\\) is the length of the projection of the vector \\(\bar{x}\\) onto the normal vector \\(\bar{a}\\). This projected length is precisely the perpendicular distance, \\(d\\), of the hyperplane from the origin.</li>
    <li>Substituting this back into the hyperplane equation:
    \\[ \bar{a}^T \bar{x} = \|\bar{a}\| \cdot (\|\bar{x}\| \cos(\theta)) = c \\]
    \\[ \|\bar{a}\| \cdot d = c \\]
    </li>
    <li>Solving for the distance \\(d\\), we get:
    \\[ d = \frac{c}{\|\bar{a}\|} \\]
    </li>
</ol>
<p>The sign of \\(c\\) (and therefore \\(d\\)) indicates direction. If \\(c > 0\\), the hyperplane is displaced from the origin in the same direction as the normal vector \\(\bar{a}\\). If \\(c < 0\\), it is displaced in the opposite direction. If \\(c=0\\), the distance is zero, meaning the hyperplane passes through the origin.</p>

<h3>4. Distance Between Two Parallel Hyperplanes</h3>
<p>Two hyperplanes are parallel if they share the same normal vector \\(\bar{a}\\). Their equations will only differ by the constant term:</p>
<ul>
    <li>Hyperplane 1: \\( \bar{a}^T \bar{x} = c_1 \\)</li>
    <li>Hyperplane 2: \\( \bar{a}^T \bar{x} = c_2 \\)</li>
</ul>
<p>Using the formula from the previous section, we can find the distance of each hyperplane from the origin:</p>
<ul>
    <li>Distance of Hyperplane 1 from origin: \\( d_1 = \frac{c_1}{\|\bar{a}\|} \\)</li>
    <li>Distance of Hyperplane 2 from origin: \\( d_2 = \frac{c_2}{\|\bar{a}\|} \\)</li>
</ul>
<p>The perpendicular distance between these two parallel hyperplanes is the difference between their individual distances from the origin:</p>
\\[ d_{\text{between}} = d_1 - d_2 = \frac{c_1}{\|\bar{a}\|} - \frac{c_2}{\|\bar{a}\|} \\]
\\[ d_{\text{between}} = \frac{c_1 - c_2}{\|\bar{a}\|} \\]
<p>This formula gives the separation between the two parallel hyperplanes that form the slab in our SVM problem.</p>

<h3>5. Formulating the SVM Optimization Problem</h3>
<p>Now we can apply these geometric principles to the SVM classifier.</p>
<p><b>Defining the Margin Hyperplanes:</b><br>
In SVM, the two parallel hyperplanes that define the slab are canonically written as:</p>
<ul>
    <li>\\( \bar{a}^T \bar{x} + b = 1 \\)</li>
    <li>\\( \bar{a}^T \bar{x} + b = -1 \\)</li>
</ul>
<p>To use our distance formula, we rewrite them in the form \\(\bar{a}^T \bar{x} = c\\):</p>
<ul>
    <li>Hyperplane 1: \\( \bar{a}^T \bar{x} = 1 - b \quad \implies c_1 = 1 - b \\)</li>
    <li>Hyperplane 2: \\( \bar{a}^T \bar{x} = -1 - b \quad \implies c_2 = -1 - b \\)</li>
</ul>

<p><b>Calculating the Margin (Slab Thickness):</b><br>
The thickness of the slab, which is the separation we want to maximize, is the distance between these two hyperplanes. Using the formula \\(d = \frac{c_1 - c_2}{\|\bar{a}\|}\\):</p>
\\[ d = \frac{(1 - b) - (-1 - b)}{\|\bar{a}\|} = \frac{1 - b + 1 + b}{\|\bar{a}\|} \\]
\\[ d = \frac{2}{\|\bar{a}\|} \\]
<p>This is a key result: the width of the margin is inversely proportional to the magnitude (norm) of the normal vector \\(\bar{a}\\).</p>

<p><b>The Optimization Objective and Constraints:</b><br>
The goal of SVM is to maximize this separation \\(d\\).</p>
<ul>
    <li><b>Objective:</b> To maximize \\( \frac{2}{\|\bar{a}\|} \\). This is mathematically equivalent to minimizing the denominator, \\( \|\bar{a}\| \\). (Often, one minimizes \\( \frac{1}{2}\|\bar{a}\|^2 \\) for mathematical convenience, but the principle is the same).</li>
    <li><b>Constraints:</b> We must ensure that all data points lie on the correct side of the margin. If we label the points of one class as \\(y_k = +1\\) and the other as \\(y_k = -1\\), this condition can be written as a single, elegant constraint:
    \\[ y_k (\bar{a}^T \bar{x}_k + b) \ge 1 \\]
    for all data points \\(k = 1, 2, \dots, m\\). This ensures that points with \\(y_k=+1\\) are on or outside the \\(\bar{a}^T \bar{x} + b = 1\\) hyperplane, and points with \\(y_k=-1\\) are on or outside the \\(\bar{a}^T \bar{x} + b = -1\\) hyperplane.</li>
</ul>

<p><b>The Final SVM Problem Formulation:</b><br>
Combining the objective and constraints, the SVM problem is formally stated as:</p>
<blockquote>
    <b>Minimize:</b> \\( \|\bar{a}\| \\) <br>
    <b>Subject to:</b> \\( y_k (\bar{a}^T \bar{x}_k + b) \ge 1 \\) for \\( k=1, \dots, m \\)
</blockquote>
<p>This is a <b>convex optimization problem</b>. The objective function is convex, and the constraints define a convex set. A significant advantage of this is that any local minimum is also a global minimum, which guarantees that we can find the single best solution efficiently using specialized computational tools (like CVX, as mentioned in the transcript).</p>
</div></div><div class="chapter" id="Lecture 51 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 51 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts, theories, and formulas related to <b>sparse regression</b> as presented in the transcript.</p>

<h3>1. Introduction to Sparse Regression</h3>
<p>Sparse regression is a specialized form of linear regression, a foundational technique in machine learning and signal processing. While standard linear regression builds a model using a linear combination of all available explanatory variables, sparse regression aims to create a model using the fewest possible variables.</p>

<p>The standard linear regression model predicts a response variable, denoted as \\(\hat{y}\\), using the following formula:</p>
\\[ \hat{y} = \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n \\]
<p>In this model:</p>
<ul>
    <li>\\(\hat{y}\\) is the predicted response.</li>
    <li>\\(x_1, x_2, \dots, x_n\\) are the <b>regressors</b> or <b>explanatory variables</b>. These are the components of an n-dimensional vector \\(\bar{x}\\).</li>
    <li>\\(\theta_1, \theta_2, \dots, \theta_n\\) are the <b>regression coefficients</b>. These coefficients determine the weight or importance of each corresponding explanatory variable in the prediction. They form a parameter vector denoted as \\(\bar{\theta}\\).</li>
</ul>

<h3>2. The Principle of Sparsity</h3>
<p>The core idea of sparse regression is to find a predictive model that is "sparse." This means the model relies on only a small, sparse subset of the available explanatory variables.</p>
<p>In terms of the model's coefficients, this translates to the following condition:
<ul>
    <li><b>Many regression coefficients (\\(\theta_i\\)) are exactly zero.</b></li>
    <li>Only a few coefficients are non-zero.</li>
</ul>
</p>
<p>If a coefficient \\(\theta_i\\) is zero, the corresponding variable \\(x_i\\) has no influence on the prediction. Therefore, by forcing many coefficients to be zero, the model effectively performs variable selection, identifying the most critical factors for explaining the response.</p>
<p>This results in a <b>sparse parameter vector</b> \\(\bar{\theta}\\). For example, if there are many regressors, the vector \\(\bar{\theta}\\) might look like this:</p>
\\[ \bar{\theta} = \begin{bmatrix} 0 \\ \vdots \\ 2 \\ 0 \\ \vdots \\ -3 \\ 0 \\ \vdots \\ 1 \\ 0 \\ \vdots \end{bmatrix} \\]
<p>This vector has a large number of zero entries and very few non-zero entries.</p>

<h3>3. The Computational Challenge of Sparsity</h3>
<p>A significant challenge in sparse regression is that we do not know in advance which coefficients should be non-zero. A naive approach would be to test every possible combination of non-zero coefficients.</p>
<p>For example, if we have \\(n=100\\) total regressors and we believe that approximately \\(k=40\\) of them are relevant (non-zero), we would need to examine every possible subset of 40 regressors. The number of combinations to check is given by the binomial coefficient:</p>
\\[ \binom{n}{k} = \binom{100}{40} \\]
<p>This number is astronomically large, making a brute-force search computationally intractable. This type of combinatorial problem is known to be <b>NP-hard</b>, as its complexity grows exponentially with the size of the problem.</p>

<h3>4. Formalizing Sparsity: The L0 Norm</h3>
<p>To formalize the goal of finding the "sparsest" solution, we can use the concept of the <b>\\(L_0\\) "norm"</b>. Although not a true mathematical norm, the \\(L_0\\) norm of a vector counts the number of its non-zero elements.</p>
<p>For the parameter vector \\(\bar{\theta}\\), the \\(L_0\\) norm is:</p>
\\[ ||\bar{\theta}||_0 = \text{number of non-zero elements in } \bar{\theta} \\]
<p>The problem of sparse regression can then be framed as an optimization problem: find the vector \\(\bar{\theta}\\) that fits the data well while having the smallest possible \\(L_0\\) norm.</p>

<h3>5. Matrix Formulation and the Underdetermined System</h3>
<p>Given a training dataset with \\(M\\) samples \\( (y_k, \bar{x}_k) \\) for \\(k=1, \dots, M\\), where \\(y_k\\) is the response and \\(\bar{x}_k\\) is the vector of regressors for the \\(k\\)-th sample, we can write the entire system of equations in matrix form:</p>
\\[ \bar{y} = X \bar{\theta} \\]
<p>Where:</p>
<ul>
    <li>\\(\bar{y}\\) is the \\(M \times 1\\) vector of observed responses.</li>
    <li>\\(X\\) is the \\(M \times n\\) matrix where each row is a sample's regressor vector \\(\bar{x}_k^T\\).</li>
    <li>\\(\bar{\theta}\\) is the \\(n \times 1\\) vector of unknown regression coefficients that we need to find.</li>
</ul>

<h4>Contrast with Conventional Linear Regression</h4>
<p>In <b>conventional linear regression</b>, we typically have more samples (equations) than regressors (unknowns), meaning \\(M \ge n\\). The matrix \\(X\\) is "tall," and the problem is overdetermined. A unique solution is found by minimizing the squared error, leading to the well-known <b>least-squares solution</b>:</p>
\\[ \hat{\theta} = (X^T X)^{-1} X^T \bar{y} \\]
<p>However, in many modern sparse regression problems, the system is <b>underdetermined</b>. This means we have significantly fewer measurements (samples) than unknowns, i.e., \\(M \ll n\\). In this case, the matrix \\(X\\) is "wide." The matrix \\(X^T X\\) becomes singular (non-invertible), and the least-squares formula cannot be used. An infinite number of solutions exist for \\(\bar{\theta}\\), and we must use the sparsity constraint to find the single, meaningful solution.</p>

<h3>6. Compressive Sensing</h3>
<p>The problem of recovering a sparse vector \\(\bar{\theta}\\) from an underdetermined system of linear equations \\(\bar{y} = X\bar{\theta}\\) is the central focus of a modern field called <b>compressive sensing</b> (or compressed sensing).</p>
<p>The name originates from the idea that we are "sensing" an n-dimensional signal or parameter vector \\(\bar{\theta}\\) using only \\(M\\) measurements, where \\(M \ll n\\). We are effectively using a "compressed" set of information to reconstruct the full signal. This reconstruction is only possible if we can leverage prior knowledge that the signal \\(\bar{\theta}\\) is sparse.</p>
<p>Compressive sensing is a revolutionary field with significant applications in:</p>
<ul>
    <li>Signal and Image Processing</li>
    <li>Medical Imaging (e.g., MRI, Tomography)</li>
    <li>Geology and Radar</li>
    <li>Machine Learning</li>
</ul>
<p>It allows for high-resolution signal recovery from far fewer measurements than required by traditional methods, fundamentally changing how data is acquired and processed in many scientific and engineering domains.</p>
</div></div><div class="chapter" id="Lecture 52 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 52 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts, theories, and formulas related to sparse regression and the Orthogonal Matching Pursuit (OMP) algorithm, as presented in the transcript.</p>

<h3>1. The Sparse Regression Model and its Interpretation</h3>
<p>Sparse regression aims to model a response vector \\(\mathbf{y}\\) using a linear combination of explanatory variables (or regressors) from a matrix \\(\mathbf{X}\\), with the constraint that only a few of these variables are used. This means the corresponding regression coefficient vector \\(\boldsymbol{\theta}\\) is "sparse," i.e., it has very few non-zero elements.</p>

<p>The fundamental model is given by:</p>
\\[ \mathbf{y} = \mathbf{X}\boldsymbol{\theta} \\]
<p>where:</p>
<ul>
    <li>\\(\mathbf{y}\\) is an \\(m \times 1\\) vector of observed responses.</li>
    <li>\\(\mathbf{X}\\) is an \\(m \times n\\) matrix of explanatory variables, where each column represents a different variable. The transcript emphasizes the case where \\(\mathbf{X}\\) is a "wide" matrix (\\(n > m\\)).</li>
    <li>\\(\boldsymbol{\theta}\\) is an \\(n \times 1\\) vector of regression coefficients. The goal is for this vector to be sparse.</li>
</ul>

<p>The model can be rewritten by expressing the matrix \\(\mathbf{X}\\) in terms of its columns \\(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\\):</p>
\\[ \mathbf{y} = [\mathbf{x}_1 | \mathbf{x}_2 | \dots | \mathbf{x}_n] \begin{pmatrix} \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{pmatrix} = \sum_{i=1}^{n} \mathbf{x}_i \theta_i \\]
<p>This form highlights that \\(\mathbf{y}\\) is being represented as a <b>linear combination of the columns of \\(\mathbf{X}\\)</b>. Since \\(\boldsymbol{\theta}\\) must be sparse, the problem becomes one of finding the <b>fewest columns</b> of \\(\mathbf{X}\\) that can be linearly combined to provide the best possible approximation of \\(\mathbf{y}\\). This reframing of the problem leads to alternative names for the task:</p>
<ul>
    <li><b>Subset Selection:</b> We are selecting the best subset of columns (explanatory variables) from the full set available in \\(\mathbf{X}\\).</li>
    <li><b>Basis Selection:</b> If we consider the columns \\(\{\mathbf{x}_1, \dots, \mathbf{x}_n\}\\) as a basis (or more accurately, a dictionary or spanning set), the task is to select a small subset of these basis vectors to represent \\(\mathbf{y}\\).</li>
</ul>

<h3>2. The Core Idea: A Greedy Approach Using Projections</h3>
<p>The algorithm described is a greedy, iterative procedure. The intuition is to build the approximation of \\(\mathbf{y}\\) one step at a time by selecting the most useful column of \\(\mathbf{X}\\) at each iteration.</p>
<p>To find the "most useful" or "best" column, we need a measure of similarity between the columns \\(\mathbf{x}_j\\) and the target vector \\(\mathbf{y}\\) (or what's left of it to be explained, known as the <b>residue</b>). The transcript explains that this similarity can be measured by calculating the <b>projection</b> of one vector onto another. A larger projection implies greater similarity.</p>
<p>The projection of \\(\mathbf{y}\\) onto a column vector \\(\mathbf{x}_j\\) is proportional to their inner product (or dot product), which is calculated as \\(\mathbf{x}_j^T \mathbf{y}\\). Therefore, the strategy is to find the column \\(\mathbf{x}_j\\) that maximizes this value.</p>
\\[ j_{\text{best}} = \arg\max_{j} |\mathbf{x}_j^T \mathbf{y}| \\]
<p>This chosen column \\(\mathbf{x}_{j_{\text{best}}}\\) is the first explanatory variable selected to explain the response \\(\mathbf{y}\\).</p>

<h3>3. The Orthogonal Matching Pursuit (OMP) Algorithm</h3>
<p>OMP is the iterative algorithm that formalizes the greedy approach described above. It iteratively "matches" the best column to the current residue and then "orthogonally" projects the data to find the new residue for the next iteration.</p>

<h4>Initialization</h4>
<ul>
    <li><b>Iteration Index:</b> Set \\(k=1\\).</li>
    <li><b>Residue:</b> The initial residue is the entire response vector, as nothing has been explained yet.
        \\[ \mathbf{r}^{(0)} = \mathbf{y} \\]
    </li>
    <li><b>Basis Matrix:</b> The initial set of selected columns is empty. This is represented by an empty matrix \\(\mathbf{X}_0\\).</li>
</ul>

<h4>Iteration Steps (for iteration \\(k\\))</h4>
<p>The algorithm repeats the following steps until a stopping criterion is met.</p>

<ol>
    <li><b>Matching Step (Find Best Column):</b><br>
    Find the column of \\(\mathbf{X}\\) that is most correlated with the residue from the previous iteration, \\(\mathbf{r}^{(k-1)}\\). This is done by finding the index \\(i_k\\) that maximizes the absolute value of the projection.
    \\[ i_k = \arg\max_{j} |\mathbf{x}_j^T \mathbf{r}^{(k-1)}| \\]
    This step identifies the most promising explanatory variable to add to the model.
    </li>

    <li><b>Augmentation Step (Update Basis):</b><br>
    The basis matrix \\(\mathbf{X}_k\\) is formed by augmenting the basis from the previous step, \\(\mathbf{X}_{k-1}\\), with the newly chosen column, \\(\mathbf{x}_{i_k}\\).
    \\[ \mathbf{X}_k = [\mathbf{X}_{k-1} | \mathbf{x}_{i_k}] \\]
    At each step, this matrix contains all the columns selected so far.
    </li>

    <li><b>Least Squares Solution (Find Coefficients):</b><br>
    With the current basis \\(\mathbf{X}_k\\), find the best linear combination of these columns to approximate the original vector \\(\mathbf{y}\\). This is a standard least squares problem:
    \\[ \min_{\boldsymbol{\theta}^{(k)}} ||\mathbf{y} - \mathbf{X}_k \boldsymbol{\theta}^{(k)}||_2^2 \\]
    The solution gives the optimal coefficients \\(\boldsymbol{\theta}^{(k)}\\) for the chosen subset of columns. The well-known formula for the least squares estimate is:
    \\[ \boldsymbol{\theta}^{(k)} = (\mathbf{X}_k^T \mathbf{X}_k)^{-1} \mathbf{X}_k^T \mathbf{y} \\]
    </li>

    <li><b>Residue Update:</b><br>
    The new residue \\(\mathbf{r}^{(k)}\\) is the part of \\(\mathbf{y}\\) that is not explained by the current approximation, \\(\mathbf{X}_k \boldsymbol{\theta}^{(k)}\\).
    \\[ \mathbf{r}^{(k)} = \mathbf{y} - \mathbf{X}_k \boldsymbol{\theta}^{(k)} \\]
    This residue is the approximation error and will be used in the matching step of the next iteration. Geometrically, this residue is orthogonal to the subspace spanned by the columns of \\(\mathbf{X}_k\\), which is where the "Orthogonal" part of the algorithm's name comes from.
    </li>
</ol>
<p>The iteration index is then incremented (\\(k \leftarrow k+1\\)) and the process repeats.</p>

<h4>Stopping Criteria</h4>
<p>The iterative process must be stopped. The transcript suggests two common criteria:</p>
<ol>
    <li><b>Approximation Error Threshold:</b> Stop when the norm of the residue falls below a small, predefined threshold \\(\epsilon\\). This means the approximation is "good enough."
    \\[ ||\mathbf{r}^{(k)}||_2 = ||\mathbf{y} - \mathbf{X}_k \boldsymbol{\theta}^{(k)}||_2 \leq \epsilon \\]
    </li>
    <li><b>Stagnation of Residue:</b> Stop when the residue no longer decreases significantly from one iteration to the next. This indicates that adding more columns is not improving the model's explanatory power.</li>
</ol>

<h3>4. Mapping the Final Solution</h3>
<p>After the algorithm terminates (e.g., at iteration \\(K\\)), the output is a small vector \\(\boldsymbol{\theta}^{(K)}\\) and a set of indices \\(\{i_1, i_2, \dots, i_K\}\\) corresponding to the columns selected. The final sparse regression vector \\(\boldsymbol{\theta}\\) (of original dimension \\(n \times 1\\)) must be constructed.</p>
<ul>
    <li>The \\(j\\)-th element of the solution vector \\(\boldsymbol{\theta}^{(K)}\\) corresponds to the column index \\(i_j\\) chosen in the \\(j\\)-th iteration.</li>
    <li>This value is placed at the \\(i_j\\)-th position in the final sparse vector \\(\boldsymbol{\theta}\\).</li>
    <li>All other elements of \\(\boldsymbol{\theta}\\) (corresponding to columns that were not selected) are set to zero.</li>
</ul>
<p>For example, if the \\(j\\)-th element of \\(\boldsymbol{\theta}^{(K)}\\) is \\(\theta^{(K)}_j\\), and this corresponds to the column with original index \\(i_j\\), then in the final \\(n \times 1\\) vector \\(\boldsymbol{\theta}\\), we set:</p>
\\[ \theta_{i_j} = \theta^{(K)}_j \\]
<p>The resulting vector \\(\boldsymbol{\theta}\\) will have a large number of zeros, thus achieving the goal of sparse regression.</p>

</div></div><div class="chapter" id="Lecture 53 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 53 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts and steps demonstrated in the transcript for solving a sparse regression problem using the <b>Orthogonal Matching Pursuit (OMP)</b> algorithm.</p>

<h3>1. The Sparse Regression Problem</h3>
<p>The core problem is to solve a system of linear equations of the form:</p>
\\[ \bar{y} = \mathbf{X} \bar{\theta} \\]
<p>where:</p>
<ul>
    <li>\\(\bar{y}\\) is a vector of known measurements.</li>
    <li>\\(\mathbf{X}\\) is a known matrix, often called the dictionary or design matrix.</li>
    <li>\\(\bar{\theta}\\) is a vector of unknown coefficients or parameters that we need to find.</li>
</ul>
<p>The problem is made challenging by two conditions present in the example:</p>
<ol>
    <li><b>Underdetermined System:</b> The matrix \\(\mathbf{X}\\) is a "wide matrix," meaning it has more columns (unknowns, \\(n\\)) than rows (measurements, \\(m\\)). In this case, \\(m=4\\) and \\(n=6\\). An underdetermined system (\\(m < n\\)) has infinitely many possible solutions for \\(\bar{\theta}\\).</li>
    <li><b>Sparsity Constraint:</b> To find a unique and meaningful solution, we impose the constraint that \\(\bar{\theta}\\) must be <b>sparse</b>. A sparse vector is one where most of its elements are zero. The challenge is that we do not know in advance which elements of \\(\bar{\theta}\\) are non-zero. The set of indices of these non-zero elements is called the <b>support</b> of the vector, which is unknown.</li>
</ol>
<p>The OMP algorithm is a greedy iterative method designed to find this sparse solution \\(\bar{\theta}\\).</p>

<h3>2. Example Setup</h3>
<p>The transcript sets up the following specific problem:</p>
<p>The measurement vector \\(\bar{y}\\) is:</p>
\\[ \bar{y} = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} \\]
<p>The matrix \\(\mathbf{X}\\), composed of six column vectors \\(\mathbf{x}_1, \dots, \mathbf{x}_6\\), is:</p>
\\[ \mathbf{X} = \begin{pmatrix} 1 & 1 & 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 1 & 1 & 0 \\ 0 & 1 & 1 & 0 & 1 & 1 \end{pmatrix} \\]
<p>Our goal is to find the sparse vector \\(\bar{\theta} \in \mathbb{R}^6\\) that satisfies \\(\bar{y} = \mathbf{X} \bar{\theta}\\).</p>

<h3>3. The Orthogonal Matching Pursuit (OMP) Algorithm</h3>

<h4>Initialization</h4>
<p>The algorithm starts by initializing two variables:</p>
<ul>
    <li>The initial <b>residue</b>, \\(\bar{r}_0\\), which is the unexplained part of \\(\bar{y}\\). Initially, nothing is explained, so the residue is the entire measurement vector.</li>
    \\[ \bar{r}_0 = \bar{y} = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} \\]
    <li>The initial <b>basis matrix</b>, \\(\mathbf{X}_0\\), which contains the columns of \\(\mathbf{X}\\) selected so far. Initially, no columns have been selected.</li>
    \\[ \mathbf{X}_0 = \text{[] (an empty matrix)} \\]
</ul>

<h4>Iteration 1</h4>
<p>Each iteration of OMP consists of three main steps: Matching, Solving, and Updating the Residue.</p>

<p><b>Step 1: Matching</b><br>
The goal is to find the column in the original matrix \\(\mathbf{X}\\) that is most correlated with the current residue \\(\bar{r}_0\\). This is done by finding the column that has the largest projection (in absolute value) onto the residue. The index \\(j\\) is chosen to maximize this correlation:</p>
\\[ i_1 = \arg\max_{j} |\langle \mathbf{x}_j, \bar{r}_0 \rangle| = \arg\max_{j} |\mathbf{x}_j^T \bar{r}_0| \\]
<p>A practical way to compute all projections at once is to multiply the transpose of \\(\mathbf{X}\\) with the residue vector:</p>
\\[ \mathbf{X}^T \bar{r}_0 = \begin{pmatrix} 1 & 1 & 0 & 0 \\ 1 & 0 & 0 & 1 \\ 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 \end{pmatrix} \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} = \begin{pmatrix} 10 \\ 7 \\ 4 \\ 11 \\ 4 \\ 3 \end{pmatrix} \\]
<p>We look for the largest value in the resulting vector, which is 11. The index of this maximum value is 4. Therefore, the first chosen index is \\(i_1 = 4\\). This means that column \\(\mathbf{x}_4\\) is the most important for explaining the signal \\(\bar{y}\\).</p>

<p><b>Step 2: Least Squares Solution</b><br>
We form a new basis matrix \\(\mathbf{X}_1\\) by augmenting the previous one with the newly selected column, \\(\mathbf{x}_4\\).</p>
\\[ \mathbf{X}_1 = [\mathbf{X}_0, \mathbf{x}_{i_1}] = [\mathbf{x}_4] = \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} \\]
<p>Now, we find the best possible approximation of \\(\bar{y}\\) using only the columns in \\(\mathbf{X}_1\\). This is a classic least squares problem: find \\(\bar{\theta}_1\\) that minimizes \\( ||\bar{y} - \mathbf{X}_1 \bar{\theta}_1||^2 \\). The solution is given by the pseudo-inverse formula:</p>
\\[ \bar{\theta}_1 = (\mathbf{X}_1^T \mathbf{X}_1)^{-1} \mathbf{X}_1^T \bar{y} \\]
<p>Let's compute this:</p>
\\[ \mathbf{X}_1^T \mathbf{X}_1 = \begin{pmatrix} 1 & 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} = 2 \\]
\\[ (\mathbf{X}_1^T \mathbf{X}_1)^{-1} = \frac{1}{2} \\]
\\[ \mathbf{X}_1^T \bar{y} = \begin{pmatrix} 1 & 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} = 11 \\]
\\[ \bar{\theta}_1 = \frac{1}{2} \times 11 = \frac{11}{2} \\]
<p>This is the estimated coefficient for the chosen column \\(\mathbf{x}_4\\).</p>

<p><b>Step 3: Residue Update</b><br>
We calculate the new residue \\(\bar{r}_1\\) by subtracting the approximation we just found from the original vector \\(\bar{y}\\).</p>
\\[ \bar{r}_1 = \bar{y} - \mathbf{X}_1 \bar{\theta}_1 = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} - \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} \left(\frac{11}{2}\right) = \begin{pmatrix} 7 - 11/2 \\ 3 - 0 \\ 4 - 11/2 \\ 0 - 0 \end{pmatrix} = \begin{pmatrix} 3/2 \\ 3 \\ -3/2 \\ 0 \end{pmatrix} \\]
<p>This completes the first iteration. The new residue \\(\bar{r}_1\\) represents the part of \\(\bar{y}\\) that is still unexplained.</p>

<h4>Iteration 2</h4>
<p>We repeat the same three steps, now using the residue from the previous iteration, \\(\bar{r}_1\\).</p>

<p><b>Step 1: Matching</b><br>
We find the column of \\(\mathbf{X}\\) that is most correlated with the new residue \\(\bar{r}_1\\).</p>
\\[ i_2 = \arg\max_{j} |\mathbf{x}_j^T \bar{r}_1| \\]
<p>Again, we compute \\(\mathbf{X}^T \bar{r}_1\\):</p>
\\[ \mathbf{X}^T \bar{r}_1 = \begin{pmatrix} 1 & 1 & 0 & 0 \\ 1 & 0 & 0 & 1 \\ 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 \end{pmatrix} \begin{pmatrix} 3/2 \\ 3 \\ -3/2 \\ 0 \end{pmatrix} = \begin{pmatrix} 9/2 \\ 3/2 \\ -3/2 \\ 0 \\ -3/2 \\ 3 \end{pmatrix} \\]
<p>The maximum absolute value in this vector is \\(9/2 = 4.5\\), which corresponds to the first index. Thus, the second chosen index is \\(i_2 = 1\\).</p>

<p><b>Step 2: Least Squares Solution</b><br>
We augment the basis matrix \\(\mathbf{X}_1\\) with the newly selected column \\(\mathbf{x}_1\\).</p>
\\[ \mathbf{X}_2 = [\mathbf{X}_1, \mathbf{x}_1] = [\mathbf{x}_4, \mathbf{x}_1] = \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \\ 0 & 0 \end{pmatrix} \\]
<p>We solve the least squares problem again to find the coefficients \\(\bar{\theta}_2\\) that best approximate \\(\bar{y}\\) using the columns in \\(\mathbf{X}_2\\).</p>
\\[ \bar{\theta}_2 = (\mathbf{X}_2^T \mathbf{X}_2)^{-1} \mathbf{X}_2^T \bar{y} \\]
<p>First, calculate \\(\mathbf{X}_2^T \mathbf{X}_2\\):</p>
\\[ \mathbf{X}_2^T \mathbf{X}_2 = \begin{pmatrix} 1 & 0 & 1 & 0 \\ 1 & 1 & 0 & 0 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \\]
<p>Next, find its inverse:</p>
\\[ (\mathbf{X}_2^T \mathbf{X}_2)^{-1} = \frac{1}{(2)(2) - (1)(1)} \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} = \frac{1}{3} \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \\]
<p>Now, calculate \\(\mathbf{X}_2^T \bar{y}\\):</p>
\\[ \mathbf{X}_2^T \bar{y} = \begin{pmatrix} 1 & 0 & 1 & 0 \\ 1 & 1 & 0 & 0 \end{pmatrix} \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} = \begin{pmatrix} 11 \\ 10 \end{pmatrix} \\]
<p>Finally, we compute \\(\bar{\theta}_2\\):</p>
\\[ \bar{\theta}_2 = \frac{1}{3} \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 11 \\ 10 \end{pmatrix} = \frac{1}{3} \begin{pmatrix} 22 - 10 \\ -11 + 20 \end{pmatrix} = \frac{1}{3} \begin{pmatrix} 12 \\ 9 \end{pmatrix} = \begin{pmatrix} 4 \\ 3 \end{pmatrix} \\]

<p><b>Step 3: Residue Update</b><br>
Calculate the final residue \\(\bar{r}_2\\):</p>
\\[ \bar{r}_2 = \bar{y} - \mathbf{X}_2 \bar{\theta}_2 = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} - \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 4 \\ 3 \end{pmatrix} = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} - \begin{pmatrix} 4+3 \\ 3 \\ 4 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix} \\]

<h4>Termination</h4>
<p>The residue \\(\bar{r}_2\\) is the zero vector. This means that we have found a perfect representation of \\(\bar{y}\\) using the chosen columns. The algorithm terminates. In practice, the algorithm would terminate when the norm of the residue falls below a small, pre-defined threshold, accounting for noise in the measurements.</p>

<h3>4. Constructing the Final Sparse Vector</h3>
<p>The algorithm has identified the non-zero coefficients and their corresponding locations.</p>
<ul>
    <li>The selected indices were \\(i_1 = 4\\) and \\(i_2 = 1\\).</li>
    <li>The final coefficient vector for the chosen basis \\(\mathbf{X}_2 = [\mathbf{x}_4, \mathbf{x}_1]\\) is \\(\bar{\theta}_2 = \begin{pmatrix} 4 \\ 3 \end{pmatrix}\\).</li>
</ul>
<p>To construct the final sparse vector \\(\bar{\theta} \in \mathbb{R}^6\\), we place the calculated coefficients at their correct positions:</p>
<ul>
    <li>The first element of \\(\bar{\theta}_2\\), which is 4, corresponds to the first column of \\(\mathbf{X}_2\\), which is \\(\mathbf{x}_4\\). So, the 4th element of \\(\bar{\theta}\\) is 4.</li>
    <li>The second element of \\(\bar{\theta}_2\\), which is 3, corresponds to the second column of \\(\mathbf{X}_2\\), which is \\(\mathbf{x}_1\\). So, the 1st element of \\(\bar{\theta}\\) is 3.</li>
</ul>
<p>All other elements of \\(\bar{\theta}\\) are zero. Therefore, the final sparse solution is:</p>
\\[ \bar{\theta} = \begin{pmatrix} 3 \\ 0 \\ 0 \\ 4 \\ 0 \\ 0 \end{pmatrix} \\]

<h3>5. Verification</h3>
<p>We can verify the solution by multiplying \\(\mathbf{X}\\) by our computed \\(\bar{\theta}\\) to see if we get back the original \\(\bar{y}\\).</p>
\\[ \mathbf{X}\bar{\theta} = \begin{pmatrix} 1 & 1 & 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 1 & 1 & 0 \\ 0 & 1 & 1 & 0 & 1 & 1 \end{pmatrix} \begin{pmatrix} 3 \\ 0 \\ 0 \\ 4 \\ 0 \\ 0 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 1 \\ 0 \\ 0 \end{pmatrix} + 4 \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 3+4 \\ 3+0 \\ 0+4 \\ 0+0 \end{pmatrix} = \begin{pmatrix} 7 \\ 3 \\ 4 \\ 0 \end{pmatrix} = \bar{y} \\]
<p>The result matches the original measurement vector, confirming that the OMP algorithm successfully found the correct 2-sparse solution to the underdetermined system.</p>
</div></div><div class="chapter" id="Lecture 54 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 54 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts and theories related to clustering, as presented in the transcript. The focus is on unsupervised machine learning, the definition of a cluster, its applications, and an introduction to the K-Means clustering algorithm.</p>

<b>1. Introduction to Clustering and Unsupervised Learning</b>
<p>
Clustering is a fundamental technique in machine learning (ML). It falls under a category of algorithms known as <b>unsupervised learning</b>.
</p>
<ul>
    <li><b>Unsupervised Learning:</b> This type of learning deals with data that has not been labeled or tagged. The goal is to analyze a large, unorganized dataset to discover hidden patterns, structures, and relationships without any prior guidance on what these patterns might be. The transcript states this means "there are no labels on data".</li>
    <li><b>Cluster:</b> A cluster is defined as a group of similar objects. The key idea is that objects within the same cluster share similar characteristics, while objects in different clusters are dissimilar. As the transcript puts it, a cluster is a "tight-knit group exhibiting some similar characteristic features." Visually, this can be imagined as groups of closely packed points in a multi-dimensional space.</li>
    <li><b>Clustering Algorithm:</b> The purpose of a clustering algorithm is to take this unlabeled data and automatically organize it into meaningful groups or clusters. It identifies these groups based on the intrinsic similarities between the data points.</li>
</ul>

<b>2. Applications of Clustering</b>
<p>
Clustering is a versatile tool with a wide range of applications across various fields. The transcript highlights several key areas:
</p>
<ul>
    <li><b>Data Analysis and Pattern Recognition:</b> At its core, clustering is used to find inherent groupings and structures in data, which is essential for analysis and recognizing patterns.</li>
    <li><b>Computer Vision and Image Segmentation:</b> A prominent application is in image processing.
        <ul>
            <li><b>Image Segmentation:</b> This is the process of partitioning a digital image into multiple segments or regions. The goal is to simplify the image into something that is more meaningful and easier to analyze. Clustering is used to group pixels with similar properties (like color, intensity, or texture).</li>
            <li><b>Examples:</b>
                <ol>
                    <li><b>Foreground-Background Separation:</b> In an image containing a person, the pixels can be grouped into two clusters: one for the person (foreground) and another for the rest of the image (background).</li>
                    <li><b>Geographical Feature Identification:</b> In a satellite image, pixels can be clustered to distinguish between different geographical features, such as land masses and bodies of water.</li>
                </ol>
            </li>
        </ul>
    </li>
    <li><b>Epidemiology:</b> When studying a disease outbreak, clustering can be used to group cases geographically. This helps identify "hotspots" or areas where the disease is spreading fastest, enabling targeted interventions to control the spread.</li>
    <li><b>Finance and Marketing:</b> Businesses can use clustering to analyze customer data. By grouping customers based on their purchasing habits or spending patterns, companies can create targeted marketing campaigns, offer personalized promotions, and understand market segments.</li>
</ul>

<b>3. K-Means Clustering Algorithm</b>
<p>
The transcript introduces K-Means as one of the simplest, most popular, and widely used clustering algorithms.
</p>
<ul>
    <li><b>The "K" in K-Means:</b> The letter 'K' is a parameter that represents the predefined <b>number of clusters</b> the algorithm will create. For example, if \\( K=2 \\), the algorithm will partition the data into two distinct clusters (binary clustering).</li>
    <li><b>The Goal:</b> Given a dataset, the K-Means algorithm aims to partition the data points into \\( K \\) different clusters in such a way that each data point belongs to the cluster with the nearest mean (or centroid).</li>
</ul>

<b>4. Key Concepts and Formulas in K-Means</b>
<p>
To understand how K-Means works, we need to define its core components:
</p>
<ul>
    <li><b>Data Set:</b> We start with a set of \\( M \\) data points, which are represented as N-dimensional vectors:
    \\[ \bar{x}_1, \bar{x}_2, \dots, \bar{x}_M \\]
    Here, \\( \bar{x}_j \\) is the \\( j \\)-th data vector.</li>

    <li><b>Clusters (\\( C_i \\)):</b> The data will be partitioned into \\( K \\) clusters, denoted as:
    \\[ C_1, C_2, \dots, C_K \\]
    Each \\( C_i \\) represents a set of data points that are grouped together.</li>

    <li><b>Centroids (\\( \bar{\mu}_i \\)):</b> Each cluster \\( C_i \\) is characterized by its center point, known as a <b>centroid</b>. The centroid is essentially the mean (average) of all the data points belonging to that cluster. It can be thought of as a representative or the "center of gravity" for the cluster. The centroids for the \\( K \\) clusters are denoted by:
    \\[ \bar{\mu}_1, \bar{\mu}_2, \dots, \bar{\mu}_K \\]
    where \\( \bar{\mu}_i \\) is the centroid of the \\( i \\)-th cluster, \\( C_i \\).</li>

    <li><b>Cluster Assignment Indicator (\\( \alpha_{ij} \\)):</b> This is a crucial parameter that indicates which cluster a particular data point belongs to. It is defined as follows:
    \\[ \alpha_{ij} = \begin{cases} 1 & \text{if data point } \bar{x}_j \text{ belongs to Cluster } i \\ 0 & \text{else} \end{cases} \\]
    <p>A fundamental assumption in K-Means is that each data point can belong to only <b>one</b> cluster. This means that for any given data point \\( \bar{x}_j \\), exactly one of the \\( \alpha_{ij} \\) values (for \\( i=1, 2, \dots, K \\)) will be 1, and all others will be 0.</p>
    <p><b>Example:</b> If we have \\( K=10 \\) clusters and we are considering the second data point (\\( j=2 \\)), and this point is assigned to the third cluster (\\( i=3 \\)), then:
    <ul>
        <li>\\( \alpha_{3,2} = 1 \\)</li>
        <li>\\( \alpha_{i,2} = 0 \\) for all \\( i \neq 3 \\) (i.e., for \\( i = 1, 2, 4, \dots, 10 \\)).</li>
    </ul>
    </li>
</ul>
<p>
Ultimately, the output of the K-Means clustering algorithm is the set of these assignment indicators, \\( \alpha_{ij} \\), for all data points and all clusters. In essence, the algorithm takes the initially unlabeled data and assigns a label (the cluster index) to each data point, thereby organizing the chaotic data into structured groups.
</p>
</div></div><div class="chapter" id="Lecture 55 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 55 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the K-means clustering algorithm, an unsupervised machine learning technique used for grouping unlabeled data. The explanation is based on the concepts, theories, and formulas presented in the transcript.</p>

<h3>1. Introduction to K-means Clustering</h3>
<p>K-means is a popular algorithm for <b>clustering</b>, which is the task of partitioning a dataset into groups, or "clusters." The goal is to group data points in such a way that points within the same cluster are more similar to each other than to those in other clusters. It is an example of <b>unsupervised learning</b> because it works with unlabeled data, meaning the algorithm must discover the underlying structure or groups on its own without any pre-defined categories.</p>

<p>The "K" in K-means refers to the number of clusters you want to find in the data, which is a parameter that must be specified beforehand. The algorithm aims to find the best placement for `K` centroids (or cluster centers) and assign each data point to the nearest centroid, thereby forming `K` distinct clusters.</p>

<h3>2. Core Concepts and Notation</h3>
<p>Before diving into the algorithm, let's define the key variables and parameters:</p>
<ul>
    <li><b>Number of Clusters (K)</b>: The predefined number of clusters to be formed.</li>
    <li><b>Data Points</b>: The dataset consists of `m` data points, represented as vectors: \\( \bar{x}_1, \bar{x}_2, \dots, \bar{x}_m \\). Each \\( \bar{x}_j \\) is a feature vector.</li>
    <li><b>Cluster Centroids (\\( \bar{\mu}_i \\))</b>: The center of a cluster `i`. There are `K` centroids, one for each cluster: \\( \bar{\mu}_1, \bar{\mu}_2, \dots, \bar{\mu}_K \\).</li>
    <li><b>Assignment Parameters (\\( \alpha_{ij} \\))</b>: These are binary parameters that indicate which cluster a data point belongs to.
    \\[ \alpha_{ij} = \begin{cases} 1 & \text{if } \bar{x}_j \text{ is assigned to cluster } i \\ 0 & \text{otherwise} \end{cases} \\]
    For any given point \\( \bar{x}_j \\), it can only belong to one cluster. Therefore, for a fixed `j`, the sum of \\( \alpha_{ij} \\) over all clusters `i` must be 1.</li>
</ul>

<h3>3. The K-means Cost Function</h3>
<p>The primary goal of the K-means algorithm is to find the cluster assignments (\\( \alpha_{ij} \\)) and centroids (\\( \bar{\mu}_i \\)) that minimize a specific cost function. This cost function, often called the distortion function, measures the total intra-cluster variation. It is defined as the sum of the squared Euclidean distances between each data point and the centroid of its assigned cluster.</p>

<p>The cost function `J` is given by:</p>
\\[ J = \sum_{i=1}^{K} \sum_{j=1}^{m} \alpha_{ij} \| \bar{x}_j - \bar{\mu}_i \|^2 \\]
<p><b>Explanation of the formula:</b></p>
<ul>
    <li>\\( \| \bar{x}_j - \bar{\mu}_i \|^2 \\) calculates the squared distance between data point `j` and centroid `i`.</li>
    <li>The \\( \alpha_{ij} \\) term acts as a switch. If point `j` is assigned to cluster `i` (\\( \alpha_{ij} = 1 \\)), its distance to centroid `i` is added to the total cost. If it's not assigned to cluster `i` (\\( \alpha_{ij} = 0 \\)), this term becomes zero.</li>
    <li>The summations add up these squared distances for all points across all clusters.</li>
</ul>
<p>Minimizing this function intuitively means making the clusters as compact as possible, ensuring that each point \\( \bar{x}_j \\) is close to the centroid \\( \bar{\mu}_i \\) of its own cluster.</p>

<h3>4. The Iterative K-means Algorithm</h3>
<p>K-means is an iterative algorithm that minimizes the cost function by repeatedly performing two main steps. Let's denote the iteration index by `l`.</p>

<h4>Step 0: Initialization</h4>
<p>The algorithm starts by initializing the `K` cluster centroids. This is typically done by randomly selecting `K` data points from the dataset to serve as the initial centroids. These initial centroids are denoted as \\( \bar{\mu}_1^{(0)}, \bar{\mu}_2^{(0)}, \dots, \bar{\mu}_K^{(0)} \\), where the superscript `(0)` indicates the initial (zeroth) iteration.</p>

<h4>The Iterative Loop (for \\( l=1, 2, \dots \\))</h4>
<p>The algorithm then enters a loop that alternates between two steps until convergence.</p>

<p><b>Step 1: Cluster Assignment (Expectation Step)</b></p>
<p>In this step, we assume the centroids are fixed from the previous iteration, \\( \bar{\mu}_i^{(l-1)} \\). The goal is to assign each data point \\( \bar{x}_j \\) to the closest centroid. This assignment is done by finding the cluster `i` that minimizes the distance \\( \| \bar{x}_j - \bar{\mu}_i^{(l-1)} \|^2 \\).</p>
<p>Mathematically, for each point \\( \bar{x}_j \\), we find the index of the closest centroid, \\( i_{\text{tilde}} \\), such that:</p>
\\[ i_{\text{tilde}} = \arg\min_{i} \| \bar{x}_j - \bar{\mu}_i^{(l-1)} \|^2 \\]
<p>We then update the assignment parameters for the current iteration `l`:</p>
\\[ \alpha_{i_{\text{tilde}}j}^{(l)} = 1 \quad \text{and} \quad \alpha_{ij}^{(l)} = 0 \quad \text{for all } i \neq i_{\text{tilde}} \\]
<p>This process is repeated for all data points \\( j = 1, \dots, m \\). This is known as a <b>hard partitioning</b> because each point is assigned exclusively to one cluster.</p>

<p><b>Step 2: Centroid Update (Maximization Step)</b></p>
<p>After assigning all points to clusters, we now have a fixed set of cluster assignments \\( \alpha_{ij}^{(l)} \\). The next step is to recompute the centroids for each cluster to better represent the points assigned to them. We update the centroids \\( \bar{\mu}_i \\) by minimizing the cost function with the new, fixed assignments.</p>
<p>For each cluster `i`, we want to find the \\( \bar{\mu}_i \\) that minimizes:</p>
\\[ \sum_{j=1}^{m} \alpha_{ij}^{(l)} \| \bar{x}_j - \bar{\mu}_i \|^2 \\]
<p>To find the minimum, we take the gradient of this expression with respect to \\( \bar{\mu}_i \\) and set it to zero. The expression can be expanded as:</p>
\\[ \sum_{j=1}^{m} \alpha_{ij}^{(l)} (\bar{x}_j - \bar{\mu}_i)^T (\bar{x}_j - \bar{\mu}_i) = \sum_{j=1}^{m} \alpha_{ij}^{(l)} (\bar{x}_j^T\bar{x}_j - 2\bar{\mu}_i^T\bar{x}_j + \bar{\mu}_i^T\bar{\mu}_i) \\]
<p>Taking the gradient with respect to \\( \bar{\mu}_i \\) and setting it to zero gives:</p>
\\[ \sum_{j=1}^{m} \alpha_{ij}^{(l)} (-2\bar{x}_j + 2\bar{\mu}_i) = 0 \\]
<p>Solving for \\( \bar{\mu}_i \\) yields the updated centroid for the `l`-th iteration, \\( \bar{\mu}_i^{(l)} \\):</p>
\\[ \bar{\mu}_i^{(l)} = \frac{\sum_{j=1}^{m} \alpha_{ij}^{(l)} \bar{x}_j}{\sum_{j=1}^{m} \alpha_{ij}^{(l)}} \\]
<p><b>Intuitive Meaning:</b> The new centroid of a cluster is simply the <b>mean (or average)</b> of all the data points assigned to that cluster. The numerator sums the vectors of all points in the cluster, and the denominator counts how many points are in the cluster.</p>

<h3>5. Convergence</h3>
<p>The algorithm repeats Steps 1 and 2 until it converges. Convergence is reached when the cluster assignments and the centroids no longer change in subsequent iterations. At this point, the algorithm has found a stable solution, which corresponds to a local minimum of the cost function. In practice, the algorithm is often stopped after a fixed number of iterations or when the change in the cost function falls below a small threshold.</p>

<h3>6. Simplified K-means Algorithm Summary</h3>
<p>Putting it all together, the K-means algorithm can be summarized in a more intuitive, non-mathematical way:</p>
<ol>
    <li><b>Initialize</b>: Randomly choose `K` initial cluster centroids.</li>
    <li><b>Repeat until convergence</b>:
        <ul>
            <li><b>Assign Points</b>: For each data point, calculate its distance to all `K` centroids and assign it to the cluster of the closest centroid.</li>
            <li><b>Update Centroids</b>: For each cluster, recalculate its centroid as the average (mean) of all the points that were just assigned to it.</li>
        </ul>
    </li>
</ol>
<p>This simple yet powerful iterative process is what makes K-means one of the most widely used clustering algorithms in machine learning.</p>
</div></div><h2>Weekly Summary</h2><div><p>This week's lectures cover three key applications of linear algebra in machine learning: Support Vector Machines (SVM), Sparse Regression, and K-Means Clustering.</p><p><b>1. Support Vector Machines (SVM)</b></p><p>The lectures introduce SVM as a powerful tool for <i>binary classification</i>, which involves partitioning data into two distinct classes (e.g., presence/absence of a disease). The focus is on linear classifiers, which use a mathematical object called a <i>hyperplane</i> to separate the data.</p><ul><li><b>Main Topics:</b><ul><li><b>Binary Classification:</b> The problem of classifying data into two categories, such as +1 or -1.</li><li><b>Hyperplanes:</b> An n-dimensional plane, defined by the equation \\(\mathbf{a}^T\mathbf{x} + b = 0\\), that divides the data space into two half-spaces. The vector \\(\mathbf{a}\\) is normal (perpendicular) to the hyperplane.</li><li><b>The SVM Objective:</b> Instead of just finding any separating hyperplane, SVM aims to find the one that maximizes the margin or "slab" between the two classes. This is achieved by finding two parallel hyperplanes, \\(\mathbf{a}^T\mathbf{x} + b = 1\\) and \\(\mathbf{a}^T\mathbf{x} + b = -1\\), and maximizing the distance between them.</li><li><b>Mathematical Formulation:</b> The distance between the two bounding hyperplanes is derived to be \\(2 / ||\mathbf{a}||\\). Therefore, to maximize this distance, one must minimize \\(||\mathbf{a}||\\).</li></ul></li><li><b>Key Takeaways:</b><br>The core idea of SVM is to create the most robust linear classifier by finding the hyperplane with the maximum possible margin between the two data classes. This translates into a solvable convex optimization problem: minimize \\(||\mathbf{a}||\\) subject to the constraint that all data points are correctly classified and lie outside the margin, i.e., \\(y_k(\mathbf{a}^T\mathbf{x}_k + b) \ge 1\\) for all data points \\(k\\).</li></ul><p><b>2. Sparse Regression and Compressive Sensing</b></p><p>This section moves from classification to regression, focusing on a special case called <i>sparse regression</i>. This is a form of linear regression where the model, \\(\mathbf{y} = \mathbf{X}\boldsymbol{\theta}\\), assumes that the parameter vector \\(\boldsymbol{\theta}\\) is <i>sparse</i>, meaning it has very few non-zero elements.</p><ul><li><b>Main Topics:</b><ul><li><b>Sparsity:</b> The assumption that a signal or parameter vector can be represented with very few non-zero coefficients. In regression, this means the outcome is explained by only a small subset of the available explanatory variables.</li><li><b>Compressive Sensing:</b> An application of sparse regression, particularly for underdetermined systems where the number of measurements (\\(m\\)) is much smaller than the number of unknown parameters (\\(n\\)). The standard least-squares solution is not applicable here.</li><li><b>Orthogonal Matching Pursuit (OMP):</b> A greedy, iterative algorithm to solve the sparse regression problem. It works by sequentially selecting the column of \\(\mathbf{X}\\) that is most correlated with the current residual (the part of \\(\mathbf{y}\\) not yet explained).</li><li><b>OMP Example:</b> A detailed numerical example demonstrates the OMP algorithm step-by-step: (1) finding the column with the maximum projection on the residue, (2) augmenting the basis with this column, (3) solving a least-squares problem with the current basis, and (4) updating the residue. The process is repeated until the residue is sufficiently small.</li></ul></li><li><b>Key Takeaways:</b><br>When faced with an underdetermined system, it is possible to recover the parameter vector \\(\boldsymbol{\theta}\\) if it is known to be sparse. The Orthogonal Matching Pursuit (OMP) algorithm provides a practical, greedy method to identify the few important explanatory variables and estimate their coefficients, effectively solving the sparse regression problem.</li></ul><p><b>3. Clustering with the K-Means Algorithm</b></p><p>The final topic covers <i>clustering</i>, a fundamental technique in <i>unsupervised learning</i> where the goal is to group unlabeled data points into meaningful clusters based on similarity.</p><ul><li><b>Main Topics:</b><ul><li><b>Unsupervised Learning:</b> A class of machine learning algorithms that work with data that has not been labeled or categorized. Clustering is a primary example.</li><li><b>The K-Means Algorithm:</b> An iterative algorithm that partitions a dataset into a pre-determined number (K) of clusters.</li><li><b>Centroids:</b> Each cluster is represented by a central point, or <i>centroid</i>, which is the mean of all the points in that cluster.</li><li><b>The K-Means Process:</b> The algorithm alternates between two main steps:<br>1. <b>Assignment Step:</b> Each data point is assigned to the cluster whose centroid is the closest (based on Euclidean distance).<br>2. <b>Update Step:</b> The centroid of each cluster is recalculated as the average of all points currently assigned to it.</li></ul></li><li><b>Key Takeaways:</b><br>K-Means is a simple yet powerful algorithm for partitioning data. It aims to minimize the within-cluster sum of squares (the total squared distance between each point and its assigned centroid). The algorithm iteratively refines cluster assignments and centroids until they stabilize, effectively grouping similar data points together without any prior labels.</li></ul></div><h2>Assignment Explanation</h2><div>
<p><b>Question 1: Circulant Matrix for ISI Channel</b></p>
<p>
The question asks to construct a circulant matrix for an Inter Symbol Interference (ISI) channel. A circulant matrix is a special type of Toeplitz matrix where each row is a cyclic shift of the row above it. Given the channel taps \\( h = [h(0), h(1), h(2), h(3)] = [-2, -1, 3, -3] \\) and \\(N=4\\) subcarriers, the corresponding \\(4 \times 4\\) circulant matrix \\(H\\) is constructed as follows:
<br>
The first row of the matrix is the vector of channel taps \\( [h(0), h(1), h(2), h(3)] \\).
<br>
Each subsequent row is the right cyclic shift of the row above it.
</p>
<ul>
    <li><b>Row 1:</b> \\([-2, -1, 3, -3]\\)</li>
    <li><b>Row 2 (shift Row 1):</b> \\([-3, -2, -1, 3]\\)</li>
    <li><b>Row 3 (shift Row 2):</b> \\([3, -3, -2, -1]\\)</li>
    <li><b>Row 4 (shift Row 3):</b> \\([-1, 3, -3, -2]\\)</li>
</ul>
<p>
This results in the matrix:
\\[
H =
\begin{bmatrix}
-2 & -1 & 3 & -3 \\
-3 & -2 & -1 & 3 \\
3 & -3 & -2 & -1 \\
-1 & 3 & -3 & -2
\end{bmatrix}
\\]
This matches the fourth option, which is the correct answer.
</p>
<hr>

<p><b>Question 2: Matrix Exponential</b></p>
<p>
This question asks for the matrix exponential \\(e^{tH}\\) of the matrix \\(H = \begin{bmatrix} 0 & 0 \\ -1 & 0 \end{bmatrix}\\).
<br>
The matrix exponential is defined by the Taylor series expansion:
\\[ e^{tH} = I + tH + \frac{(tH)^2}{2!} + \frac{(tH)^3}{3!} + \dots \\]
First, we compute the powers of \\(H\\):
\\[ H^2 = H \cdot H = \begin{bmatrix} 0 & 0 \\ -1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ -1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} = \mathbf{0} \\]
Since \\(H^2\\) is the zero matrix, all higher powers (\\(H^3, H^4, \dots\\)) will also be zero. Such a matrix is called nilpotent.
<br>
The Taylor series therefore truncates after the first two terms:
\\[ e^{tH} = I + tH \\]
Substituting the matrices for \\(I\\) and \\(H\\):
\\[ e^{tH} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + t \begin{bmatrix} 0 & 0 \\ -1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ -t & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ -t & 1 \end{bmatrix} \\]
This is the correct answer.
</p>
<hr>

<p><b>Question 3: Bias in User-Rating Matrix</b></p>
<p>
The question asks to calculate the bias \\(r_a\\) from a given user-rating matrix. In the context of recommender systems, the bias (often denoted as \\(\mu\\)) is the overall average of all known ratings.
<br>
The given ratings are: 2, 4, 3, 3, 3, 3, 4, 2.
<br>
To find the average, we sum all the ratings and divide by the number of ratings.
</p>
<ul>
    <li><b>Sum of all ratings:</b> \\( 2 + 4 + 3 + 3 + 3 + 3 + 4 + 2 = 24 \\)</li>
    <li><b>Number of ratings:</b> 8</li>
    <li><b>Average rating (bias):</b> \\( r_a = \frac{24}{8} = 3 \\)</li>
</ul>
<p>
The bias \\(r_a\\) is 3.
</p>
<hr>

<p><b>Question 4: OFDM Technology</b></p>
<p>
The question asks how Orthogonal Frequency Division Multiplexing (OFDM) enables high-speed data transmission in systems like 4G and 5G.
<br>
The main challenge in high-speed wireless communication is multipath fading, which causes Inter-Symbol Interference (ISI). ISI occurs when delayed copies of a transmitted symbol interfere with subsequent symbols. OFDM combats this by dividing a high-rate data stream into many lower-rate streams, which are transmitted in parallel on different orthogonal subcarrier frequencies.
<br>
By doing this, the symbol duration on each subcarrier becomes much longer than the delay spread of the channel. This significantly reduces the effect of ISI. Additionally, a cyclic prefix is added to each OFDM symbol to further eliminate ISI and maintain subcarrier orthogonality.
<br>
Therefore, the primary benefit of OFDM is its ability to effectively handle and eliminate ISI, allowing for reliable high-speed data transmission. The correct answer is "by eliminating Inter Symbol Interference (ISI)".
</p>
<hr>

<p><b>Question 5: Support Vector Machines (SVMs)</b></p>
<p>
The question asks about the application of Support Vector Machines (SVMs).
<br>
SVMs are a powerful set of supervised learning models used for data analysis. Their most prominent and well-known application is for <b>classification</b> tasks. An SVM classifier works by finding the hyperplane that best separates data points belonging to different classes in a high-dimensional space. The "best" separation is achieved by maximizing the margin, or the distance between the hyperplane and the nearest data points from each class.
<br>
While SVMs can also be adapted for regression tasks (known as Support Vector Regression or SVR), their primary and most common use is classification.
</p>
<hr>

<p><b>Question 6: SC-FDMA Technology</b></p>
<p>
The question asks about the application of Single-Carrier Frequency-Division Multiple Access (SC-FDMA) technology.
<br>
SC-FDMA is a multiple access scheme that has a key advantage over its close relative, OFDMA: a lower Peak-to-Average Power Ratio (PAPR). A high PAPR requires expensive and power-inefficient linear amplifiers. For mobile devices (user equipment), power efficiency is critical for maximizing battery life.
<br>
Because of its low PAPR property, SC-FDMA was chosen as the multiple access scheme for the <b>uplink of 4G systems</b> (i.e., for transmissions from the mobile phone to the base station). The downlink (base station to mobile) uses OFDMA, as power efficiency is less of a concern for the base station.
</p>
<hr>

<p><b>Question 7: IFFT Matrix</b></p>
<p>
The question asks to identify the \\(4 \times 4\\) Inverse Fast Fourier Transform (IFFT) matrix.
<br>
The \\(N \times N\\) Inverse Discrete Fourier Transform (IDFT) matrix, \\(F^{-1}\\), is defined by its elements:
\\[ (F^{-1})_{k,n} = \frac{1}{N} e^{j \frac{2\pi kn}{N}} \\]
For \\(N=4\\), the primitive root of unity is \\(W_4 = e^{-j2\pi/4} = -j\\). The IDFT matrix uses the conjugate, so we need powers of \\(W_4^{-1} = e^{j2\pi/4} = j\\).
<br>
The matrix is constructed as:
\\[ F^{-1} = \frac{1}{4} \begin{bmatrix}
(W_4^{-1})^0 & (W_4^{-1})^0 & (W_4^{-1})^0 & (W_4^{-1})^0 \\
(W_4^{-1})^0 & (W_4^{-1})^1 & (W_4^{-1})^2 & (W_4^{-1})^3 \\
(W_4^{-1})^0 & (W_4^{-1})^2 & (W_4^{-1})^4 & (W_4^{-1})^6 \\
(W_4^{-1})^0 & (W_4^{-1})^3 & (W_4^{-1})^6 & (W_4^{-1})^9
\end{bmatrix} \\]
Substituting the values \\(j^0=1, j^1=j, j^2=-1, j^3=-j, j^4=1, \dots\\):
\\[ F^{-1} = \frac{1}{4} \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & j & -1 & -j \\
1 & -1 & 1 & -1 \\
1 & -j & -1 & j
\end{bmatrix} \\]
This matches the first option.
</p>
<hr>

<p><b>Question 8: Matrix Exponential</b></p>
<p>
This question asks for the matrix exponential \\(e^{tH}\\) where \\(H = \omega \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\\).
<br>
The matrix \\(\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\\) is a generator of rotations. The matrix exponential \\(e^{tH}\\) corresponds to a rotation matrix.
<br>
We can compute this using the Taylor series. Let's find the powers of \\(H\\):
\\[ H = \begin{bmatrix} 0 & -\omega \\ \omega & 0 \end{bmatrix} \\]
\\[ H^2 = \begin{bmatrix} 0 & -\omega \\ \omega & 0 \end{bmatrix} \begin{bmatrix} 0 & -\omega \\ \omega & 0 \end{bmatrix} = \begin{bmatrix} -\omega^2 & 0 \\ 0 & -\omega^2 \end{bmatrix} = -\omega^2 I \\]
\\[ H^3 = -\omega^2 H, \quad H^4 = \omega^4 I, \quad \dots \\]
The Taylor series for \\(e^{tH}\\) is:
\\[ e^{tH} = I + tH + \frac{t^2H^2}{2!} + \frac{t^3H^3}{3!} + \dots \\]
\\[ e^{tH} = I + tH - \frac{t^2\omega^2 I}{2!} - \frac{t^3\omega^2 H}{3!} + \dots \\]
Grouping terms with \\(I\\) and \\(H\\):
\\[ e^{tH} = I \left(1 - \frac{(\omega t)^2}{2!} + \frac{(\omega t)^4}{4!} - \dots \right) + \frac{H}{\omega} \left(\omega t - \frac{(\omega t)^3}{3!} + \frac{(\omega t)^5}{5!} - \dots \right) \\]
The series in the parentheses are the Taylor series for \\(\cos(\omega t)\\) and \\(\sin(\omega t)\\), respectively.
\\[ e^{tH} = I \cos(\omega t) + \frac{H}{\omega} \sin(\omega t) \\]
Substituting \\(I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\\) and \\(\frac{H}{\omega} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\\):
\\[ e^{tH} = \begin{bmatrix} \cos(\omega t) & 0 \\ 0 & \cos(\omega t) \end{bmatrix} + \begin{bmatrix} 0 & -\sin(\omega t) \\ \sin(\omega t) & 0 \end{bmatrix} = \begin{bmatrix} \cos(\omega t) & -\sin(\omega t) \\ \sin(\omega t) & \cos(\omega t) \end{bmatrix} \\]
This is the standard 2D rotation matrix.
</p>
<hr>

<p><b>Question 9: Operations in OFDM</b></p>
<p>
The question asks about the core signal processing operations in an OFDM system.
<br>
OFDM works by transmitting data in the frequency domain. Each data symbol is assigned to modulate a specific orthogonal subcarrier.
</p>
<ul>
    <li><b>At the transmitter:</b> The set of frequency-domain symbols (one for each subcarrier) needs to be converted into a time-domain signal for transmission over the air. This transformation from the frequency domain to the time domain is performed by the <b>Inverse Fast Fourier Transform (IFFT)</b>.</li>
    <li><b>At the receiver:</b> The incoming time-domain signal is captured. To recover the data symbols from each subcarrier, the signal must be transformed back to the frequency domain. This is accomplished using the <b>Fast Fourier Transform (FFT)</b>.</li>
</ul>
<p>
Therefore, the correct sequence of operations is IFFT at the transmitter and FFT at the receiver.
</p>
<hr>

<p><b>Question 10: Linear Minimum Mean Square Error (LMMSE) Estimate</b></p>
<p>
The question asks for the LMMSE estimate of a vector \\(\bar{x}\\) from a measurement \\(\bar{y}\\) based on the linear model \\(\bar{y} = H\bar{x} + \bar{n}\\).
<br>
The formula for the LMMSE estimator is:
\\[ \hat{x} = (H^T R_{nn}^{-1} H + R_{xx}^{-1})^{-1} H^T R_{nn}^{-1} \bar{y} \\]
We are given:
\\[ H = \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \\ 1 & -1 \end{bmatrix}, \quad \bar{y} = \begin{bmatrix} -1 \\ 3 \\ -2 \\ 1 \end{bmatrix}, \quad R_{xx} = \frac{1}{2}I, \quad R_{nn} = I \\]
From the covariances, we get \\(R_{nn}^{-1} = I\\) and \\(R_{xx}^{-1} = 2I\\).
<br>
The formula simplifies to:
\\[ \hat{x} = (H^T H + 2I)^{-1} H^T \bar{y} \\]
Let's compute the terms:
<br>
1.  \\(H^T H\\):
    \\[ H^T H = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & -1 & 1 & -1 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \\ 1 & -1 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} = 4I \\]
2.  \\((H^T H + 2I)^{-1}\\):
    \\[ (4I + 2I)^{-1} = (6I)^{-1} = \frac{1}{6}I \\]
3.  \\(H^T \bar{y}\\):
    \\[ H^T \bar{y} = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & -1 & 1 & -1 \end{bmatrix} \begin{bmatrix} -1 \\ 3 \\ -2 \\ 1 \end{bmatrix} = \begin{bmatrix} -1+3-2+1 \\ -1-3-2-1 \end{bmatrix} = \begin{bmatrix} 1 \\ -7 \end{bmatrix} \\]
4.  Finally, combine the results:
    \\[ \hat{x} = \frac{1}{6}I \cdot \begin{bmatrix} 1 \\ -7 \end{bmatrix} = \frac{1}{6}\begin{bmatrix} 1 \\ -7 \end{bmatrix} \\]
<p>
<i>Note: The calculated answer based on the problem's values is \\(\frac{1}{6}\begin{bmatrix} 1 \\ -7 \end{bmatrix}\\). However, this is not among the options. The accepted answer is \\(\frac{1}{6}\begin{bmatrix} 1 \\ 1 \end{bmatrix}\\). This indicates a likely typo in the question's provided vector \\(\bar{y}\\). For the accepted answer to be correct, the term \\(H^T \bar{y}\\) would need to be \\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\\). This would happen, for instance, if \\(\bar{y} = [3, 1, -2, -1]^T\\). Assuming the typo and following the logic to the given answer, we select \\(\frac{1}{6}\begin{bmatrix} 1 \\ 1 \end{bmatrix}\\).</i>
</p>
</div></div><div class="week" id="week_11"><h1 class="week-title">Week 11</h1><h2>Transcript Explanations</h2><div class="chapter" id="Lecture 56 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 56 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts from the transcript concerning stochastic processes, particularly focusing on Discrete-Time Markov Chains (DTMCs). The explanation breaks down the core ideas, theories, and the mathematical notation used.</p>

<b><h3>1. Introduction to Stochastic Processes</h3></b>
<p>A <b>stochastic process</b> is a mathematical and probabilistic model used to describe a system that evolves randomly over time. Instead of the system's future state being completely predictable, it is described in terms of probabilities.</p>

<p><b>Key Characteristics:</b></p>
<ul>
    <li><b>Randomness:</b> The evolution of the system has an element of chance. For example, we cannot predict tomorrow's exact stock price with 100% certainty.</li>
    <li><b>Temporal Correlation:</b> The future state, while random, is often correlated with its past states. For instance, tomorrow's temperature is likely to be close to today's temperature. It is highly unlikely to jump from 35°C to 50°C overnight. The history of the process provides valuable information for predicting its future behavior.</li>
</ul>

<p><b>Examples of Stochastic Processes:</b></p>
<ul>
    <li>The daily temperature of a city.</li>
    <li>The price of a stock or a stock market index over time.</li>
    <li>The inventory level of a product in a warehouse.</li>
    <li>The status of a machine in a factory (e.g., functional or faulty).</li>
</ul>

<b><h3>2. Modeling Stochastic Processes</h3></b>
<p>A stochastic process can be modeled as a time series, which is a sequence of observations at different points in time.</p>

<p><b>Discrete-Time vs. Continuous-Time Processes:</b></p>
<ul>
    <li><b>Discrete-Time Stochastic Process:</b> The state of the system is observed at distinct, separate points in time. This is represented by a sequence \\(x_n\\), where \\(n\\) is an integer index representing time steps (e.g., \\(n = 0, 1, 2, \ldots\\)). The time unit for \\(n\\) can be hours, days, seconds, etc. This is the focus of the transcript.</li>
    <li><b>Continuous-Time Stochastic Process:</b> The state of the system is observed at every instant over a continuous time interval. This is represented by \\(x(t)\\), where \\(t\\) is a real number.</li>
</ul>

<p><b>State and State Space:</b></p>
<ul>
    <li>The value \\(x_n\\) is called the <b>state</b> of the system at time \\(n\\).</li>
    <li>The set of all possible values that \\(x_n\\) can take is called the <b>state space</b>, denoted by \\(S\\).</li>
</ul>

<p>The state space can be infinite or finite. For practical analysis, an infinite state space is often simplified into a finite one through a process called <b>quantization</b> or <b>discretization</b>. For example:</p>
<ul>
    <li><b>Temperature:</b> The theoretical state space might be a continuous interval, like \\([-10°C, 50°C]\\). For practical purposes, this can be quantized into discrete values with a step size of 0.1°C, resulting in a finite state space: \\(\{ \ldots, 49.8, 49.9, 50.0 \}\\).</li>
    <li><b>Stock Index:</b> The theoretical state space could be \\([0, \infty)\\). This can be practically modeled by a finite set of values in a specific range with a fixed step size, for example, \\(\{ \ldots, 48000, 48010, 48020, \ldots \}\\).</li>
</ul>
<p>Using a finite state space makes the analysis of the system more manageable.</p>

<b><h3>3. The Markov Property and Markov Chains</h3></b>
<p>The central goal in analyzing a stochastic process is to characterize its evolution, specifically to predict the future state \\(x_{n+1}\\) based on its past history \\((x_0, x_1, \ldots, x_n)\\). This prediction is made in a probabilistic sense.</p>

<p>A key simplifying assumption in many models is the <b>Markov property</b>. This property states that the future evolution of the process depends <i>only</i> on its current state, and not on the sequence of events that preceded it. In other words, the current state captures all the information from the past that is relevant for predicting the future.</p>

<p>Mathematically, for any state \\(s_j\\) in the state space \\(S\\), the Markov property is defined as:</p>
\\[ P(x_{n+1} = s_j \mid x_n, x_{n-1}, \ldots, x_0) = P(x_{n+1} = s_j \mid x_n) \\]
<p><b>Intuition:</b> Given the present state \\(x_n\\), the past \\((x_{n-1}, \ldots, x_0)\\) becomes irrelevant for predicting the future state \\(x_{n+1}\\). The future is "decoupled" from the past by the present.</p>

<p>A discrete-time stochastic process that satisfies the Markov property is called a <b>Discrete-Time Markov Chain (DTMC)</b>.</p>

<b><h3>4. Time-Homogeneous Markov Chains</h3></b>
<p>In many practical scenarios, the rules governing the system's evolution do not change over time. This leads to the concept of time homogeneity.</p>

<p>A DTMC is said to be <b>time-homogeneous</b> (or stationary) if the probability of transitioning from one state to another does not depend on the specific time \\(n\\) at which the transition occurs. The probability only depends on the starting and ending states.</p>

<p>Mathematically, this means that the conditional probability \\(P(x_{n+1} = s_j \mid x_n = s_i)\\) is the same for all time steps \\(n\\). For example:</p>
\\[ P(x_1 = s_j \mid x_0 = s_i) = P(x_2 = s_j \mid x_1 = s_i) = P(x_3 = s_j \mid x_2 = s_i) = \ldots \\]

<b><h3>5. One-Step Transition Probabilities</h3></b>
<p>For a time-homogeneous DTMC, the probability of moving from state \\(s_i\\) to state \\(s_j\\) in a single time step is a constant value. This constant is called the <b>one-step transition probability</b> and is denoted by \\(P_{ij}\\).</p>

<p>The formula is:</p>
\\[ P_{ij} = P(x_{n+1} = s_j \mid x_n = s_i) \\]
<p>This value represents the probability of the system being in state \\(s_j\\) at the next time step, given that it is currently in state \\(s_i\\). Since the chain is time-homogeneous, this probability does not depend on \\(n\\).</p>

<p>These one-step transition probabilities are fundamental because they completely characterize the dynamic behavior of the DTMC. By collecting all these probabilities into a matrix (the transition probability matrix), one can analyze the long-term behavior of the system, make predictions, and understand its properties, which is a core application of linear algebra in this field.</p>

</div></div><div class="chapter" id="Lecture 57 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 57 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts related to Discrete-Time Markov Chains (DTMCs) as presented in the transcript. The focus is on one-step transition probabilities, the transition probability matrix, and their properties, illustrated with examples.</p>

<b>1. One-Step Transition Probability</b>
<p>The core concept for describing the dynamics of a Markov chain is the <b>one-step transition probability</b>. This is the probability that the process will be in a specific state \\( S_j \\) at the next time step \\( (n+1) \\), given that it is currently in state \\( S_i \\) at time step \\( n \\).</p>
<p>Mathematically, this conditional probability is expressed as:</p>
\\[ P(X_{n+1} = S_j | X_n = S_i) \\]
<p>where:</p>
<ul>
<li>\\( X_n \\) is the random variable representing the state of the process at time \\( n \\).</li>
<li>\\( S_i \\) and \\( S_j \\) are states from the state space.</li>
</ul>

<p>A key assumption for the models discussed is that they are <b>time-homogeneous</b>. This means the transition probability does not depend on the specific time step \\( n \\), only on the starting and ending states. Therefore, the probability of moving from state \\( S_i \\) to \\( S_j \\) is the same at any point in time.</p>
\\[ P(X_{n+1} = S_j | X_n = S_i) = P(X_1 = S_j | X_0 = S_i) \\]
<p>Due to this property, we can use a simpler notation for the one-step transition probability:</p>
\\[ p_{ij} = P(X_{n+1} = S_j | X_n = S_i) \\]
<p>Here, \\( p_{ij} \\) represents the probability of transitioning from state \\( i \\) to state \\( j \\) in a single time step.</p>

<b>2. The Transition Probability Matrix (TPM)</b>
<p>To characterize the entire DTMC, we can organize all the one-step transition probabilities into a matrix known as the <b>transition probability matrix</b>, denoted by \\( \mathbf{P} \\). If the system has \\( N \\) possible states, this will be an \\( N \times N \\) matrix.</p>
<p>The element in the \\( i \\)-th row and \\( j \\)-th column of the matrix \\( \mathbf{P} \\) is the transition probability \\( p_{ij} \\).</p>
\\[ \mathbf{P} = \begin{pmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \end{pmatrix} \\]
<p>The structure of this matrix has a clear interpretation:</p>
<ul>
<li><b>Rows</b> correspond to the <b>starting state</b> (the state at time \\( n \\)). For example, the first row contains all the probabilities for transitions <i>out of</i> state 1.</li>
<li><b>Columns</b> correspond to the <b>ending state</b> (the state at time \\( n+1 \\)). For example, the first column contains all probabilities for transitions <i>into</i> state 1.</li>
</ul>
<p>For instance, the element \\( p_{21} \\) represents the probability of moving from state 2 to state 1 in one step:</p>
\\[ p_{21} = P(X_{n+1} = S_1 | X_n = S_2) \\]

<b>3. Properties of the Transition Probability Matrix</b>
<p>Since the elements of \\( \mathbf{P} \\) are probabilities, the matrix must satisfy two fundamental properties:</p>
<ol>
    <li><b>Non-negativity:</b> Every element of the matrix must be a valid probability, meaning it must be greater than or equal to zero.
        \\[ p_{ij} \ge 0 \quad \text{for all } i, j \\]
    </li>
    <li><b>Rows Sum to One:</b> If the process is in state \\( S_i \\), it must transition to <i>some</i> state in the next step (including possibly staying in state \\( S_i \\)). Therefore, the sum of all transition probabilities from a given starting state \\( S_i \\) must be equal to 1. This means the sum of the elements in each row of the matrix \\( \mathbf{P} \\) must be 1.
        \\[ \sum_{j=1}^{n} p_{ij} = 1 \quad \text{for each row } i = 1, 2, \ldots, n \\]
    </li>
</ol>

<b>4. State Transition Diagram</b>
<p>A <b>state transition diagram</b> is a graphical tool used to visualize a DTMC. It provides a more intuitive representation than the matrix, especially for understanding the flow of the process.</p>
<ul>
    <li>Each state is represented by a node (e.g., a circle).</li>
    <li>A directed edge (an arrow) from state \\( S_i \\) to state \\( S_j \\) represents a possible transition.</li>
    <li>The arrow is labeled with the corresponding transition probability \\( p_{ij} \\).</li>
    <li>If \\( p_{ij} = 0 \\), no arrow is drawn from state \\( i \\) to state \\( j \\).</li>
</ul>
<p>This diagram is very convenient for visualizing the behavior of the Markov chain, while the matrix is more suitable for mathematical computations.</p>

<b>Example 1: A 3-State DTMC</b>
<p>Consider a DTMC with 3 states and the following transition probability matrix:</p>
\\[ \mathbf{P} = \begin{pmatrix} 0.35 & 0.35 & 0.3 \\ 0.2 & 0.7 & 0.1 \\ 0.4 & 0 & 0.6 \end{pmatrix} \\]
<p>Here:</p>
<ul>
    <li>\\( p_{23} = 0.1 \\) is the probability of moving from state 2 to state 3 in one step: \\( P(X_{n+1}=3 | X_n=2) = 0.1 \\).</li>
    <li>\\( p_{32} = 0 \\) indicates that it is impossible to transition directly from state 3 to state 2.</li>
</ul>
<p>The state transition diagram for this matrix would look like this:</p>
<center>
    <img src="https://i.imgur.com/8Qn7l6w.png" alt="State Transition Diagram for 3-State DTMC" width="450">
</center>
<p>In the diagram: S1, S2, and S3 are the states. The arrows show the direction of transition and are labeled with their probabilities. For example, the arrow from S2 to S3 has the label 0.1, representing \\(p_{23}\\). The loop on S2 represents \\(p_{22} = 0.7\\).</p>


<b>Example 2: Industrial Reliability Model</b>
<p>This example models a machine that can be in one of two states:</p>
<ul>
    <li>\\( S_1 \\): Operational</li>
    <li>\\( S_2 \\): Faulty</li>
</ul>
<p>The transition probabilities are defined as follows:</p>
<ul>
    <li>The probability that an operational machine remains operational in the next time step is 0.95.
        \\[ p_{11} = P(X_{n+1} = S_1 | X_n = S_1) = 0.95 \\]
    </li>
    <li>The probability of breakdown (operational to faulty) must be \\( 1 - p_{11} \\), since the machine must either stay operational or become faulty.
        \\[ p_{12} = P(X_{n+1} = S_2 | X_n = S_1) = 1 - 0.95 = 0.05 \\]
    </li>
    <li>The probability that a faulty machine remains faulty is 0.10.
        \\[ p_{22} = P(X_{n+1} = S_2 | X_n = S_2) = 0.10 \\]
    </li>
    <li>The probability of repair (faulty to operational) is \\( 1 - p_{22} \\).
        \\[ p_{21} = P(X_{n+1} = S_1 | X_n = S_2) = 1 - 0.10 = 0.90 \\]
    </li>
</ul>
<p>These probabilities are collected into a \\( 2 \times 2 \\) transition probability matrix:</p>
\\[ \mathbf{P} = \begin{pmatrix} 0.95 & 0.05 \\ 0.90 & 0.10 \end{pmatrix} \\]
<p>Here, the first row describes the transitions from the "Operational" state, and the second row describes transitions from the "Faulty" state. Both rows sum to 1, as required.</p>
</div></div><div class="chapter" id="Lecture 58 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 58 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts and examples of Discrete Time Markov Chains (DTMCs) presented in the transcript. The focus is on modeling systems with multiple components and systems with boundary conditions.</p>

<b>1. Industrial Reliability with Two Independent Machines</b>
<p>The transcript extends a previous example of a single machine's reliability to a more complex system involving two machines. The core principles for this extension are the concept of a multi-component state space and the probabilistic rule for independent events.</p>

<b>a. Key Assumption: Independence</b>
<p>A crucial assumption is that the two machines operate <b>independently</b>. This means that the breakdown or repair of one machine has no influence on the status of the other. In probability theory, if two events, A and B, are independent, the probability of both events occurring is the product of their individual probabilities. This is a fundamental rule used throughout the calculations.</p>
\\[ P(A \cap B) = P(A) \times P(B) \\]

<b>b. Defining the State Space</b>
<p>With two machines, the state of the system is determined by the status of both. The transcript notes that since the machines are similar (interchangeable), we only care about the <i>number</i> of machines in each state, not <i>which specific machine</i>. This simplifies the state space to three possible states:</p>
<ul>
    <li><b>S1:</b> Both machines are operational.</li>
    <li><b>S2:</b> One machine is operational, and one is faulty.</li>
    <li><b>S3:</b> Both machines are faulty.</li>
</ul>

<b>c. Calculating One-Step Transition Probabilities</b>
<p>The goal is to build a 3x3 transition probability matrix, \\(P\\), where each element \\(P_{ij}\\) is the probability of moving from state \\(S_i\\) to state \\(S_j\\) in one time step (one hour). The transcript illustrates the calculation for two specific transitions.</p>

<p><b>i. Transition from S1 to S1 (Both Operational to Both Operational)</b></p>
<p>This transition, denoted as \\(P_{11}\\), occurs if Machine 1 remains operational AND Machine 2 remains operational. The probability for a single machine to remain operational is given as 0.95. Due to independence, we multiply the probabilities:</p>
\\[ P_{11} = P(X_{n+1}=S_1 | X_n=S_1) \\]
\\[ P_{11} = P(\text{M1 stays op}) \times P(\text{M2 stays op}) \\]
\\[ P_{11} = 0.95 \times 0.95 = 0.9025 \\]

<p><b>ii. Transition from S3 to S2 (Both Faulty to One Operational/One Faulty)</b></p>
<p>This transition, \\(P_{32}\\), is more complex. It can happen in two mutually exclusive ways:</p>
<ol>
    <li>Machine 1 is repaired (Faulty → Operational) AND Machine 2 remains faulty (Faulty → Faulty).</li>
    <li>Machine 1 remains faulty (Faulty → Faulty) AND Machine 2 is repaired (Faulty → Operational).</li>
</ol>
<p>From the single-machine data, we know:</p>
<ul>
    <li>Probability of repair (Faulty → Op) = 0.9</li>
    <li>Probability of staying faulty (Faulty → Faulty) = 0.1</li>
</ul>
<p>Since the two scenarios are mutually exclusive, we add their probabilities. For each scenario, we multiply the probabilities of the machine events because they are independent.</p>
\\[ P_{32} = P(X_{n+1}=S_2 | X_n=S_3) \\]
\\[ P_{32} = [P(\text{M1 repaired}) \times P(\text{M2 stays faulty})] + [P(\text{M1 stays faulty}) \times P(\text{M2 repaired})] \\]
\\[ P_{32} = (0.9 \times 0.1) + (0.1 \times 0.9) \\]
\\[ P_{32} = 0.09 + 0.09 = 0.18 \\]

<p>By computing all nine such probabilities, we can construct the full transition probability matrix for this system.</p>

<b>d. The Transition Probability Matrix (P)</b>
<p>The resulting 3x3 matrix, which characterizes the entire dynamic behavior of the two-machine system, is given as:</p>
\\[ P = \begin{pmatrix} 0.9025 & 0.095 & 0.0025 \\ 0.855 & 0.14 & 0.005 \\ 0.81 & 0.18 & 0.01 \end{pmatrix} \\]

<b>2. Stock Market Price Modeling</b>
<p>This example demonstrates how DTMCs can model systems with discrete states and defined transition rules, including boundary conditions.</p>

<b>a. Problem Setup</b>
<p>The system models the daily price of a stock, which is simplified to a finite number of states.</p>
<ul>
    <li><b>State Space (S):</b> The possible prices are \\(\$100, \$200, \$300, \$400, \$500\\). These are denoted as states \\(S_1, S_2, S_3, S_4, S_5\\) respectively.</li>
    <li><b>Transition Rules:</b> From any non-boundary state, the price can change in one day as follows:
        <ul>
            <li>Increase by \\(\$100\\) with probability \\(0.2\\).</li>
            <li>Decrease by \\(\$100\\) with probability \\(0.2\\).</li>
            <li>Remain constant. Since the probabilities for any given starting state must sum to 1, this probability is \\(1 - 0.2 - 0.2 = 0.6\\).</li>
        </ul>
    </li>
    <li><b>Boundary Conditions:</b>
        <ul>
            <li>At \\(\$100\\) (State S1), the price cannot decrease further.</li>
            <li>At \\(\$500\\) (State S5), the price cannot increase further.</li>
        </ul>
        This is a key feature. The probability that would normally be assigned to the impossible transition (e.g., decreasing from \\(\$100\\)) is reallocated to the "remain constant" transition.
    </li>
</ul>

<b>b. Calculating Transition Probabilities</b>
<p>Let's calculate the probabilities for a few rows of the transition matrix.</p>

<p><b>i. From S3 (\\(\$300\\))</b></p>
<p>This is a standard, non-boundary state.</p>
<ul>
    <li>\\(P_{32}\\) (Decrease to \\(\$200\\)): \\( P(X_{n+1}=S_2 | X_n=S_3) = 0.2 \\)</li>
    <li>\\(P_{34}\\) (Increase to \\(\$400\\)): \\( P(X_{n+1}=S_4 | X_n=S_3) = 0.2 \\)</li>
    <li>\\(P_{33}\\) (Remain at \\(\$300\\)): \\( P(X_{n+1}=S_3 | X_n=S_3) = 1 - 0.2 - 0.2 = 0.6 \\)</li>
    <li>\\(P_{31}\\) and \\(P_{35}\\) are 0, as a jump of \\(\$200\\) in one step is not allowed.</li>
</ul>

<p><b>ii. From S1 (\\(\$100\\)) - A Boundary State</b></p>
<p>Here, a decrease is impossible.</p>
<ul>
    <li>\\(P_{12}\\) (Increase to \\(\$200\\)): \\( P(X_{n+1}=S_2 | X_n=S_1) = 0.2 \\)</li>
    <li>\\(P_{11}\\) (Remain at \\(\$100\\)): The probability of increasing is 0.2. The probability of decreasing is 0. All remaining probability goes to staying constant. Therefore, \\( P(X_{n+1}=S_1 | X_n=S_1) = 1 - 0.2 = 0.8 \\).</li>
</ul>

<p><b>iii. From S5 (\\(\$500\\)) - A Boundary State</b></p>
<p>Similarly, an increase is impossible.</p>
<ul>
    <li>\\(P_{54}\\) (Decrease to \\(\$400\\)): \\( P(X_{n+1}=S_4 | X_n=S_5) = 0.2 \\)</li>
    <li>\\(P_{55}\\) (Remain at \\(\$500\\)): \\( P(X_{n+1}=S_5 | X_n=S_5) = 1 - 0.2 = 0.8 \\).</li>
</ul>

<b>c. The Transition Probability Matrix (P)</b>
<p>Combining all calculations gives the 5x5 transition matrix for the stock price model. The rows and columns correspond to the states \\(S_1\\) through \\(S_5\\).</p>
\\[ P = \begin{pmatrix} 0.8 & 0.2 & 0 & 0 & 0 \\ 0.2 & 0.6 & 0.2 & 0 & 0 \\ 0 & 0.2 & 0.6 & 0.2 & 0 \\ 0 & 0 & 0.2 & 0.6 & 0.2 \\ 0 & 0 & 0 & 0.2 & 0.8 \end{pmatrix} \\]

<p>These examples illustrate the power and versatility of the DTMC framework for modeling a wide range of real-world systems, from industrial engineering to finance.</p>
</div></div><div class="chapter" id="Lecture 59 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 59 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts discussed in the transcript, focusing on how to determine the <b>n-step transition probabilities</b> for a Discrete-Time Markov Chain (DTMC).</p>

<h3>1. Characterizing n-Step Transition Probabilities</h3>
<p>While the one-step transition probability matrix, <b>P</b>, describes the probability of moving from one state to another in a single time step, we are often interested in the probability of a transition over multiple steps. An <b>m-step transition probability</b> is the probability that the process will be in state \\(s_j\\) after \\(m\\) steps, given that it is currently in state \\(s_i\\). This is denoted as:</p>
\\[ P(X_{n+m} = s_j | X_n = s_i) \\]
<p>This probability is independent of the specific time \\(n\\) due to the <b>time-homogeneity</b> property of the Markov chains being discussed. The core idea is to derive these multi-step probabilities from the known one-step probabilities.</p>

<h3>2. The Two-Step Transition Probability and Matrix Multiplication</h3>
<p>To understand the general case, we begin by analyzing the two-step transition (where \\(m=2\\)). The probability of moving from state \\(s_i\\) to state \\(s_j\\) in two steps is written as:</p>
\\[ P(X_{n+2} = s_j | X_n = s_i) \\]
<p>To make this transition, the process must pass through some intermediate state \\(s_k\\) at time \\(n+1\\). The key insight is that the two-step transition probabilities can be found by squaring the one-step transition probability matrix, <b>P</b>.</p>

<p>Let's examine the \\((i, j)\\)-th element of the matrix \\(P^2 = P \times P\\). Standard matrix multiplication gives us:</p>
\\[ (P^2)_{ij} = \sum_{k=1}^{N} p_{ik} p_{kj} \\]
<p>where \\(N\\) is the total number of states.</p>

<p>We can interpret the terms in this summation probabilistically:</p>
<ul>
    <li>\\(p_{ik} = P(X_{n+1} = s_k | X_n = s_i)\\) is the probability of going from state \\(i\\) to state \\(k\\) in one step.</li>
    <li>\\(p_{kj} = P(X_{n+2} = s_j | X_{n+1} = s_k)\\) is the probability of going from state \\(k\\) to state \\(j\\) in the next step. Note that we use the same probability value as \\(P(X_{n+1} = s_j | X_n = s_k)\\) because of the time-homogeneity property.</li>
</ul>

<p>The product \\(p_{ik} p_{kj}\\) represents the probability of the specific path \\(s_i \rightarrow s_k \rightarrow s_j\\). Since the process must pass through one of the intermediate states \\(s_1, s_2, ..., s_N\\), we sum the probabilities of all possible intermediate paths to find the total probability of transitioning from \\(s_i\\) to \\(s_j\\) in two steps.</p>

<h4>Connection to the Total Probability Rule</h4>
<p>This summation is a direct application of the <b>Law of Total Probability</b>. The rule states that the probability of an event A can be found by summing its conditional probabilities with respect to a set of mutually exclusive and exhaustive events \\(B_i\\):</p>
\\[ P(A) = \sum_{i=1}^{N} P(A \cap B_i) = \sum_{i=1}^{N} P(A | B_i) P(B_i) \\]
<p>In our context:</p>
<ul>
    <li>Event A is \\(\{X_{n+2} = s_j\}\\) given \\(\{X_n = s_i\}\\).</li>
    <li>The set of events \\(B_k\\) is \\(\{X_{n+1} = s_k\}\\) for \\(k = 1, ..., N\\). These are mutually exclusive (the process can only be in one state at time \\(n+1\\)) and exhaustive (it must be in one of the possible states).</li>
</ul>
<p>Applying this, we get:</p>
\\[ P(X_{n+2} = s_j | X_n = s_i) = \sum_{k=1}^{N} P(X_{n+2}=s_j, X_{n+1}=s_k | X_n=s_i) \\]
\\[ = \sum_{k=1}^{N} P(X_{n+2}=s_j | X_{n+1}=s_k) \times P(X_{n+1}=s_k | X_n=s_i) \\]
\\[ = \sum_{k=1}^{N} p_{kj} \times p_{ik} = (P^2)_{ij} \\]
<p>Thus, the \\((i, j)\\)-th element of \\(P^2\\) is precisely the two-step transition probability from state \\(s_i\\) to state \\(s_j\\).</p>

<h3>3. Generalization to m-Step Transitions</h3>
<p>This logic extends naturally. The <b>m-step transition probability matrix</b> is obtained by raising the one-step transition matrix <b>P</b> to the power of \\(m\\):</p>
\\[ P^{(m)} = P^m = \underbrace{P \times P \times \dots \times P}_{m \text{ times}} \\]
<p>The \\((i, j)\\)-th element of this matrix, \\((P^m)_{ij}\\), gives the probability of transitioning from state \\(s_i\\) to state \\(s_j\\) in exactly \\(m\\) steps:</p>
\\[ (P^m)_{ij} = P(X_{n+m} = s_j | X_n = s_i) \\]

<h3>4. Examples</h3>

<h4>Example 1: Industrial Reliability</h4>
<p>A machine can be in one of two states: 1 (Operational, O) or 2 (Faulty, F). The one-step transition matrix <b>P</b> is:</p>
\\[ P = \begin{pmatrix} 0.95 & 0.05 \\ 0.90 & 0.10 \end{pmatrix} \\]
<p>The two-step transition matrix, \\(P^2\\), is calculated as:</p>
\\[ P^2 = P \times P = \begin{pmatrix} 0.95 & 0.05 \\ 0.90 & 0.10 \end{pmatrix} \begin{pmatrix} 0.95 & 0.05 \\ 0.90 & 0.10 \end{pmatrix} = \begin{pmatrix} 0.9475 & 0.0525 \\ 0.9450 & 0.0550 \end{pmatrix} \\]
<p>Let's interpret the element \\((P^2)_{11} = 0.9475\\). This is the probability that a machine starting in the Operational state is still Operational after two time steps. This can happen in two ways:</p>
<ol>
    <li>It stays Operational in the first step and stays Operational in the second (O → O → O). Probability = \\(0.95 \times 0.95 = 0.9025\\).</li>
    <li>It becomes Faulty in the first step and is repaired in the second (O → F → O). Probability = \\(0.05 \times 0.90 = 0.0450\\).</li>
</ol>
<p>The sum is \\(0.9025 + 0.0450 = 0.9475\\), matching our matrix calculation. This shows that over two steps, the probability of the machine being operational (0.9475) is slightly lower than after one step (0.95), and the probability of it becoming faulty (0.0525) is slightly higher than after one step (0.05).</p>

<h4>Example 2: Stock Price</h4>
<p>A stock price can be in one of five states: \\(s_1=\$100, s_2=\$200, s_3=\$300, s_4=\$400, s_5=\$500\\). The one-step transition matrix is:</p>
\\[ P = \begin{pmatrix} 0.8 & 0.2 & 0 & 0 & 0 \\ 0.2 & 0.6 & 0.2 & 0 & 0 \\ 0 & 0.2 & 0.6 & 0.2 & 0 \\ 0 & 0 & 0.2 & 0.6 & 0.2 \\ 0 & 0 & 0 & 0.2 & 0.8 \end{pmatrix} \\]
<p>The two-step transition matrix \\(P^2\\) is:</p>
\\[ P^2 = \begin{pmatrix} 0.68 & 0.28 & 0.04 & 0 & 0 \\ 0.28 & 0.44 & 0.24 & 0.04 & 0 \\ 0.04 & 0.24 & 0.44 & 0.24 & 0.04 \\ 0 & 0.04 & 0.24 & 0.44 & 0.28 \\ 0 & 0 & 0.04 & 0.28 & 0.68 \end{pmatrix} \\]
<p>Let's analyze the element \\((P^2)_{35} = 0.04\\). This is the probability of the stock price moving from $300 (state 3) to $500 (state 5) in two days. Based on the one-step matrix, the only way for this to happen is to move from state 3 to state 4, and then from state 4 to state 5.</p>
<ul>
    <li>Probability(3 → 4) = \\(p_{34} = 0.2\\)</li>
    <li>Probability(4 → 5) = \\(p_{45} = 0.2\\)</li>
</ul>
<p>The total probability is \\(p_{34} \times p_{45} = 0.2 \times 0.2 = 0.04\\), which matches the entry in the \\(P^2\\) matrix.</p>

<h3>5. A Fundamental Property of Transition Matrices</h3>
<p>A crucial property that holds for <i>any</i> m-step transition probability matrix (\\(P, P^2, P^3, \dots\\)) is that <b>the sum of the elements in each row must equal 1</b>.</p>
\\[ \sum_{j=1}^{N} (P^m)_{ij} = 1 \quad \text{for all rows } i \\]
<p>This is because if the process starts in state \\(s_i\\), after \\(m\\) steps it must end up in one of the possible states \\(s_1, s_2, \dots, s_N\\). Summing the probabilities of transitioning to all possible destination states must therefore equal 1.</p>
</div></div><div class="chapter" id="Lecture 60 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 60 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts discussed in the transcript, focusing on the limiting behavior of Discrete-Time Markov Chains (DTMCs) and the concept of a stationary distribution.</p>

<h3>1. Limiting Behavior of a DTMC</h3>
<p>The central question addressed is: What happens to the probability of a DTMC being in a particular state after a very long time? Let \\(X_n\\) be the state of the DTMC at time step \\(n\\). We are interested in the behavior of the probability \\(P(X_n = s_j)\\) as \\(n \to \infty\\).</p>

<b>N-Step Transition Probabilities</b>
<p>The analysis begins by examining the \\(n\\)-step transition probability matrix, denoted as \\(P^n\\). The element \\((i, j)\\) of this matrix, \\(p_{ij}^{(n)}\\), represents the probability of transitioning from state \\(s_i\\) to state \\(s_j\\) in exactly \\(n\\) steps.</p>
\\[ p_{ij}^{(n)} = P(X_n = s_j | X_0 = s_i) \\]

<p><b>Example: Industrial Reliability Problem</b></p>
<p>The transcript uses an example of a machine with two states: Operational (\\(s_1\\)) and Faulty (\\(s_2\\)).</p>
<ul>
    <li>The <b>1-step transition matrix</b> \\(P\\) is given by:
        \\[ P = \begin{pmatrix} 0.95 & 0.05 \\ 0.10 & 0.90 \end{pmatrix} \\]
    </li>
    <li>The <b>2-step transition matrix</b> \\(P^2\\) is:
        \\[ P^2 = \begin{pmatrix} 0.9025 + 0.005 & 0.0475 + 0.045 \\ 0.095 + 0.09 & 0.005 + 0.81 \end{pmatrix} = \begin{pmatrix} 0.9075 & 0.0925 \\ 0.185 & 0.815 \end{pmatrix} \\]
        <i>(Note: The transcript provides slightly different values for \\(P^2\\), possibly from a different example, but the concept remains the same.)</i>
    </li>
    <li>As \\(n\\) increases, for example to \\(n=10\\), the matrix \\(P^{10}\\) becomes:
         \\[ P^{10} \approx \begin{pmatrix} 0.6688 & 0.3312 \\ 0.6624 & 0.3376 \end{pmatrix} \\]
        <i>(Again, the transcript uses a different numerical example where convergence is faster, showing \\(P^{10} \approx \begin{pmatrix} 0.9474 & 0.0526 \\ 0.9474 & 0.0526 \end{pmatrix}\\). We will proceed using the conceptual point from the transcript.)</i>
    </li>
</ul>

<p>The key observation is that as \\(n\\) becomes very large, the rows of the matrix \\(P^n\\) become identical.
\\[ \lim_{n \to \infty} P^n = \begin{pmatrix} \pi_1 & \pi_2 & \dots & \pi_N \\ \pi_1 & \pi_2 & \dots & \pi_N \\ \vdots & \vdots & \ddots & \vdots \\ \pi_1 & \pi_2 & \dots & \pi_N \end{pmatrix} \\]
Each row of this limiting matrix is the same vector, which we can call \\(\bar{\pi} = (\pi_1, \pi_2, \dots, \pi_N)\\).

<p>This implies that after a long time, the probability of being in state \\(s_j\\) is \\(\pi_j\\), regardless of the initial state \\(s_i\\).
\\[ \pi_j = \lim_{n \to \infty} P(X_n = s_j | X_0 = s_i) \quad \text{for all } i \\]
The system's "memory" of its starting state fades over time.</p>

<b>Unconditional Limiting Probability</b>
<p>Using this result, we can find the unconditional probability of being in state \\(s_j\\) at a very large time \\(n\\). By the law of total probability:</p>
\\[ P(X_n = s_j) = \sum_i P(X_n = s_j | X_0 = s_i) \cdot P(X_0 = s_i) \\]
Taking the limit as \\(n \to \infty\\):
\\[ \lim_{n \to \infty} P(X_n = s_j) = \sum_i \left( \lim_{n \to \infty} P(X_n = s_j | X_0 = s_i) \right) \cdot P(X_0 = s_i) \\]
Since the limit of the conditional probability is \\(\pi_j\\) for all starting states \\(s_i\\), we get:
\\[ \lim_{n \to \infty} P(X_n = s_j) = \sum_i \pi_j \cdot P(X_0 = s_i) \\]
We can factor out \\(\pi_j\\) as it does not depend on the summation index \\(i\\):
\\[ = \pi_j \sum_i P(X_0 = s_i) \\]
Since the sum of initial probabilities over all possible states is 1, we are left with:
\\[ \lim_{n \to \infty} P(X_n = s_j) = \pi_j \\]
This powerful result shows that the long-run probability of the DTMC being in state \\(s_j\\) converges to \\(\pi_j\\), regardless of the initial probability distribution. This vector \\(\bar{\pi}\\) is known as the <b>limiting distribution</b>.

<h3>2. Stationary Distribution</h3>
<p>The concept of a stationary distribution is closely related to the limiting behavior. A probability distribution \\(\bar{\pi}\\) is called <b>stationary</b> if, once the system is in that distribution, it remains in that distribution for all subsequent time steps.</p>

<p>Let's assume the initial state probabilities are given by the vector \\(\bar{\pi}\\), so \\(P(X_0 = s_i) = \pi_i\\). The probability of being in state \\(s_j\\) at the next time step (\\(n=1\\)) is:</p>
\\[ P(X_1 = s_j) = \sum_i P(X_1 = s_j | X_0 = s_i) \cdot P(X_0 = s_i) = \sum_i p_{ij} \cdot \pi_i \\]
For the distribution to be stationary, this probability must be equal to the initial probability \\(\pi_j\\). Therefore, the condition for a stationary distribution is:
\\[ \pi_j = \sum_{i} \pi_i p_{ij} \quad \text{for all } j \\]

<p>In matrix notation, if \\(\bar{\pi}\\) is a row vector, this system of equations can be written as:</p>
\\[ \bar{\pi} = \bar{\pi} P \\]
This equation means that if the DTMC starts with the probability distribution \\(\bar{\pi}\\), the distribution of states at time \\(n=1\\) will also be \\(\bar{\pi}\\), and by extension, it will be \\(\bar{\pi}\\) for all future times \\(n\). The distribution does not change with time, hence the term "stationary."</p>

<b>Finding the Stationary Distribution via Linear Algebra</b>
<p>The equation \\(\bar{\pi} = \bar{\pi} P\\), or \\(\bar{\pi}^T = P^T \bar{\pi}^T\\) for column vectors, reveals a deep connection to linear algebra. This equation states that \\(\bar{\pi}\\) is a <b>left eigenvector</b> of the transition matrix \\(P\\) corresponding to an <b>eigenvalue of 1</b>.</p>
<p>To find the stationary distribution, we need to solve this eigenvector problem. However, eigenvectors are only unique up to a scalar multiple. Since \\(\bar{\pi}\\) must be a valid probability distribution, its components must satisfy an additional constraint: they must sum to 1.</p>
\\[ \sum_i \pi_i = 1 \\]
Therefore, the stationary distribution \\(\bar{\pi}\\) is the unique left eigenvector of \\(P\\) corresponding to the eigenvalue 1 whose components sum to 1.</p>

<p>For many common types of DTMCs (specifically, irreducible and aperiodic ones, which are collectively called ergodic), the following is true:</p>
<ol>
    <li>A unique stationary distribution exists.</li>
    <li>The limiting distribution exists and is equal to this unique stationary distribution.</li>
</ol>
<p>This explains why the rows of \\(P^n\\) in the example converged to a vector that is also the stationary distribution of the system. Finding this eigenvector is often a more direct way to calculate the long-run probabilities than by raising the matrix \\(P\\) to a high power.</p>

</div></div><div class="chapter" id="Lecture 61 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 61 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the key concepts from the transcript, focusing on the least squares problem, the conditions for its standard solution, and the approach for solving it in more general cases.</p>

<b><h3>1. The Least Squares Problem Revisited</h3></b>
<p>The discussion begins by revisiting the fundamental least squares problem. This problem arises when we have an overdetermined system of linear equations, where there are more equations than unknowns.</p>
<p>The system is represented as:</p>
\\[ \mathbf{y} = A \mathbf{x} \\]
<p>Here:</p>
<ul>
    <li>\\(\mathbf{y}\\) is a vector of \\(m\\) observations or measurements, \\(\mathbf{y} \in \mathbb{R}^m\\).</li>
    <li>\\(A\\) is an \\(m \times n\\) matrix, where \\(m > n\\). This is often called a "tall matrix" because it has more rows than columns.</li>
    <li>\\(\mathbf{x}\\) is a vector of \\(n\\) unknown variables, \\(\mathbf{x} \in \mathbb{R}^n\\).</li>
</ul>
<p>Expanded, the system looks like this:</p>
\\[
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix} =
\begin{bmatrix} | & | & & | \\ \mathbf{a}_1 & \mathbf{a}_2 & \dots & \mathbf{a}_n \\ | & | & & | \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
\\[]
<p>Because there are more equations (\\(m\\)) than unknowns (\\(n\\)), an exact solution \\(\mathbf{x}\\) that satisfies all equations simultaneously typically does not exist. Instead, the goal is to find the vector \\(\mathbf{x}\\) that makes the error (or residual) vector, \\(\mathbf{y} - A\mathbf{x}\\), as small as possible. The "least squares" method minimizes the squared Euclidean norm of this error:</p>
\\[ \min_{\mathbf{x}} ||\mathbf{y} - A\mathbf{x}||^2 \\]

<b><h3>2. The Standard Solution and the Pseudo-Inverse</h3></b>
<p>For a specific class of problems, there is a well-known closed-form solution for the least squares estimate of \\(\mathbf{x}\\), often denoted as \\(\hat{\mathbf{x}}\\). This solution is given by the formula:</p>
\\[ \hat{\mathbf{x}} = (A^T A)^{-1} A^T \mathbf{y} \\]
<p>The matrix term \\((A^T A)^{-1} A^T\\) is very important and is called the <b>pseudo-inverse</b> of \\(A\\), denoted by \\(A^\dagger\\). Using this notation, the solution can be written more compactly:</p>
\\[ \hat{\mathbf{x}} = A^\dagger \mathbf{y} \quad \text{where} \quad A^\dagger = (A^T A)^{-1} A^T \\]
<p>A critical point highlighted in the transcript is that this formula is not universally applicable. Its existence hinges on whether the matrix \\(A^T A\\) can be inverted.</p>

<b><h3>3. Condition for the Existence of the Standard Solution</h3></b>
<p>The central question addressed is: under what conditions is the matrix \\(A^T A\\) invertible? The transcript presents and proves a fundamental theorem of linear algebra.</p>
<p><b>Theorem:</b> The \\(n \times n\\) matrix \\(A^T A\\) is invertible if and only if the \\(m \times n\\) matrix \\(A\\) has <b>full column rank</b>.</p>
<p>Full column rank means that all the columns of \\(A\\) are linearly independent. For an \\(m \times n\\) matrix, this is equivalent to saying that the rank of the matrix is equal to its number of columns:</p>
\\[ \text{rank}(A) = n \\]
<p>A proof for this "if and only if" statement is provided in two parts.</p>

<b><h4>Proof</h4></b>
<p><b>Part 1: If A has full column rank, then \\(A^T A\\) is invertible.</b></p>
<p>This is proven by contradiction. </p>
<ol>
    <li><b>Assumption:</b> Let \\(A\\) have full column rank, so \\(\text{rank}(A) = n\\).</li>
    <li><b>Contradictory Premise:</b> Assume that \\(A^T A\\) is <i>not</i> invertible. This means it is a singular matrix.</li>
    <li>A property of singular matrices is that they have a non-trivial null space. This implies there exists a non-zero vector \\(\mathbf{u} \neq \mathbf{0}\\) such that:
    \\[ A^T A \mathbf{u} = \mathbf{0} \\]
    </li>
    <li>Pre-multiply both sides by \\(\mathbf{u}^T\\):
    \\[ \mathbf{u}^T (A^T A \mathbf{u}) = \mathbf{u}^T \mathbf{0} = 0 \\]
    </li>
    <li>Using the properties of transposes, \\(\mathbf{u}^T A^T = (A\mathbf{u})^T\\), we can rewrite the left side:
    \\[ (A\mathbf{u})^T (A\mathbf{u}) = 0 \\]
    </li>
    <li>The dot product of a vector with itself is the square of its norm. Therefore:
    \\[ ||A\mathbf{u}||^2 = 0 \\]
    </li>
    <li>This implies that the vector \\(A\mathbf{u}\\) must be the zero vector:
    \\[ A\mathbf{u} = \mathbf{0} \\]
    </li>
    <li>We started with a non-zero vector \\(\mathbf{u}\\) and found that \\(A\mathbf{u} = \mathbf{0}\\). This means that \\(A\\) has a non-trivial null space. However, a matrix with a non-trivial null space cannot have full column rank. This contradicts our initial assumption that \\(\text{rank}(A) = n\\).</li>
    <li>Therefore, our premise that \\(A^T A\\) is not invertible must be false. Conclusion: If \\(A\\) has full column rank, \\(A^T A\\) must be invertible.</li>
</ol>

<p><b>Part 2: If \\(A^T A\\) is invertible, then A has full column rank.</b></p>
<p>This part is also proven by contradiction (or more formally, by proving the contrapositive).</p>
<ol>
    <li><b>Assumption:</b> Let \\(A^T A\\) be invertible.</li>
    <li><b>Contradictory Premise:</b> Assume that \\(A\\) does <i>not</i> have full column rank. This means \\(\text{rank}(A) < n\\).</li>
    <li>If \\(A\\) does not have full column rank, its columns are linearly dependent, which means it has a non-trivial null space. Thus, there exists a non-zero vector \\(\mathbf{u} \neq \mathbf{0}\\) such that:
    \\[ A\mathbf{u} = \mathbf{0} \\]
    </li>
    <li>Pre-multiply both sides by \\(A^T\\):
    \\[ A^T(A\mathbf{u}) = A^T\mathbf{0} \\]
    \\[ A^T A \mathbf{u} = \mathbf{0} \\]
    </li>
    <li>This result shows that the non-zero vector \\(\mathbf{u}\\) is in the null space of \\(A^T A\\). This means \\(A^T A\\) has a non-trivial null space, which implies that \\(A^T A\\) is singular and therefore <i>not</i> invertible.</li>
    <li>This contradicts our initial assumption that \\(A^T A\\) is invertible.</li>
    <li>Therefore, our premise that \\(A\\) does not have full column rank must be false. Conclusion: If \\(A^T A\\) is invertible, \\(A\\) must have full column rank.</li>
</ol>
<p>This completes the proof that \\(A^T A\\) is invertible if and only if \\(A\\) has full column rank.</p>

<b><h3>4. The Rank-Deficient Case: What if A Does Not Have Full Column Rank?</h3></b>
<p>The next logical question is what to do when \\(\text{rank}(A) < n\\). In this scenario, \\(A^T A\\) is not invertible, and the standard formula for the least squares solution cannot be used. This is known as the rank-deficient case.</p>
<p>To solve the problem in this more general setting, the transcript proposes using the <b>Singular Value Decomposition (SVD)</b> of the matrix \\(A\\).</p>
<p>The SVD of an \\(m \times n\\) matrix \\(A\\) is given by:</p>
\\[ A = U \Sigma V^T \\]
<p>For a "tall" matrix \\(A\\) (where \\(m>n\\)), the dimensions of the SVD components are typically:</p>
<ul>
    <li>\\(U\\) is an \\(m \times n\\) matrix with orthonormal columns.</li>
    <li>\\(\Sigma\\) is an \\(n \times n\\) diagonal matrix containing the singular values \\(\sigma_i\\) in decreasing order: \\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_n \ge 0\\).</li>
    <li>\\(V\\) is an \\(n \times n\\) orthogonal matrix.</li>
</ul>
<p>If \\(A\\) is rank-deficient with \\(\text{rank}(A) = r < n\\), then only the first \\(r\\) singular values will be positive, and the remaining \\(n-r\\) will be zero.</p>
\\[ \sigma_1 \ge \dots \ge \sigma_r > 0 \quad \text{and} \quad \sigma_{r+1} = \dots = \sigma_n = 0 \\]

<p>The transcript sets up the next step by constructing a full orthonormal basis for the \\(m\\)-dimensional space. The SVD provides the \\(m \times n\\) matrix \\(U\\). This can be extended to a square \\(m \times m\\) orthogonal matrix \\(\bar{U}\\) by finding \\(m-n\\) additional orthonormal vectors that are orthogonal to the columns of \\(U\\). These new vectors form a matrix \\(\tilde{U}\\) of size \\(m \times (m-n)\\), such that:</p>
\\[ \bar{U} = \begin{bmatrix} U & | & \tilde{U} \end{bmatrix} \\]
<p>This new matrix \\(\tilde{U}\\) has the properties:</p>
<ul>
    <li>\\(\tilde{U}^T \tilde{U} = I\\) (its columns are orthonormal).</li>
    <li>\\(U^T \tilde{U} = \mathbf{0}\\) (it is orthogonal to the original basis from \\(U\\)).</li>
</ul>
<p>The transcript concludes by stating that this SVD framework will be used in the subsequent module to derive a solution for the least squares problem even when \\(A\\) is rank-deficient.</p>

</div></div><div class="chapter" id="Lecture 62 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham"><h3 class="heading">Lecture 62 | Applied Linear Algebra | Vector Properties | Prof AK Jagannatham</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, which revisits the least squares problem, particularly for scenarios where the matrix <b>A</b> does not have full column rank. In such cases, the matrix \\( A^T A \\) is not invertible, and the standard normal equation solution \\( \mathbf{x} = (A^T A)^{-1} A^T \mathbf{y} \\) cannot be used. The explanation below uses the Singular Value Decomposition (SVD) to derive a more general solution.</p>

<h3>1. Revisiting the Least Squares Problem</h3>
<p>The standard least squares problem aims to find a vector \\( \mathbf{x} \\) that minimizes the squared Euclidean norm of the residual vector \\( \mathbf{y} - A\mathbf{x} \\):</p>
\\[ \min_{\mathbf{x}} \| \mathbf{y} - A\mathbf{x} \|^2 \\]
<p>When the matrix \\(A\\) (of size \\(m \times n\\), with \\(m > n\\)) does not have full column rank, it means its columns are linearly dependent. This implies that the matrix \\(A^T A\\) is singular (not invertible), preventing the direct calculation of the solution using the normal equations.</p>

<h3>2. Matrix Decomposition and Unitary Transformation</h3>
<p>To handle this case, we use the Singular Value Decomposition (SVD) of \\(A\\) and construct a special unitary matrix.</p>

<b>Singular Value Decomposition (SVD):</b>
<p>Any matrix \\(A\\) can be decomposed as:</p>
\\[ A = U \Sigma V^T \\]
<p>where:</p>
<ul>
    <li>\\(U\\) is an \\(m \times n\\) matrix with orthonormal columns (from the reduced SVD).</li>
    <li>\\(\Sigma\\) is an \\(n \times n\\) diagonal matrix containing the singular values \\(\sigma_1, \sigma_2, \dots, \sigma_n\\).</li>
    <li>\\(V\\) is an \\(n \times n\\) unitary matrix (\\(V^T V = V V^T = I\\)).</li>
</ul>

<b>Constructing a Full Unitary Matrix \\(\bar{U}\\):</b>
<p>The \\(m \times n\\) matrix \\(U\\) can be extended into a full \\(m \times m\\) square unitary matrix, denoted as \\(\bar{U}\\). This is done by finding an \\(m \times (m-n)\\) matrix \\(\tilde{U}\\) whose columns are orthonormal and are orthogonal to the columns of \\(U\\). This means:</p>
<ul>
    <li>\\( \tilde{U}^T \tilde{U} = I \\) (orthonormal columns)</li>
    <li>\\( \tilde{U}^T U = 0 \\) (orthogonality to \\(U\\))</li>
</ul>
<p>The full unitary matrix \\(\bar{U}\\) is then constructed by concatenating \\(U\\) and \\(\tilde{U}\\):</p>
\\[ \bar{U} = [U \quad \tilde{U}] \\]
<p>This resulting \\(m \times m\\) matrix \\(\bar{U}\\) is unitary. We can verify this by checking if \\(\bar{U}^T \bar{U} = I\\):</p>
\\[ \bar{U}^T \bar{U} = \begin{bmatrix} U^T \\ \tilde{U}^T \end{bmatrix} [U \quad \tilde{U}] = \begin{bmatrix} U^T U & U^T \tilde{U} \\ \tilde{U}^T U & \tilde{U}^T \tilde{U} \end{bmatrix} = \begin{bmatrix} I & 0 \\ 0 & I \end{bmatrix} = I_{m \times m} \\]
<p>Since \\(\bar{U}\\) is a square matrix and \\(\bar{U}^T \bar{U} = I\\), it is a unitary matrix, which also implies \\(\bar{U} \bar{U}^T = I\\). A key property of unitary matrices is that they preserve the norm of a vector when they multiply it, as they represent rotations and/or reflections.</p>

<h3>3. Reformulating the Cost Function</h3>
<p>Using the property that unitary matrices preserve norms, we can pre-multiply the residual vector by \\(\bar{U}^T\\) without changing the value of the cost function:</p>
\\[ \| \mathbf{y} - A\mathbf{x} \|^2 = \| \bar{U}^T (\mathbf{y} - A\mathbf{x}) \|^2 \\]
<p>Let's expand the term inside the norm:</p>
\\[ \bar{U}^T (\mathbf{y} - A\mathbf{x}) = \bar{U}^T \mathbf{y} - \bar{U}^T A \mathbf{x} \\]
<p>We can write \\(\bar{U}^T A \mathbf{x}\\) by substituting \\(A = U \Sigma V^T\\):</p>
\\[ \bar{U}^T A = \begin{bmatrix} U^T \\ \tilde{U}^T \end{bmatrix} (U \Sigma V^T) = \begin{bmatrix} U^T U \\ \tilde{U}^T U \end{bmatrix} \Sigma V^T = \begin{bmatrix} I_n \\ 0 \end{bmatrix} \Sigma V^T = \begin{bmatrix} \Sigma V^T \\ 0 \end{bmatrix} \\]
<p>Next, we define new variables for the transformed vector \\(\mathbf{y}\\):</p>
<ul>
    <li>\\(\hat{\mathbf{y}} = U^T \mathbf{y}\\)</li>
    <li>\\(\tilde{\mathbf{y}} = \tilde{U}^T \mathbf{y}\\)</li>
</ul>
<p>So, \\( \bar{U}^T \mathbf{y} = \begin{bmatrix} \hat{\mathbf{y}} \\ \tilde{\mathbf{y}} \end{bmatrix} \\). The cost function now becomes:</p>
\\[ \| \begin{bmatrix} \hat{\mathbf{y}} \\ \tilde{\mathbf{y}} \end{bmatrix} - \begin{bmatrix} \Sigma V^T \mathbf{x} \\ 0 \end{bmatrix} \|^2 = \| \begin{bmatrix} \hat{\mathbf{y}} - \Sigma V^T \mathbf{x} \\ \tilde{\mathbf{y}} \end{bmatrix} \|^2 \\]
<p>The squared norm of a block vector is the sum of the squared norms of its blocks. Therefore, the expression splits into two parts:</p>
\\[ \| \hat{\mathbf{y}} - \Sigma V^T \mathbf{x} \|^2 + \| \tilde{\mathbf{y}} \|^2 \\]
<p>The second term, \\(\| \tilde{\mathbf{y}} \|^2\\), is a constant value that does not depend on \\(\mathbf{x}\\). Thus, to minimize the overall expression, we only need to minimize the first term.</p>
\\[ \min_{\mathbf{x}} \| \hat{\mathbf{y}} - \Sigma V^T \mathbf{x} \|^2 \\]

<h3>4. Solving the Simplified Problem</h3>
<p>We introduce a change of variables: \\(\hat{\mathbf{x}} = V^T \mathbf{x}\\). The minimization problem becomes:</p>
\\[ \min_{\hat{\mathbf{x}}} \| \hat{\mathbf{y}} - \Sigma \hat{\mathbf{x}} \|^2 \\]
<p>Since \\(A\\) is not full rank, some of its singular values are zero. Let \\(r\\) be the rank of \\(A\\), meaning there are \\(r\\) non-zero singular values (\\(\sigma_1, \dots, \sigma_r\\)) and \\(n-r\\) zero singular values. The minimization problem can be written component-wise:</p>
\\[ \| \hat{\mathbf{y}} - \Sigma \hat{\mathbf{x}} \|^2 = \sum_{i=1}^{n} (\hat{y}_i - \sigma_i \hat{x}_i)^2 \\]
<p>We can split this sum into two parts:</p>
\\[ \sum_{i=1}^{r} (\hat{y}_i - \sigma_i \hat{x}_i)^2 + \sum_{i=r+1}^{n} (\hat{y}_i - 0 \cdot \hat{x}_i)^2 = \underbrace{\sum_{i=1}^{r} (\hat{y}_i - \sigma_i \hat{x}_i)^2}_{\text{Term 1}} + \underbrace{\sum_{i=r+1}^{n} (\hat{y}_i)^2}_{\text{Term 2}} \\]
<p>Term 2 is a constant and cannot be changed. Term 1 can be minimized by setting each of its components to zero. This gives us the optimal values for the first \\(r\\) components of \\(\hat{\mathbf{x}}\\):</p>
\\[ \hat{y}_i - \sigma_i \hat{x}_i = 0 \implies \hat{x}_i = \frac{\hat{y}_i}{\sigma_i} \quad \text{for } i = 1, \dots, r \\]
<p>Notice that the components \\(\hat{x}_{r+1}, \dots, \hat{x}_n\\) do not affect the least squares error at all. They can be set to any arbitrary value. This implies there are infinitely many solutions for \\(\mathbf{x}\\) that produce the same minimum error.</p>

<h3>5. The Minimum Norm Solution</h3>
<p>Among the infinite possible solutions, a standard practice is to choose the one with the smallest Euclidean norm, \\(\| \mathbf{x} \|\\). This is known as the minimum-norm least-squares solution.</p>
<p>Since \\( \hat{\mathbf{x}} = V^T \mathbf{x} \\) and \\(V^T\\) is a unitary matrix, the norms are preserved: \\( \| \mathbf{x} \|^2 = \| \hat{\mathbf{x}} \|^2 \\). Therefore, minimizing \\(\| \mathbf{x} \|\\) is equivalent to minimizing \\(\| \hat{\mathbf{x}} \|\\).</p>
<p>The norm of \\(\hat{\mathbf{x}}\\) is:</p>
\\[ \| \hat{\mathbf{x}} \|^2 = \sum_{i=1}^{r} |\hat{x}_i|^2 + \sum_{i=r+1}^{n} |\hat{x}_i|^2 \\]
<p>The values \\(\hat{x}_1, \dots, \hat{x}_r\\) are already fixed by the minimization of the error. To minimize the total norm, we must choose the arbitrary components \\(\hat{x}_{r+1}, \dots, \hat{x}_n\\) to be zero:</p>
\\[ \hat{x}_i = 0 \quad \text{for } i = r+1, \dots, n \\]

<h3>6. The Pseudoinverse and the General Solution</h3>
<p>Combining these results, the minimum norm solution for \\(\hat{\mathbf{x}}\\) is:</p>
\\[ \hat{x}_i = \begin{cases} \hat{y}_i / \sigma_i & \text{if } i \le r \\ 0 & \text{if } i > r \end{cases} \\]
<p>This can be expressed in matrix form as:</p>
\\[ \hat{\mathbf{x}} = \Sigma^\dagger \hat{\mathbf{y}} \\]
<p>Here, \\(\Sigma^\dagger\\) is the <b>pseudoinverse</b> of the diagonal matrix \\(\Sigma\\). It is formed by taking the reciprocal of the non-zero singular values and leaving the zeros as they are:</p>
\\[ (\Sigma^\dagger)_{ii} = \begin{cases} 1/\sigma_i & \text{if } \sigma_i \ne 0 \\ 0 & \text{if } \sigma_i = 0 \end{cases} \\]
<p>Now, we substitute back the original variables:</p>
<ul>
    <li>\\( \hat{\mathbf{x}} = V^T \mathbf{x} \\)</li>
    <li>\\( \hat{\mathbf{y}} = U^T \mathbf{y} \\)</li>
</ul>
<p>The equation becomes:</p>
\\[ V^T \mathbf{x} = \Sigma^\dagger U^T \mathbf{y} \\]
<p>To solve for \\(\mathbf{x}\\), we pre-multiply both sides by \\(V\\):</p>
\\[ V V^T \mathbf{x} = V \Sigma^\dagger U^T \mathbf{y} \\]
<p>Since \\(V\\) is a square unitary matrix, \\(V V^T = I\\), which gives the final solution:</p>
\\[ \mathbf{x}_{LS} = V \Sigma^\dagger U^T \mathbf{y} \\]
<p>The term \\(V \Sigma^\dagger U^T\\) is defined as the general pseudoinverse of \\(A\\), denoted \\(A^\dagger\\). This formula provides the minimum-norm least-squares solution and is valid for any matrix \\(A\\), regardless of its rank.</p>

<h3>7. Connection to the Full Rank Case</h3>
<p>This general formula is consistent with the solution for the full column rank case. If \\(A\\) has full column rank (\\(r=n\\)), then all its singular values are non-zero. In this scenario, \\(\Sigma^\dagger\\) becomes \\(\Sigma^{-1}\\) (an \\(n \times m\\) matrix whose top-left block is the inverse of the diagonal \\(n \times n\\) part of \\(\Sigma\\)).</p>
<p>The solution is \\(\mathbf{x} = V \Sigma^{-1} U^T \mathbf{y}\\). It can be shown that this is equivalent to the familiar normal equation solution \\( (A^T A)^{-1} A^T \mathbf{y} \\):</p>
<ul>
    <li>\\(A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V (\Sigma^T \Sigma) V^T\\).</li>
    <li>\\((A^T A)^{-1} = (V (\Sigma^T \Sigma) V^T)^{-1} = V (\Sigma^T \Sigma)^{-1} V^T\\).</li>
    <li>\\((A^T A)^{-1} A^T = V (\Sigma^T \Sigma)^{-1} V^T (V \Sigma^T U^T) = V (\Sigma^T \Sigma)^{-1} \Sigma^T U^T\\).</li>
</ul>
<p>The term \\((\Sigma^T \Sigma)^{-1} \Sigma^T\\) simplifies to \\(\Sigma^{-1}\\), yielding \\(V \Sigma^{-1} U^T\\). Thus, the pseudoinverse method correctly reduces to the conventional solution when \\(A\\) has full column rank.</p>
</div></div><h2>Weekly Summary</h2><div>
<p>This week's lectures cover two major topics: an introduction to Discrete-Time Markov Chains (DTMCs) and a deeper analysis of the Least Squares problem, particularly for rank-deficient systems.</p>

<p><b>1. Discrete-Time Markov Chains (DTMCs)</b></p>
<p>This section introduces stochastic processes as a probabilistic framework for modeling systems that evolve randomly over time. The focus is on a specific, highly useful type of stochastic process known as a Discrete-Time Markov Chain.</p>
<p><b>Main Topics Covered:</b></p>
<ul>
<li><b>Introduction to Stochastic Processes:</b> A stochastic process is a probabilistic model for a system's evolution over time. Examples include daily temperatures, stock prices, inventory levels, and machine reliability (operational vs. faulty).</li>
<li><b>The Markov Property:</b> A simplifying assumption stating that the future state of the system depends only on the current state, not on the entire history of how it arrived there. Mathematically, this is expressed as:<br>\\( P(X_{n+1}=s_j | X_n=s_i, X_{n-1}, ..., X_0) = P(X_{n+1}=s_j | X_n=s_i) \\)</li>
<li><b>Transition Probability Matrix:</b> A DTMC's evolution is fully characterized by its one-step transition probabilities, \\(p_{ij}\\), which are organized into a square matrix \\(P\\). Each element \\(p_{ij}\\) represents the probability of moving from state \\(i\\) to state \\(j\\) in one time step.</li>
<li><b>Properties of the Transition Matrix (P):</b> All entries are non-negative (\\(p_{ij} \geq 0\\)), and the sum of the probabilities in each row must equal 1.</li>
<li><b>Building DTMC Models:</b> Several examples are developed, including a machine reliability problem with two states (operational, faulty), which is then extended to a more complex three-state model for two independent machines. A stock price model with boundary conditions is also constructed.</li>
<li><b>N-Step Transition Probabilities:</b> The probability of transitioning from one state to another in \\(m\\) steps is found in the matrix \\(P^m\\) (the transition matrix \\(P\\) raised to the power of \\(m\\)).</li>
<li><b>Limiting and Stationary Distributions:</b> The long-term behavior of a DTMC is explored. For many DTMCs, as \\(n \to \infty\\), the matrix \\(P^n\\) converges to a matrix where all rows are identical. This identical row vector, \\(\pi\\), is the <b>limiting distribution</b>, representing the long-run probability of being in each state, regardless of the starting state. This is also related to the <b>stationary distribution</b>, a probability distribution \\(\pi\\) that remains unchanged over time, satisfying the equation \\(\pi^T P = \pi^T\\).</li>
</ul>
<p><b>Key Takeaways:</b></p>
<ul>
<li>A DTMC is a powerful tool for modeling systems that change states randomly over time, based on the principle that only the present state matters for predicting the future.</li>
<li>The entire behavior of a time-homogeneous DTMC is captured by its transition probability matrix \\(P\\).</li>
<li>The m-step transition probabilities are conveniently calculated by finding the matrix power \\(P^m\\).</li>
<li>The long-term probabilities of a DTMC often converge to a unique stationary distribution \\(\pi\\), which can be found by computing \\(\lim_{n \to \infty} P^n\\) or by finding the left eigenvector of \\(P\\) corresponding to an eigenvalue of 1.</li>
</ul>
<br>
<p><b>2. Revisiting the Least Squares Problem</b></p>
<p>This section provides a more detailed look at the least squares solution for overdetermined systems \\(y = Ax\\), focusing on the conditions under which the standard solution is valid and how to proceed when it is not.</p>
<p><b>Main Topics Covered:</b></p>
<ul>
<li><b>The Standard Least Squares Solution:</b> For an overdetermined system (more equations than unknowns), the least squares solution that minimizes \\(\\|y - Ax\\|^2\\) is given by \\(\hat{x} = (A^T A)^{-1} A^T y\\).</li>
<li><b>Condition for the Standard Solution:</b> The formula above is only valid if the matrix \\(A^T A\\) is invertible. It was proven that \\(A^T A\\) is invertible if and only if the matrix \\(A\\) has <b>full column rank</b> (i.e., its columns are linearly independent).</li>
<li><b>The Rank-Deficient Case:</b> If \\(A\\) does not have full column rank (its rank is less than the number of columns), \\(A^T A\\) is not invertible, and the standard formula fails. In this scenario, there are infinitely many solutions for \\(x\\) that produce the same minimum least squares error.</li>
<li><b>Minimum Norm Least Squares Solution:</b> When infinite solutions exist, the standard practice is to choose the unique solution that has the minimum norm (\\(\\|x\\|\\)).</li>
<li><b>Solution via Singular Value Decomposition (SVD):</b> The SVD (\\(A = U \Sigma V^T\\)) provides a robust method to solve the least squares problem in all cases, including the rank-deficient one. By transforming the problem using the SVD matrices, a solution is derived that minimizes the error and, in the rank-deficient case, also minimizes the solution norm \\(\\|x\\|\\).</li>
</ul>
<p><b>Key Takeaways:</b></p>
<ul>
<li>The familiar least squares formula \\(\hat{x} = (A^T A)^{-1} A^T y\\) is not universally applicable; it requires the matrix \\(A\\) to have full column rank.</li>
<li>When \\(A\\) is rank-deficient, the SVD must be used to find the least squares solution. This approach naturally leads to the minimum norm solution among all possible solutions.</li>
<li>The general formula for the pseudo-inverse \\(A^\dagger\\) is \\(A^\dagger = V \Sigma^\dagger U^T\\), where \\(\Sigma^\dagger\\) is formed by taking the reciprocal of the non-zero singular values of \\(A\\) and keeping the zeros.</li>
<li>This SVD-based formula for the solution, \\(\hat{x} = V \Sigma^\dagger U^T y\\), is the most general form of the least squares solution, as it is valid whether \\(A\\) has full column rank or is rank-deficient.</li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<h3>Question 1: Sparse Signal Estimation</h3>
<p><b>Question:</b> Consider the sparse signal estimation problem \\( y = A h \\) where:</p>
\\[ y = \begin{bmatrix} 0 \\ -4 \\ 8 \\ 4 \end{bmatrix}, \quad A = \begin{bmatrix} 1 & 0 & 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 1 & 1 & 0 \\ 1 & 1 & 0 & 0 & 0 & 1 \\ 0 & 1 & 1 & 0 & 1 & 1 \end{bmatrix}, \quad h = \begin{bmatrix} h_1 \\ h_2 \\ h_3 \\ h_4 \\ h_5 \\ h_6 \end{bmatrix} \\]
<p>The question asks for the index of the column chosen in the first iteration of an algorithm like Orthogonal Matching Pursuit (OMP).</p>
<p><b>Explanation:</b> The first step in OMP is to find the column of the matrix \\(A\\) that is most correlated with the measurement vector \\(y\\). This is done by finding the column \\(a_j\\) that maximizes the absolute value of the inner product \\(|\langle y, a_j \rangle|\\) or \\(|y^T a_j|\\).</p>
<p>Let's calculate the inner product of \\(y\\) with each column of \\(A\\):</p>
<ul>
    <li>\\( y^T a_1 = [0, -4, 8, 4] \cdot [1, 0, 1, 0]^T = 0 \cdot 1 + (-4) \cdot 0 + 8 \cdot 1 + 4 \cdot 0 = 8 \\)</li>
    <li>\\( y^T a_2 = [0, -4, 8, 4] \cdot [0, 1, 1, 1]^T = 0 \cdot 0 + (-4) \cdot 1 + 8 \cdot 1 + 4 \cdot 1 = -4 + 8 + 4 = 8 \\)</li>
    <li>\\( y^T a_3 = [0, -4, 8, 4] \cdot [1, 0, 0, 1]^T = 0 \cdot 1 + (-4) \cdot 0 + 8 \cdot 0 + 4 \cdot 1 = 4 \\)</li>
    <li>\\( y^T a_4 = [0, -4, 8, 4] \cdot [0, 1, 0, 0]^T = 0 \cdot 0 + (-4) \cdot 1 + 8 \cdot 0 + 4 \cdot 0 = -4 \\)</li>
    <li>\\( y^T a_5 = [0, -4, 8, 4] \cdot [0, 1, 1, 1]^T = 0 \cdot 0 + (-4) \cdot 1 + 8 \cdot 1 + 4 \cdot 1 = -4 + 8 + 4 = 8 \\)</li>
    <li>\\( y^T a_6 = [0, -4, 8, 4] \cdot [1, 0, 1, 1]^T = 0 \cdot 1 + (-4) \cdot 0 + 8 \cdot 1 + 4 \cdot 1 = 8 + 4 = 12 \\)</li>
</ul>
<p>The absolute values of these inner products are 8, 8, 4, 4, 8, and 12. The maximum value is 12, which corresponds to the 6th column (\\(a_6\\)). Therefore, the index chosen in the first iteration should be 6.</p>
<p><i>Note: The provided answer is "5", and the submission was marked incorrect. Based on the standard OMP algorithm and the data visible in the image, the calculated answer is 6. There may be an error in the question's provided matrix, the vector y, or the accepted answer.</i></p>

<h3>Question 2: SVM Hyperplane Distance</h3>
<p><b>Question:</b> In a Support Vector Machine (SVM), two hyperplanes are given by the equations:</p>
\\[ \bar{x}_1 + 2\bar{x}_2 + 3\bar{x}_3 + \dots + N\bar{x}_N = \sqrt{2} \\]
\\[ \bar{x}_1 + 2\bar{x}_2 + 3\bar{x}_3 + \dots + N\bar{x}_N = -\sqrt{2} \\]
<p>What is the distance between these hyperplanes?</p>
<p><b>Explanation:</b> The distance between two parallel hyperplanes given by \\(w^T x = c_1\\) and \\(w^T x = c_2\\) is calculated using the formula:
\\[ \text{Distance} = \frac{|c_1 - c_2|}{\|w\|} \\]
In this problem:
<ul>
    <li>The weight vector is \\(w = [1, 2, 3, \dots, N]^T\\).</li>
    <li>The constants are \\(c_1 = \sqrt{2}\\) and \\(c_2 = -\sqrt{2}\\).</li>
</ul>
First, we calculate the L2-norm of the weight vector \\(w\\):
\\[ \|w\|^2 = 1^2 + 2^2 + 3^2 + \dots + N^2 \\]
This is the sum of the first \\(N\\) squares, for which the formula is \\(\frac{N(N+1)(2N+1)}{6}\\).
So, \\(\|w\| = \sqrt{\frac{N(N+1)(2N+1)}{6}}\\).
Now, we can find the distance:
\\[ \text{Distance} = \frac{|\sqrt{2} - (-\sqrt{2})|}{\sqrt{\frac{N(N+1)(2N+1)}{6}}} = \frac{2\sqrt{2}}{\sqrt{\frac{N(N+1)(2N+1)}{6}}} \\]
\\[ \text{Distance} = 2\sqrt{2} \cdot \sqrt{\frac{6}{N(N+1)(2N+1)}} = \sqrt{8} \cdot \sqrt{\frac{6}{N(N+1)(2N+1)}} \\]
\\[ \text{Distance} = \sqrt{\frac{48}{N(N+1)(2N+1)}} = \frac{\sqrt{16 \cdot 3}}{\sqrt{N(N+1)(2N+1)}} = \frac{4\sqrt{3}}{\sqrt{N(N+1)(2N+1)}} \\]
The correct answer is \\(\frac{4\sqrt{3}}{\sqrt{N(N+1)(2N+1)}}\\).</p>

<h3>Question 3: K-means Centroid Update</h3>
<p><b>Question:</b> In the k-means clustering algorithm with points \\(\bar{x}^{(j)}, j=1, 2, \dots, m\\), let \\(\alpha_i^{(l)}(j)\\) be the cluster assignment indicator which is 1 if point \\(\bar{x}^{(j)}\\) is assigned to cluster \\(i\\) in iteration \\(l\\), and 0 otherwise. How is the centroid \\(\bar{\mu}_i^{(l)}\\) of cluster \\(i\\) evaluated?</p>
<p><b>Explanation:</b> The k-means algorithm alternates between two steps: assigning points to the nearest cluster and updating the cluster's centroid. The update step involves recalculating the centroid of each cluster to be the mean (average) of all data points assigned to it.
To calculate the new centroid \\(\bar{\mu}_i^{(l)}\\) for cluster \\(i\\), we perform two operations:
<ol>
    <li>Sum all the data points \\(\bar{x}^{(j)}\\) that are currently assigned to cluster \\(i\\). Using the indicator function, this sum is \\( \sum_{j=1}^{m} \alpha_i^{(l)}(j) \bar{x}^{(j)} \\).</li>
    <li>Count the number of points in cluster \\(i\\), which is \\( \sum_{j=1}^{m} \alpha_i^{(l)}(j) \\).</li>
</ol>
The centroid is the sum of the points divided by the count:
\\[ \bar{\mu}_i^{(l)} = \frac{\sum_{j=1}^{m} \alpha_i^{(l)}(j) \bar{x}^{(j)}}{\sum_{j=1}^{m} \alpha_i^{(l)}(j)} \\]
The accepted answer matches this formula.</p>

<h3>Question 4: Linear Dynamical System</h3>
<p><b>Question:</b> A linear dynamical system is modeled as:
\\[ \frac{d}{dt}\bar{v}(t) = H_1\bar{v}(t), \quad 0 \le t < 2T \\]
\\[ \frac{d}{dt}\bar{v}(t) = H_2\bar{v}(t), \quad 2T \le t < 4T \\]
The output \\(\bar{v}(3T)\\) at time \\(t=3T\\) is given as what, in terms of the initial state \\(\bar{v}(0)\\)?</p>
<p><b>Explanation:</b> This system's behavior is described by two different matrices, \\(H_1\\) and \\(H_2\\), over two consecutive time intervals. We must solve for the state evolution piece by piece.
The solution to the differential equation \\(\frac{d}{dt}\bar{v}(t) = H\bar{v}(t)\\) from an initial time \\(t_0\\) is \\(\bar{v}(t) = e^{H(t-t_0)}\bar{v}(t_0)\\).
<ol>
    <li><b>Interval 1 (0 to 2T):</b> The system evolves with \\(H_1\\). The state at time \\(2T\\) is found by evolving from \\(t=0\\):
    \\[ \bar{v}(2T) = e^{H_1(2T - 0)}\bar{v}(0) = e^{2TH_1}\bar{v}(0) \\]
    </li>
    <li><b>Interval 2 (2T to 3T):</b> The system evolves with \\(H_2\\). The state at time \\(3T\\) is found by evolving from \\(t=2T\\), using \\(\bar{v}(2T)\\) as the new initial condition:
    \\[ \bar{v}(3T) = e^{H_2(3T - 2T)}\bar{v}(2T) = e^{TH_2}\bar{v}(2T) \\]
    </li>
</ol>
By substituting the expression for \\(\bar{v}(2T)\\) from step 1 into the equation from step 2, we get the final state at \\(3T\\):
\\[ \bar{v}(3T) = e^{TH_2} \left( e^{2TH_1}\bar{v}(0) \right) = e^{TH_2} e^{2TH_1} \bar{v}(0) \\]
Note that matrix exponentials do not generally commute, so the order \\(e^{TH_2} e^{2TH_1}\\) is important.</p>

<h3>Question 5: Matrix Exponential with Eigendecomposition</h3>
<p><b>Question:</b> Given the eigenvalue decomposition of a matrix \\(H\\) as \\(H = U\Lambda U^{-1}\\), the matrix exponential \\(e^{tH}\\) reduces to what expression?</p>
<p><b>Explanation:</b> The matrix exponential \\(e^A\\) is defined by its Taylor series expansion: \\(e^A = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \dots \\).
First, let's find the expression for powers of \\(H\\) using its eigendecomposition:
\\[ H^2 = (U\Lambda U^{-1})(U\Lambda U^{-1}) = U\Lambda(U^{-1}U)\Lambda U^{-1} = U\Lambda^2 U^{-1} \\]
In general, \\(H^k = U\Lambda^k U^{-1}\\).
Now, we substitute this into the Taylor series for \\(e^{tH}\\):
\\[ e^{tH} = I + tH + \frac{(tH)^2}{2!} + \dots = I + t(U\Lambda U^{-1}) + \frac{t^2(U\Lambda^2 U^{-1})}{2!} + \dots \\]
Factoring out \\(U\\) and \\(U^{-1}\\) (and noting that \\(I = UIU^{-1}\\)):
\\[ e^{tH} = U(I)U^{-1} + U(t\Lambda)U^{-1} + U\left(\frac{(t\Lambda)^2}{2!}\right)U^{-1} + \dots \\]
\\[ e^{tH} = U \left( I + t\Lambda + \frac{(t\Lambda)^2}{2!} + \dots \right) U^{-1} \\]
The expression in the parentheses is the Taylor series for \\(e^{t\Lambda}\\). Therefore:
\\[ e^{tH} = U e^{t\Lambda} U^{-1} \\]
This is the correct simplification.</p>

<h3>Question 6: Sparse Regression Column Selection</h3>
<p><b>Question:</b> In the sparse regression problem \\(\bar{y} = X\bar{\theta}\\), where \\(X = [\bar{X}_1, \bar{X}_2, \dots, \bar{X}_n]\\), how is the column most similar to \\(\bar{y}\\) determined?</p>
<p><b>Explanation:</b> This question describes the first step of a greedy algorithm like Orthogonal Matching Pursuit (OMP), which is used to solve sparse regression problems. The goal is to find which column of the matrix \\(X\\) (which represents a feature or basis vector) is most "aligned" or "correlated" with the output vector \\(\bar{y}\\).
Similarity between vectors is measured by their inner product (dot product). A larger inner product indicates a stronger projection of one vector onto another. To find the most similar column \\(\bar{X}_j\\), we should maximize the inner product \\(\bar{X}_j^T \bar{y}\\).
Therefore, the index of the most similar column is found by:
\\[ \arg \max_j \bar{X}_j^T \bar{y} \\]
<i>Note: More generally, one maximizes the absolute value, \\(\arg \max_j |\bar{X}_j^T \bar{y}|\\), but among the given choices, maximizing the inner product itself is the intended answer.</i></p>

<h3>Question 7: Autonomous Linear Dynamical System Model</h3>
<p><b>Question:</b> What is the general model for an autonomous Linear Dynamical System?</p>
<p><b>Explanation:</b> Let's break down the term:
<ul>
    <li><b>Dynamical System:</b> A system whose state, \\(\bar{v}(t)\\), evolves over time. Its evolution is described by a differential equation.</li>
    <li><b>Linear:</b> The rate of change of the state, \\(\frac{d}{dt}\bar{v}(t)\\), is a linear function of the state \\(\bar{v}(t)\\). This means it can be represented by a matrix multiplication: \\(\frac{d}{dt}\bar{v}(t) = H \bar{v}(t)\\).</li>
    <li><b>Autonomous:</b> The rule governing the system's evolution does not explicitly depend on time. In the equation \\(\frac{d}{dt}\bar{v}(t) = H \bar{v}(t)\\), this means the matrix \\(H\\) is constant and does not change with time (i.e., \\(H\\) is not a function \\(H(t)\\)).</li>
</ul>
Combining these properties, the general model for an autonomous linear dynamical system is:
\\[ \frac{d}{dt}\bar{v}(t) = H\bar{v}(t) \\]
where \\(H\\) is a constant matrix.</p>

<h3>Question 8: Sparse Vector Recovery Technique</h3>
<p><b>Question:</b> Consider the linear system \\(\bar{y} = X\bar{\theta}\\), where \\(X\\) is an \\(m \times n\\) matrix with \\(m \ll n\\) (a "fat" matrix). The vector \\(\bar{\theta}\\) is known to be sparse (contains many zeros). Which technique can be used to determine \\(\bar{\theta}\\)?</p>
<p><b>Explanation:</b> This is a classic underdetermined system of equations, meaning there are fewer equations (m) than unknowns (n), leading to infinite solutions. The additional information that \\(\bar{\theta}\\) is sparse allows us to find a unique, meaningful solution.
Let's evaluate the options:
<ul>
    <li><b>Least Squares:</b> This method minimizes \\(\|\bar{y} - X\bar{\theta}\|_2^2\\). For an underdetermined system, it does not yield a unique solution and does not enforce sparsity.</li>
    <li><b>Least Norm:</b> This finds the solution to \\(\bar{y} = X\bar{\theta}\\) that has the minimum L2 norm (\\(\|\bar{\theta}\|_2\\)). The resulting vector is unique but generally not sparse.</li>
    <li><b>Orthogonal Matching Pursuit (OMP):</b> This is a greedy iterative algorithm specifically designed to find sparse solutions to underdetermined linear systems. It is a standard method for sparse recovery and compressed sensing.</li>
    <li><b>Principal Component Analysis (PCA):</b> This is a statistical method for dimensionality reduction. It is not used for solving systems of linear equations.</li>
</ul>
Therefore, Orthogonal Matching Pursuit is the correct technique for this problem.</p>

<h3>Question 9: Sparsity in Sparse Regression</h3>
<p><b>Question:</b> In the sparse regression problem \\(\bar{y} = X\bar{\theta}\\), which of the following statements is true?</p>
<p><b>Explanation:</b> The term "sparse regression" refers to a regression scenario where the goal is to model an output vector \\(\bar{y}\\) as a linear combination of the columns of a matrix \\(X\\), under the assumption that only a few of these columns are needed.
The model is \\(\bar{y} = \theta_1 \bar{X}_1 + \theta_2 \bar{X}_2 + \dots + \theta_n \bar{X}_n\\). The "sparsity" constraint or assumption applies to the vector of regression coefficients, \\(\bar{\theta} = [\theta_1, \theta_2, \dots, \theta_n]^T\\). It means that most of the coefficients \\(\theta_j\\) are expected to be zero.
Therefore, the defining characteristic of sparse regression is that <b>the vector of regression coefficients \\(\bar{\theta}\\) is sparse</b>.</p>

<h3>Question 10: SVM Hyperplane Distance (Numerical)</h3>
<p><b>Question:</b> In an SVM, the hyperplanes for classification are given by:</p>
\\[ 3x_1 - 4x_2 + 2 \ge 1 \\]
\\[ 3x_1 - 4x_2 + 2 \le -1 \\]
<p>What is the distance between these hyperplanes?</p>
<p><b>Explanation:</b> The hyperplanes are the boundaries where the equalities hold. Let's rewrite the equations for the two hyperplanes:
<ol>
    <li>\\(3x_1 - 4x_2 + 2 = 1 \implies 3x_1 - 4x_2 = -1\\)</li>
    <li>\\(3x_1 - 4x_2 + 2 = -1 \implies 3x_1 - 4x_2 = -3\\)</li>
</ol>
These two planes are parallel. We can use the formula for the distance between two parallel planes, \\(w^T x = c_1\\) and \\(w^T x = c_2\\), which is:
\\[ \text{Distance} = \frac{|c_1 - c_2|}{\|w\|} \\]
From the equations, we can identify:
<ul>
    <li>The weight vector \\(w = [3, -4]^T\\).</li>
    <li>The constants \\(c_1 = -1\\) and \\(c_2 = -3\\).</li>
</ul>
First, calculate the norm of \\(w\\):
\\[ \|w\| = \sqrt{3^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5 \\]
Now, apply the distance formula:
\\[ \text{Distance} = \frac{|-1 - (-3)|}{5} = \frac{|2|}{5} = \frac{2}{5} \\]
The distance between the hyperplanes is \\( \frac{2}{5} \\).</p>
</div></div><div class="week" id="week_12"><h1 class="week-title">Week 12</h1><h2>Transcript Explanations</h2><div class="chapter" id="noc21_ee33-Lec 63"><h3 class="heading">noc21_ee33-Lec 63</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas related to <b>Weighted Least Squares (WLS)</b>, as presented in the transcript. WLS is introduced as a powerful and useful generalization of the standard Least Squares (LS) principle.</p>

<h3>1. Review of Standard Least Squares (LS)</h3>
<p>The conventional Least Squares (LS) problem aims to find the best approximation for a system of linear equations that may not have an exact solution, typically when there are more equations than unknowns. This is often the case when dealing with experimental data.</p>
<p><b>The Objective:</b></p>
<p>The goal is to find a vector \\(\bar{x}\\) that minimizes the squared Euclidean norm (or length) of the error vector. The error is the difference between the observed data vector \\(\bar{y}\\) and the model's prediction, \\(A\bar{x}\\).</p>
<p>The cost function to be minimized is:</p>
\\[ \min_{\bar{x}} ||\bar{y} - A\bar{x}||^2 \\]
<p>Where:</p>
<ul>
    <li>\\(\bar{y}\\) is the \\(m \times 1\\) observation vector.</li>
    <li>\\(A\\) is an \\(m \times n\\) matrix, often a "tall matrix" where the number of rows \\(m\\) (equations) is greater than or equal to the number of columns \\(n\\) (unknowns), i.e., \\(m \ge n\\).</li>
    <li>\\(\bar{x}\\) is the \\(n \times 1\\) vector of unknown parameters we want to find.</li>
    <li>The term \\(\bar{e} = \bar{y} - A\bar{x}\\) represents the error vector. Minimizing \\(||\bar{e}||^2\\) means minimizing the sum of the squares of the individual errors.</li>
</ul>

<p><b>The Solution:</b></p>
<p>The unique solution \\(\hat{x}\\) that minimizes this cost function is given by the normal equations, leading to the formula:</p>
\\[ \hat{x} = (A^T A)^{-1} A^T \bar{y} \\]
<p>The matrix \\((A^T A)^{-1} A^T\\) is known as the <b>pseudo-inverse</b> of \\(A\\).</p>

<h3>2. Introduction to Weighted Least Squares (WLS)</h3>
<p>Weighted Least Squares (WLS) extends the standard LS problem by allowing some errors to be treated as more significant than others. Instead of minimizing the simple sum of squared errors, WLS introduces a <b>weighting matrix</b> \\(W\\) to assign a specific weight to each error component.</p>

<p><b>The WLS Cost Function:</b></p>
<p>The standard LS cost function can be written using vector transpose notation as:</p>
\\[ (\bar{y} - A\bar{x})^T (\bar{y} - A\bar{x}) \\]
<p>In the WLS formulation, the weighting matrix \\(W\\) is inserted into this expression, creating a new cost function:</p>
\\[ f(\bar{x}) = (\bar{y} - A\bar{x})^T W (\bar{y} - A\bar{x}) \\]
<p>This is the central cost function for the WLS problem. If we define the error vector as \\(\bar{e} = \bar{y} - A\bar{x}\\), the cost function is simply \\(\bar{e}^T W \bar{e}\\). This quadratic form can be expanded as:</p>
\\[ \bar{e}^T W \bar{e} = \sum_{i=1}^{m} \sum_{j=1}^{m} e_i w_{ij} e_j \\]
<p>Here, \\(e_i\\) and \\(e_j\\) are components of the error vector, and \\(w_{ij}\\) are the "weighting coefficients" from the matrix \\(W\\). This shows that the cost is no longer a simple sum of squared errors but a weighted combination of products of errors.</p>

<p><b>Relation to Standard LS:</b></p>
<p>WLS reduces to the standard LS problem if the weighting matrix \\(W\\) is the identity matrix (\\(I\\)). In this case, \\(w_{ij} = 1\\) if \\(i=j\\) and \\(0\\) otherwise. The cost function simplifies to:</p>
\\[ \sum_{i=1}^{m} e_i (1) e_i = \sum_{i=1}^{m} e_i^2 = ||\bar{e}||^2 \\]
<p>This is exactly the standard LS cost function.</p>

<h3>3. Properties of the Weighting Matrix \\(W\\)</h3>
<p>The weighting matrix \\(W\\) cannot be an arbitrary matrix. To ensure that the cost function \\(\bar{e}^T W \bar{e}\\) behaves like a squared error (i.e., it is always non-negative), \\(W\\) must be a <b>positive semi-definite (PSD)</b> matrix.</p>
<p>A matrix \\(W\\) is positive semi-definite if for any non-zero vector \\(\bar{z}\\):</p>
\\[ \bar{z}^T W \bar{z} \ge 0 \\]
<p>This property guarantees that the WLS cost function is always greater than or equal to zero. PSD matrices have several key properties, including:</p>
<ul>
    <li>They are symmetric (\\(W = W^T\\)).</li>
    <li>Their diagonal elements are non-negative (\\(w_{ii} \ge 0\\)).</li>
    <li>All their eigenvalues are non-negative.</li>
</ul>

<h3>4. Decomposing the WLS Cost Function</h3>
<p>A key property of any positive semi-definite matrix \\(W\\) is that it can be decomposed into the product of a matrix and its transpose. This is known as the <b>Cholesky decomposition</b> or matrix square root.</p>
\\[ W = (W^{1/2})^T W^{1/2} \\]
<p>Here, \\(W^{1/2}\\) is the "square root" of the matrix \\(W\\). This decomposition allows us to rewrite the WLS cost function in a form that resembles the standard LS problem.</p>

<p>Starting with the WLS cost function:</p>
\\[ (\bar{y} - A\bar{x})^T W (\bar{y} - A\bar{x}) \\]
<p>Substitute the decomposition of \\(W\\):</p>
\\[ (\bar{y} - A\bar{x})^T (W^{1/2})^T W^{1/2} (\bar{y} - A\bar{x}) \\]
<p>Using the property \\((BC)^T = C^T B^T\\), we can group the terms:</p>
\\[ (W^{1/2}(\bar{y} - A\bar{x}))^T (W^{1/2}(\bar{y} - A\bar{x})) \\]
<p>This is the squared norm of the vector \\(W^{1/2}(\bar{y} - A\bar{x})\\):</p>
\\[ || W^{1/2}(\bar{y} - A\bar{x}) ||^2 \\]
<p>This is a compact and insightful representation of the WLS cost function. It shows that WLS is equivalent to performing a standard least squares minimization on a "weighted" or "transformed" version of the error vector.</p>

<h3>5. Derivation of the WLS Solution</h3>
<p>To find the vector \\(\hat{x}\\) that minimizes the WLS cost function, we must take the gradient of the cost function with respect to \\(\bar{x}\\) and set it to zero.</p>

<p><b>Step 1: Expand the cost function \\(f(\bar{x})\\)</b></p>
\\[ f(\bar{x}) = (\bar{y} - A\bar{x})^T W (\bar{y} - A\bar{x}) \\]
\\[ f(\bar{x}) = (\bar{y}^T - \bar{x}^T A^T) W (\bar{y} - A\bar{x}) \\]
\\[ f(\bar{x}) = \bar{y}^T W \bar{y} - \bar{y}^T W A \bar{x} - \bar{x}^T A^T W \bar{y} + \bar{x}^T A^T W A \bar{x} \\]
<p>Since the middle two terms are scalars and transposes of each other, they are equal. Thus, we can combine them:</p>
\\[ f(\bar{x}) = \bar{y}^T W \bar{y} - 2\bar{x}^T A^T W \bar{y} + \bar{x}^T (A^T W A) \bar{x} \\]

<p><b>Step 2: Compute the gradient \\(\nabla_{\bar{x}} f(\bar{x})\\)</b></p>
<p>We use the following standard matrix calculus identities:</p>
<ul>
    <li>The gradient of a constant is zero: \\(\nabla_{\bar{x}} (\text{constant}) = \bar{0}\\).</li>
    <li>\\(\nabla_{\bar{x}} (\bar{x}^T \bar{c}) = \bar{c}\\).</li>
    <li>\\(\nabla_{\bar{x}} (\bar{x}^T P \bar{x}) = 2P\bar{x}\\), provided \\(P\\) is a symmetric matrix.</li>
</ul>
<p>Applying these to our expanded cost function:</p>
<ul>
    <li>The term \\(\bar{y}^T W \bar{y}\\) is constant with respect to \\(\bar{x}\\), so its gradient is \\(\bar{0}\\).</li>
    <li>The term \\(-2\bar{x}^T (A^T W \bar{y})\\) is of the form \\(-2\bar{x}^T \bar{c}\\). Its gradient is \\(-2A^T W \bar{y}\\).</li>
    <li>The term \\(\bar{x}^T (A^T W A) \bar{x}\\) is of the form \\(\bar{x}^T P \bar{x}\\), where \\(P = A^T W A\\). Since \\(W\\) is symmetric, \\(P\\) is also symmetric. Its gradient is \\(2(A^T W A)\bar{x}\\).</li>
</ul>
<p>Combining these results, the gradient is:</p>
\\[ \nabla_{\bar{x}} f(\bar{x}) = \bar{0} - 2A^T W \bar{y} + 2A^T W A \bar{x} \\]

<p><b>Step 3: Set the gradient to zero and solve for \\(\hat{x}\\)</b></p>
\\[ -2A^T W \bar{y} + 2A^T W A \hat{x} = \bar{0} \\]
\\[ 2A^T W A \hat{x} = 2A^T W \bar{y} \\]
\\[ A^T W A \hat{x} = A^T W \bar{y} \\]
<p>Finally, we solve for \\(\hat{x}\\) by multiplying by the inverse of \\(A^T W A\\):</p>
\\[ \hat{x} = (A^T W A)^{-1} A^T W \bar{y} \\]
<p>This is the <b>Weighted Least Squares (WLS) solution</b>. It provides the optimal vector \\(\hat{x}\\) that minimizes the weighted squared error. As noted before, if \\(W = I\\), this formula simplifies directly to the standard LS solution.</p>

</div></div><div class="chapter" id="noc21_ee33-Lec 64"><h3 class="heading">noc21_ee33-Lec 64</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas discussed in the transcript, focusing on the theory of Weighted Least Squares (WLS) and its application to a specific estimation problem.</p>

<h3>1. The Weighted Least Squares (WLS) Cost Function</h3>
<p>The discussion begins by contrasting the Conventional Least Squares (CLS) method with the Weighted Least Squares (WLS) method. The goal of both is to find an estimate \\(\bar{x}\\) that minimizes the difference between the observed data \\(\bar{y}\\) and the model prediction \\(A\bar{x}\\).</p>
<p><b>Conventional Least Squares (CLS):</b></p>
<p>The CLS cost function is the squared Euclidean norm of the residual vector \\((\bar{y} - A\bar{x})\\).</p>
\\[ J_{CLS}(\bar{x}) = ||\bar{y} - A\bar{x}||^2 = (\bar{y} - A\bar{x})^T (\bar{y} - A\bar{x}) \\]

<p><b>Weighted Least Squares (WLS):</b></p>
<p>The WLS method introduces a <b>weighting matrix</b> \\(W\\) to the cost function. This matrix allows us to assign different levels of importance or confidence to different measurements in \\(\bar{y}\\).</p>
\\[ J_{WLS}(\bar{x}) = (\bar{y} - A\bar{x})^T W (\bar{y} - A\bar{x}) \\]
<p>The components are:</p>
<ul>
    <li>\\(\bar{y}\\): An \\(m \times 1\\) vector of observations.</li>
    <li>\\(A\\): An \\(m \times n\\) matrix representing the model.</li>
    <li>\\(\bar{x}\\): An \\(n \times 1\\) vector of parameters to be estimated.</li>
    <li>\\(W\\): An \\(m \times m\\) weighting matrix.</li>
</ul>

<h4>Properties of the Weighting Matrix \\(W\\)</h4>
<p>The weighting matrix \\(W\\) is not just any matrix; it must be <b>positive semi-definite</b>. This property ensures that the cost function represents a squared quantity and is always non-negative. Key properties of a positive semi-definite matrix include:</p>
<ul>
    <li>Its eigenvalues are non-negative (\\(\lambda_i \ge 0\\)).</li>
    <li>It can be decomposed into the form \\(W = C^T C\\) for some matrix \\(C\\). A common decomposition is using the matrix square root, \\(W = (W^{1/2})^T W^{1/2}\\).</li>
</ul>
<p>Using this decomposition, the WLS cost function can be rewritten as a standard squared norm, which clarifies its structure:</p>
\\[ J_{WLS}(\bar{x}) = (\bar{y} - A\bar{x})^T (W^{1/2})^T W^{1/2} (\bar{y} - A\bar{x}) = ||W^{1/2}(\bar{y} - A\bar{x})||^2 \\]
<p>This shows that WLS is equivalent to transforming the original problem with \\(W^{1/2}\\) and then applying conventional least squares.</p>

<h3>2. The WLS Estimate Formula</h3>
<p>By taking the derivative of the WLS cost function with respect to \\(\bar{x}\\) and setting it to zero, one can derive the closed-form solution for the WLS estimate, denoted as \\(\hat{x}\\).</p>
\\[ \hat{x}_{WLS} = (A^T W A)^{-1} A^T W \bar{y} \\]
<p>This formula gives the optimal parameter vector \\(\bar{x}\\) that minimizes the weighted sum of squared errors.</p>

<h3>3. Example: Estimating a Scalar from Noisy Observations</h3>
<p>To make the concept concrete, the transcript presents a simple example of estimating a single scalar parameter.</p>
<p><b>Problem Setup:</b></p>
<ul>
    <li><b>Scalar Parameter:</b> The number of parameters is \\(n=1\\), so the vector \\(\bar{x}\\) becomes a scalar \\(x\\).</li>
    <li><b>Model Matrix:</b> The matrix \\(A\\) is an \\(m \times 1\\) vector, which is set to be the vector of all ones, denoted \\(\bar{1}\\).
        \\[ A = \bar{a} = \bar{1} = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \\]
    </li>
</ul>
<p><b>The Observation Model:</b></p>
<p>The model describes making \\(m\\) noisy measurements (\\(y_1, y_2, \dots, y_m\\)) of the same scalar quantity \\(x\\).</p>
\\[ \bar{y} = A x + \bar{v} \implies \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} x + \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_m \end{bmatrix} \\]
<p>Here, \\(\bar{v}\\) is a vector of random noise affecting each measurement.</p>

<h4>Properties of the Noise</h4>
<p>The statistical properties of the noise are crucial for determining the optimal weighting matrix. The noise is assumed to be:</p>
<ul>
    <li><b>Zero-Mean Gaussian Noise:</b> \\(E[v_i] = 0\\).</li>
    <li><b>Uncorrelated:</b> The noise on one measurement is statistically independent of the noise on another. \\(E[v_i v_j] = 0\\) for \\(i \neq j\\).</li>
    <li><b>Non-Identically Distributed:</b> Each measurement can have a different noise level, meaning the variances are not all the same. The variance of the \\(i\\)-th noise sample is \\(E[v_i^2] = \sigma_i^2\\).</li>
</ul>
<p>This type of noise is called <b>Independent Non-Identically Distributed (i.n.i.d.)</b> noise.</p>
<p>The <b>noise covariance matrix</b>, \\(R\\), captures these properties. For uncorrelated noise, \\(R\\) is a diagonal matrix where the diagonal entries are the variances of the respective noise components.</p>
\\[ R = \begin{bmatrix} \sigma_1^2 & 0 & \cdots & 0 \\ 0 & \sigma_2^2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma_m^2 \end{bmatrix} \\]

<h3>4. The Optimal Weighting Matrix</h3>
<p>A key insight presented is that for optimal estimation, the weighting matrix \\(W\\) should be the <b>inverse of the noise covariance matrix</b>.</p>
\\[ W_{optimal} = R^{-1} \\]
<p>For a diagonal covariance matrix, its inverse is simply a diagonal matrix with the reciprocals of the original diagonal elements.</p>
\\[ W = R^{-1} = \begin{bmatrix} 1/\sigma_1^2 & 0 & \cdots & 0 \\ 0 & 1/\sigma_2^2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1/\sigma_m^2 \end{bmatrix} \\]

<h4>Special Case: i.i.d. Noise</h4>
<p>If the noise were <b>Independent and Identically Distributed (i.i.d.)</b>, all variances would be equal, \\(\sigma_i^2 = \sigma^2\\). In this case:</p>
<ul>
    <li>The covariance matrix becomes \\(R = \sigma^2 I\\), where \\(I\\) is the identity matrix.</li>
    <li>The weighting matrix becomes \\(W = R^{-1} = \frac{1}{\sigma^2} I\\).</li>
</ul>
<p>The WLS cost function simplifies to:</p>
\\[ J_{WLS}(\bar{x}) = (\bar{y} - A\bar{x})^T \left(\frac{1}{\sigma^2}I\right) (\bar{y} - A\bar{x}) = \frac{1}{\sigma^2} ||\bar{y} - A\bar{x}||^2 \\]
<p>Since \\(1/\sigma^2\\) is a positive constant, minimizing this is equivalent to minimizing the CLS cost function \\(||\bar{y} - A\bar{x}||^2\\). This shows that <b>WLS reduces to CLS when the noise is i.i.d.</b></p>

<h3>5. Deriving the WLS Estimate for the Example</h3>
<p>We now apply the general WLS formula to our specific example, using \\(A = \bar{1}\\) and \\(W = R^{-1}\\).</p>
\\[ \hat{x} = (\bar{1}^T R^{-1} \bar{1})^{-1} \bar{1}^T R^{-1} \bar{y} \\]
<p>Let's evaluate the two main components:</p>
<p>1. <b>The term \\(\bar{1}^T R^{-1} \bar{1}\\):</b></p>
\\[ \bar{1}^T R^{-1} \bar{1} = \begin{bmatrix} 1 & \dots & 1 \end{bmatrix} \begin{bmatrix} 1/\sigma_1^2 & & 0 \\ & \ddots & \\ 0 & & 1/\sigma_m^2 \end{bmatrix} \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} = \sum_{i=1}^{m} \frac{1}{\sigma_i^2} \\]
<p>This is a scalar, so its inverse is its reciprocal.</p>
<p>2. <b>The term \\(\bar{1}^T R^{-1} \bar{y}\\):</b></p>
\\[ \bar{1}^T R^{-1} \bar{y} = \begin{bmatrix} 1 & \dots & 1 \end{bmatrix} \begin{bmatrix} 1/\sigma_1^2 & & 0 \\ & \ddots & \\ 0 & & 1/\sigma_m^2 \end{bmatrix} \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix} = \sum_{i=1}^{m} \frac{y_i}{\sigma_i^2} \\]

<p>Combining these, the final WLS estimate for \\(x\\) is:</p>
\\[ \hat{x} = \frac{\sum_{i=1}^{m} \frac{1}{\sigma_i^2} y_i}{\sum_{i=1}^{m} \frac{1}{\sigma_i^2}} \\]

<h3>6. The Intuition Behind the Result</h3>
<p>This final formula provides a powerful and intuitive result. It is a <b>weighted average</b> of the measurements \\(y_i\\). The weight for each measurement \\(y_i\\) is \\(w_i = 1/\sigma_i^2\\).</p>
<p>This weighting scheme is logical:</p>
<ul>
    <li>If a measurement \\(y_i\\) has a <b>large noise variance</b> (\\(\sigma_i^2\\) is large), it is considered unreliable. The corresponding weight \\(1/\sigma_i^2\\) will be small, so this measurement has less influence on the final estimate.</li>
    <li>If a measurement \\(y_i\\) has a <b>small noise variance</b> (\\(\sigma_i^2\\) is small), it is considered reliable. The corresponding weight \\(1/\sigma_i^2\\) will be large, giving this measurement more influence on the final estimate.</li>
</ul>
<p>This demonstrates why setting \\(W = R^{-1}\\) is the optimal choice. It automatically de-emphasizes noisy data and gives more credence to clean data, leading to a more accurate and robust estimate. If we had incorrectly chosen \\(W=R\\), noisier measurements would have received higher weights, leading to a poor estimation principle.</p>
</div></div><div class="chapter" id="noc21_ee33-Lec 65"><h3 class="heading">noc21_ee33-Lec 65</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the <b>Woodbury Matrix Identity</b>, also known as the <b>Matrix Inversion Lemma</b>. This identity is a powerful tool in linear algebra that simplifies the computation of a matrix inverse under certain conditions.</p>

<h3>1. The Woodbury Matrix Identity</h3>

<p>The Woodbury Matrix Identity provides an analytical expression for the inverse of a sum of two matrices. Specifically, it addresses the case where one matrix, <b>A</b>, is added to a low-rank matrix, which is represented as the product <b>UCV</b>. The identity is stated as follows:</p>
\\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} \\]
<p>At first glance, the right-hand side of the equation appears more complicated than the left-hand side. However, the identity's utility becomes clear when considering the dimensions and properties of the matrices involved.</p>

<h3>2. Motivation and Application</h3>

<p>The primary motivation for using the Woodbury Matrix Identity is to reduce the computational complexity of matrix inversion in specific, yet common, scenarios. The simplification occurs when:</p>
<ol>
    <li>The inverse of matrix <b>A</b>, denoted \\(A^{-1}\\), is either already known or easy to compute. For example, <b>A</b> could be an identity matrix, a diagonal matrix, or a unitary matrix.</li>
    <li>The matrix <b>UCV</b> represents a "low-rank update" to matrix <b>A</b>. This means the rank of <b>UCV</b> is much smaller than its dimensions.</li>
</ol>

<p><b>Low-Rank Structure</b></p>
<p>A low-rank structure often arises when the inner dimension of the matrix product is small. Consider the example from the transcript with the following matrix dimensions:</p>
<ul>
    <li><b>A</b> is a large \\(500 \times 500\\) matrix.</li>
    <li><b>U</b> is a "tall" \\(500 \times 20\\) matrix.</li>
    <li><b>C</b> is a small \\(20 \times 20\\) matrix.</li>
    <li><b>V</b> is a "flat" \\(20 \times 500\\) matrix.</li>
</ul>
<p>The product \\(UCV\\) is a \\(500 \times 500\\) matrix, the same size as <b>A</b>. However, its rank is at most 20 (the smallest dimension in the product chain). The term \\(A + UCV\\) is therefore a full-size \\(500 \times 500\\) matrix.</p>

<p><b>Computational Advantage</b></p>
<p>Without the Woodbury identity, calculating \\((A + UCV)^{-1}\\) would require inverting a dense \\(500 \times 500\\) matrix, which is computationally expensive.</p>
<p>Using the identity, the most complex operation becomes the inversion of the term \\(C^{-1} + VA^{-1}U\\). Let's check the dimensions of this term:</p>
<ul>
    <li><b>V</b> is \\(20 \times 500\\)</li>
    <li><b>A<sup>-1</sup></b> is \\(500 \times 500\\)</li>
    <li><b>U</b> is \\(500 \times 20\\)</li>
</ul>
<p>The product \\(VA^{-1}U\\) results in a \\(20 \times 20\\) matrix. Since \\(C^{-1}\\) is also \\(20 \times 20\\), the entire term \\(C^{-1} + VA^{-1}U\\) is a \\(20 \times 20\\) matrix. Inverting a \\(20 \times 20\\) matrix is significantly faster and less complex than inverting a \\(500 \times 500\\) matrix. The identity thus transforms a large inversion problem into a much smaller one, with the rest of the operations being matrix multiplications.</p>

<p>An extreme example is a <b>rank-one update</b>, where <b>U</b> is a column vector, <b>V</b> is a row vector, and <b>C</b> is a scalar. In this case, the inversion is reduced to a simple scalar division.</p>

<h3>3. Proof of the Woodbury Matrix Identity</h3>

<p>The proof relies on two preliminary properties that are first established.</p>

<h4>Property 1</h4>
<p>This property provides an alternative expression for \\((I+P)^{-1}\\), where <b>I</b> is the identity matrix.</p>
\\[ (I+P)^{-1} = I - (I+P)^{-1}P \\]
<p><b>Derivation:</b></p>
<ol>
    <li>Start with \\((I+P)^{-1}\\) and multiply it by the identity matrix, <b>I</b>:
    \\[ (I+P)^{-1} = (I+P)^{-1}I \\]
    </li>
    <li>Rewrite <b>I</b> by adding and subtracting <b>P</b>: \\(I = (I+P) - P\\).
    \\[ (I+P)^{-1} = (I+P)^{-1}((I+P) - P) \\]
    </li>
    <li>Distribute the term:
    \\[ (I+P)^{-1} = (I+P)^{-1}(I+P) - (I+P)^{-1}P \\]
    </li>
    <li>Simplify, since \\((I+P)^{-1}(I+P) = I\\):
    \\[ (I+P)^{-1} = I - (I+P)^{-1}P \\]
    </li>
</ol>

<h4>Property 2</h4>
<p>This property establishes a relationship between two expressions involving matrices <b>P</b> and <b>Q</b>. It is sometimes called the "push-through" identity.</p>
\\[ (I+PQ)^{-1}P = P(I+QP)^{-1} \\]
<p><b>Derivation:</b></p>
<ol>
    <li>Start with the expression \\(P(I+QP)\\). Distributing <b>P</b> gives \\(P + PQP\\).</li>
    <li>Now consider the expression \\((I+PQ)P\\). Distributing <b>P</b> gives \\(P + PQP\\).</li>
    <li>Since both expressions are equal, we have:
    \\[ (I+PQ)P = P(I+QP) \\]
    </li>
    <li>To isolate the desired terms, pre-multiply both sides by \\((I+PQ)^{-1}\\) and post-multiply both sides by \\((I+QP)^{-1}\\):
    \\[ (I+PQ)^{-1}(I+PQ)P(I+QP)^{-1} = (I+PQ)^{-1}P(I+QP)(I+QP)^{-1} \\]
    </li>
    <li>This simplifies to the final property:
    \\[ P(I+QP)^{-1} = (I+PQ)^{-1}P \\]
    </li>
</ol>

<h4>Main Proof</h4>
<p>The main proof applies these two properties to derive the Woodbury identity.</p>
<ol>
    <li>Start with the expression to be inverted, \\((A + UCV)^{-1}\\), and factor out <b>A</b> from the left:
    \\[ (A + UCV)^{-1} = [A(I + A^{-1}UCV)]^{-1} \\]
    </li>
    <li>Apply the inverse property \\((XY)^{-1} = Y^{-1}X^{-1}\\):
    \\[ (A + UCV)^{-1} = (I + A^{-1}UCV)^{-1} A^{-1} \\]
    </li>
    <li>Now, apply <b>Property 1</b> to the term \\((I + A^{-1}UCV)^{-1}\\) by setting \\(P = A^{-1}UCV\\):
    \\[ (I + A^{-1}UCV)^{-1} = I - (I + A^{-1}UCV)^{-1}(A^{-1}UCV) \\]
    </li>
    <li>Substitute this back into the main expression:
    \\[ (A + UCV)^{-1} = [I - (I + A^{-1}UCV)^{-1}(A^{-1}UCV)]A^{-1} \\]
    \\[ = A^{-1} - (I + A^{-1}UCV)^{-1}A^{-1}UCVA^{-1} \\]
    </li>
    <li>Focus on the sub-expression \\((I + A^{-1}UCV)^{-1}A^{-1}U\\). We can apply <b>Property 2</b> here by setting \\(P = A^{-1}U\\) and \\(Q = CV\\). This gives:
    \\[ (I + A^{-1}UCV)^{-1}A^{-1}U = A^{-1}U(I + CVA^{-1}U)^{-1} \\]
    </li>
    <li>Substitute this result back into the main equation from step 4:
    \\[ (A + UCV)^{-1} = A^{-1} - [A^{-1}U(I + CVA^{-1}U)^{-1}]CVA^{-1} \\]
    </li>
    <li>Now, let's simplify the term \\((I + CVA^{-1}U)^{-1}\\). We can factor out <b>C</b> from the expression inside the inverse (assuming <b>C</b> is invertible):
    \\[ I + CVA^{-1}U = C C^{-1} + CVA^{-1}U = C(C^{-1} + VA^{-1}U) \\]
    Taking the inverse gives:
    \\[ (I + CVA^{-1}U)^{-1} = [C(C^{-1} + VA^{-1}U)]^{-1} = (C^{-1} + VA^{-1}U)^{-1}C^{-1} \\]
    </li>
    <li>Substitute this final piece back into the expression from step 6:
    \\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U[(C^{-1} + VA^{-1}U)^{-1}C^{-1}]CVA^{-1} \\]
    </li>
    <li>The \\(C^{-1}C\\) terms cancel to become the identity matrix, leaving the final form of the Woodbury Matrix Identity:
    \\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} \\]
    </li>
</ol>
<p>This completes the proof, demonstrating how a computationally intensive large matrix inversion can be transformed into a smaller one, which is particularly useful in fields like signal processing, machine learning, and communications.</p>
</div></div><div class="chapter" id="noc21_ee33-Lec 66"><h3 class="heading">noc21_ee33-Lec 66</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas discussed in the transcript, focusing on the Woodbury Matrix Identity and its application to a rank-one matrix update.</p>

<b>1. The Woodbury Matrix Identity</b>
<p>The core concept discussed is the <b>Woodbury Matrix Identity</b>, also known as the <b>Matrix Inversion Lemma</b>. This identity provides an analytical expression for the inverse of a matrix that has been modified by a low-rank update. It is particularly useful when the inverse of the original matrix is already known or easy to compute.</p>
<p>The general form of the identity is given for a matrix \\(A + UCV\\), where \\(A, U, C, V\\) are matrices of compatible dimensions. Its inverse is expressed as:</p>
\\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} \\]
<p><b>Key conditions for its utility:</b></p>
<ul>
    <li>The matrix \\(A\\) must be invertible, and its inverse \\(A^{-1}\\) should be known or easy to find.</li>
    <li>The matrix \\(C\\) must also be invertible.</li>
    <li>The identity is most powerful when the matrix \\(UCV\\) is a <b>low-rank matrix</b>. This often occurs when \\(U\\) is a "tall" matrix (more rows than columns) and \\(V\\) is a "flat" or "wide" matrix (more columns than rows). In such cases, the matrix \\(C^{-1} + VA^{-1}U\\) is much smaller in dimension than the original matrix \\(A + UCV\\), making its inversion computationally cheaper.</li>
</ul>

<b>2. Application: Inverse of an Identity Matrix plus a Rank-One Matrix</b>
<p>The transcript demonstrates a classic and very useful application of this identity: finding the inverse of a matrix formed by adding a rank-one matrix to the identity matrix. A rank-one matrix can be expressed as the outer product of two vectors, in this case, a vector \\(\mathbf{x}\\) with its own transpose, \\(\mathbf{x}\mathbf{x}^T\\).</p>
<p>The problem is to compute the inverse of:</p>
\\[ I + \mathbf{x}\mathbf{x}^T \\]
<p>To apply the Woodbury formula, we map the terms as follows:</p>
\\[ A + UCV \rightarrow I + \mathbf{x}(1)\mathbf{x}^T \\]
<ul>
    <li>\\(A = I\\) (the \\(n \times n\\) identity matrix). Its inverse is trivial: \\(A^{-1} = I\\).</li>
    <li>\\(U = \mathbf{x}\\) (an \\(n \times 1\\) column vector, a "tall" matrix).</li>
    <li>\\(C = 1\\) (a \\(1 \times 1\\) scalar). Its inverse is also trivial: \\(C^{-1} = 1\\).</li>
    <li>\\(V = \mathbf{x}^T\\) (a \\(1 \times n\\) row vector, a "flat" matrix).</li>
</ul>
<p>Substituting these into the Woodbury identity gives:</p>
\\[ (I + \mathbf{x}\mathbf{x}^T)^{-1} = I^{-1} - I^{-1}\mathbf{x}(1^{-1} + \mathbf{x}^T I^{-1} \mathbf{x})^{-1}\mathbf{x}^T I^{-1} \\]
<p>This expression simplifies significantly:</p>
<p><b>Step 1: Simplify the inverses.</b><br>
Since \\(I^{-1} = I\\) and \\(1^{-1} = 1\\), the formula becomes:</p>
\\[ (I + \mathbf{x}\mathbf{x}^T)^{-1} = I - I\mathbf{x}(1 + \mathbf{x}^T I \mathbf{x})^{-1}\mathbf{x}^T I \\]
<p><b>Step 2: Simplify the matrix products.</b><br>
Multiplying by the identity matrix \\(I\\) doesn't change the other terms (\\(I\mathbf{x} = \mathbf{x}\\) and \\(\mathbf{x}^T I = \mathbf{x}^T\\)):</p>
\\[ (I + \mathbf{x}\mathbf{x}^T)^{-1} = I - \mathbf{x}(1 + \mathbf{x}^T \mathbf{x})^{-1}\mathbf{x}^T \\]
<p><b>Step 3: Interpret the central term.</b><br>
The term \\(\mathbf{x}^T \mathbf{x}\\) is the dot product of the vector \\(\mathbf{x}\\) with itself, which is the squared Euclidean norm (or magnitude) of the vector, denoted \\(\|\mathbf{x}\|^2\\). This is a scalar quantity. Therefore, the term \\((1 + \mathbf{x}^T\mathbf{x})^{-1}\\) is the reciprocal of the scalar \\(1 + \|\mathbf{x}\|^2\\).</p>
<p>This leads to the final, elegant formula:</p>
\\[ (I + \mathbf{x}\mathbf{x}^T)^{-1} = I - \frac{\mathbf{x}\mathbf{x}^T}{1 + \|\mathbf{x}\|^2} \\]
<p>This result is powerful because it allows us to compute the inverse without performing any complex matrix inversion algorithm. The right-hand side only requires computing a vector norm, an outer product, a scalar division, and a matrix subtraction, which are all computationally efficient operations.</p>

<b>3. Numerical Example</b>
<p>The transcript illustrates this with a concrete example. The goal is to find the inverse of the matrix:</p>
\\[ M = \begin{pmatrix} 2 & -2 & -1 \\ -2 & 5 & 2 \\ -1 & 2 & 2 \end{pmatrix} \\]
<p><b>Step 1: Decompose the matrix.</b><br>
First, we recognize that this matrix can be written in the form \\(I + \mathbf{x}\mathbf{x}^T\\). We identify the identity matrix \\(I\\) (in this case, \\(3 \times 3\\)) and the vector \\(\mathbf{x}\\):</p>
\\[ \mathbf{x} = \begin{pmatrix} 1 \\ -2 \\ -1 \end{pmatrix} \\]
The outer product \\(\mathbf{x}\mathbf{x}^T\\) is:
\\[ \mathbf{x}\mathbf{x}^T = \begin{pmatrix} 1 \\ -2 \\ -1 \end{pmatrix} \begin{pmatrix} 1 & -2 & -1 \end{pmatrix} = \begin{pmatrix} 1 & -2 & -1 \\ -2 & 4 & 2 \\ -1 & 2 & 1 \end{pmatrix} \\]
Adding the identity matrix \\(I = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}\\) gives the original matrix \\(M\\).</p>

<p><b>Step 2: Apply the derived formula.</b><br>
We use the formula \\((I + \mathbf{x}\mathbf{x}^T)^{-1} = I - \frac{\mathbf{x}\mathbf{x}^T}{1 + \|\mathbf{x}\|^2}\\).</p>
<p>First, calculate the squared norm of \\(\mathbf{x}\\):</p>
\\[ \|\mathbf{x}\|^2 = (1)^2 + (-2)^2 + (-1)^2 = 1 + 4 + 1 = 6 \\]
<p>Next, calculate the denominator:</p>
\\[ 1 + \|\mathbf{x}\|^2 = 1 + 6 = 7 \\]
<p>Now, substitute back into the formula:</p>
\\[ M^{-1} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} - \frac{1}{7} \begin{pmatrix} 1 & -2 & -1 \\ -2 & 4 & 2 \\ -1 & 2 & 1 \end{pmatrix} \\]
<p><b>Step 3: Perform the final calculation.</b><br>
Subtracting the scaled matrix from the identity matrix element-wise:</p>
\\[ M^{-1} = \begin{pmatrix} 1 - \frac{1}{7} & 0 - \frac{-2}{7} & 0 - \frac{-1}{7} \\ 0 - \frac{-2}{7} & 1 - \frac{4}{7} & 0 - \frac{2}{7} \\ 0 - \frac{-1}{7} & 0 - \frac{2}{7} & 1 - \frac{1}{7} \end{pmatrix} = \begin{pmatrix} \frac{6}{7} & \frac{2}{7} & \frac{1}{7} \\ \frac{2}{7} & \frac{3}{7} & -\frac{2}{7} \\ \frac{1}{7} & -\frac{2}{7} & \frac{6}{7} \end{pmatrix} \\]
<p>Factoring out \\(\frac{1}{7}\\) gives the final result presented in the transcript:</p>
\\[ M^{-1} = \frac{1}{7} \begin{pmatrix} 6 & 2 & 1 \\ 2 & 3 & -2 \\ 1 & -2 & 6 \end{pmatrix} \\]
</div></div><div class="chapter" id="noc21_ee33-Lec 67"><h3 class="heading">noc21_ee33-Lec 67</h3><div>
<p>This transcript provides a detailed derivation of the mean of a <b>conditional Gaussian distribution</b>. This is a fundamental concept in statistics, signal processing, and machine learning, used whenever one wants to estimate or infer an unknown random quantity based on related observations. The explanation breaks down the problem, introduces a clever mathematical tool to solve it, and derives a key result.</p>

<h3>1. Problem Definition: The Conditional Gaussian</h3>
<p>The core problem is to understand the statistical properties of a Gaussian random vector, let's call it \\(\mathbf{h}\\), after we have observed another related Gaussian random vector, \\(\mathbf{y}\\). In machine learning terminology:</p>
<ul>
    <li>\\(\mathbf{h}\\) is the unknown <b>parameter vector</b> we want to learn or estimate (e.g., future stock prices, channel coefficients in communications).</li>
    <li>\\(\mathbf{y}\\) is the <b>observation vector</b>, which contains the data we have collected (e.g., past stock data, received signals).</li>
</ul>
<p>The goal is to find the conditional probability density function (PDF) of \\(\mathbf{h}\\) given \\(\mathbf{y}\\), which is denoted as \\(f(\mathbf{h}|\mathbf{y})\\). Since \\(\mathbf{h}\\) and \\(\mathbf{y}\\) are jointly Gaussian, this conditional distribution will also be Gaussian. To fully characterize a Gaussian distribution, we need to find its mean and its covariance matrix.</p>

<h3>2. Statistical Setup and Assumptions</h3>
<p>The derivation is based on the following setup:</p>
<ul>
    <li><b>Random Vectors:</b>
        <ul>
            <li>\\(\mathbf{h}\\) is an \\(M \times 1\\) Gaussian random vector.</li>
            <li>\\(\mathbf{y}\\) is an \\(N \times 1\\) Gaussian random vector.</li>
        </ul>
    </li>
    <li><b>Zero Mean Assumption:</b> For simplicity, both vectors are initially assumed to have a mean of zero.
        \\[ E[\mathbf{h}] = \mathbf{0} \\]
        \\[ E[\mathbf{y}] = \mathbf{0} \\]
        The case for non-zero means is an extension of this result.
    </li>
    <li><b>Jointly Gaussian:</b> This is a critical assumption. It means that any linear combination of the elements of \\(\mathbf{h}\\) and \\(\mathbf{y}\\) will result in a Gaussian random variable. This property is what ensures the conditional distribution is also Gaussian.
    </li>
    <li><b>Covariance Matrices:</b> These matrices describe the variance within each vector and the correlation between them.
        <ul>
            <li><b>Prior Covariance of h:</b> This matrix, \\(\mathbf{R}_h\\), describes the uncertainty in \\(\mathbf{h}\\) <i>before</i> any observations are made. It's often called the "a priori" information.
                \\[ \mathbf{R}_h = E[\mathbf{h}\mathbf{h}^T] \\]
            </li>
            <li><b>Covariance of y:</b>
                \\[ \mathbf{R}_y = E[\mathbf{y}\mathbf{y}^T] \\]
            </li>
            <li><b>Cross-Covariance Matrices:</b> These matrices capture the statistical relationship between \\(\mathbf{h}\\) and \\(\mathbf{y}\\).
                \\[ \mathbf{R}_{hy} = E[\mathbf{h}\mathbf{y}^T] \\]
                \\[ \mathbf{R}_{yh} = E[\mathbf{y}\mathbf{h}^T] = \mathbf{R}_{hy}^T \\]
            </li>
        </ul>
    </li>
</ul>

<h3>3. The Derivation Strategy: Introducing an Auxiliary Variable</h3>
<p>The direct derivation of the conditional PDF can be complex. The transcript uses a common and elegant technique: defining a new auxiliary random vector, \\(\mathbf{z}\\), that is constructed to be statistically independent of the observation vector \\(\mathbf{y}\\). This simplifies the problem significantly.</p>
<p>The auxiliary vector \\(\mathbf{z}\\) is defined as:</p>
\\[ \mathbf{z} = \mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]
<p>This specific construction is chosen because it effectively subtracts the part of \\(\mathbf{h}\\) that is correlated with \\(\mathbf{y}\\).</p>

<h3>4. Key Properties of the Auxiliary Vector \\(\mathbf{z}\\)</h3>
<p>The derivation proceeds by establishing three key properties of \\(\mathbf{z}\\).</p>

<p><b>A. Gaussianity of \\(\mathbf{z}\\)</b><br>
Since \\(\mathbf{z}\\) is a linear combination of two jointly Gaussian vectors (\\(\mathbf{h}\\) and \\(\mathbf{y}\\)), \\(\mathbf{z}\\) is itself a Gaussian random vector.</p>

<p><b>B. Mean of \\(\mathbf{z}\\)</b><br>
The mean of \\(\mathbf{z}\\) is calculated using the linearity of the expectation operator:</p>
\\[ E[\mathbf{z}] = E[\mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y}] \\]
\\[ E[\mathbf{z}] = E[\mathbf{h}] - \mathbf{R}_{hy} \mathbf{R}_y^{-1} E[\mathbf{y}] \\]
<p>Since both \\(E[\mathbf{h}]\\) and \\(E[\mathbf{y}]\\) are zero, the result is:</p>
\\[ E[\mathbf{z}] = \mathbf{0} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{0} = \mathbf{0} \\]
<p>So, \\(\mathbf{z}\\) is also a zero-mean Gaussian random vector.</p>

<p><b>C. Uncorrelation and Independence from \\(\mathbf{y}\\)</b><br>
This is the most crucial step. We check if \\(\mathbf{z}\\) and \\(\mathbf{y}\\) are correlated by calculating their cross-covariance matrix, \\(E[\mathbf{z}\mathbf{y}^T]\\).</p>
\\[ E[\mathbf{z}\mathbf{y}^T] = E\left[ \left( \mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \right) \mathbf{y}^T \right] \\]
<p>Using the linearity of expectation again:</p>
\\[ E[\mathbf{z}\mathbf{y}^T] = E[\mathbf{h}\mathbf{y}^T] - E\left[ \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \mathbf{y}^T \right] \\]
\\[ E[\mathbf{z}\mathbf{y}^T] = E[\mathbf{h}\mathbf{y}^T] - \mathbf{R}_{hy} \mathbf{R}_y^{-1} E[\mathbf{y}\mathbf{y}^T] \\]
<p>Substituting the definitions of \\(\mathbf{R}_{hy}\\) and \\(\mathbf{R}_y\\):</p>
\\[ E[\mathbf{z}\mathbf{y}^T] = \mathbf{R}_{hy} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{R}_y \\]
<p>Since \\(\mathbf{R}_y^{-1} \mathbf{R}_y\\) is the identity matrix \\(\mathbf{I}\\):</p>
\\[ E[\mathbf{z}\mathbf{y}^T] = \mathbf{R}_{hy} - \mathbf{R}_{hy}\mathbf{I} = \mathbf{0} \\]
<p>This result shows that \\(\mathbf{z}\\) and \\(\mathbf{y}\\) are <b>uncorrelated</b>. For jointly Gaussian random vectors, being uncorrelated is equivalent to being <b>statistically independent</b>. This means that observing \\(\mathbf{y}\\) provides no information about \\(\mathbf{z}\\), so \\(f(\mathbf{z}|\mathbf{y}) = f(\mathbf{z})\\).</p>

<h3>5. Deriving the Conditional Mean \\(E[\mathbf{h}|\mathbf{y}]\\)</h3>
<p>With the independence of \\(\mathbf{z}\\) and \\(\mathbf{y}\\) established, we can now find the conditional mean of \\(\mathbf{h}\\).</p>
<p><b>Step 1:</b> Start with the conditional expectation of \\(\mathbf{z}\\) given \\(\mathbf{y}\\). Because they are independent, this is simply the unconditional mean of \\(\mathbf{z}\\):</p>
\\[ E[\mathbf{z}|\mathbf{y}] = E[\mathbf{z}] = \mathbf{0} \\]

<p><b>Step 2:</b> Express this same conditional expectation using the definition of \\(\mathbf{z}\\):</p>
\\[ E[\mathbf{z}|\mathbf{y}] = E[\mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} | \mathbf{y}] \\]
\\[ E[\mathbf{z}|\mathbf{y}] = E[\mathbf{h}|\mathbf{y}] - E[\mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} | \mathbf{y}] \\]

<p><b>Step 3:</b> Simplify the second term. When we are "given \\(\mathbf{y}\\)", the vector \\(\mathbf{y}\\) is treated as a known, constant value. Therefore, the term \\(\mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y}\\) is no longer random; it is a fixed quantity. The expectation of a constant is just the constant itself.</p>
\\[ E[\mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} | \mathbf{y}] = \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]
<p>So the equation becomes:</p>
\\[ E[\mathbf{z}|\mathbf{y}] = E[\mathbf{h}|\mathbf{y}] - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]

<p><b>Step 4:</b> Equate the results from Step 1 and Step 3.</p>
\\[ \mathbf{0} = E[\mathbf{h}|\mathbf{y}] - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]

<p><b>Step 5:</b> Rearrange to find the final result for the conditional mean.</p>
\\[ \mathbf{E[h|y]} = \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]

<h3>Conclusion</h3>
<p>This final formula gives the <b>conditional expectation</b> (or mean) of the parameter vector \\(\mathbf{h}\\) given the observation vector \\(\mathbf{y}\\). It is the first key parameter of the conditional Gaussian PDF \\(f(\mathbf{h}|\mathbf{y})\\). This result is extremely powerful because it provides the best linear estimate of \\(\mathbf{h}\\) based on the observation \\(\mathbf{y}\\). The derivation of the second parameter, the conditional covariance matrix, is mentioned as the topic for a future module.</p>
</div></div><div class="chapter" id="noc21_ee33-Lec 68"><h3 class="heading">noc21_ee33-Lec 68</h3><div>
<p>This document provides a detailed explanation of the concepts and formulas discussed in the transcript, focusing on the <b>Conditional Gaussian Probability Density Function (PDF)</b>. This is a fundamental concept in statistics and linear algebra, particularly relevant in estimation theory.</p>

<h3>1. Introduction to the Conditional Gaussian PDF</h3>
<p>The core topic is understanding the probability distribution of a random vector \\(\mathbf{h}\\) when we have observed another related random vector \\(\mathbf{y}\\). The key assumption is that \\(\mathbf{h}\\) and \\(\mathbf{y}\\) are <b>jointly Gaussian</b> random vectors.</p>
<p>In a typical estimation problem:</p>
<ul>
    <li>\\(\mathbf{y}\\) represents a set of observations or measurements.</li>
    <li>\\(\mathbf{h}\\) represents an unknown quantity or set of parameters that we want to estimate based on the observations \\(\mathbf{y}\\).</li>
</ul>
<p>The conditional PDF, denoted as \\(p(\mathbf{h}|\mathbf{y})\\), describes the probability of \\(\mathbf{h}\\) taking on a certain value given that we know the value of \\(\mathbf{y}\\). A key result is that if \\(\mathbf{h}\\) and \\(\mathbf{y}\\) are jointly Gaussian, then the conditional distribution of \\(\mathbf{h}\\) given \\(\mathbf{y}\\) is also Gaussian.</p>
<p>A Gaussian distribution is completely defined by its mean and its covariance matrix. The transcript details the derivation of these two parameters for the conditional case.</p>

<h3>2. The Conditional Mean (Recap)</h3>
<p>The transcript first reminds us of the formula for the conditional mean, which is the expected value of \\(\mathbf{h}\\) given \\(\mathbf{y}\\). This value is often used as the best estimate for \\(\mathbf{h}\\).</p>
<p>The conditional mean is given by:</p>
\\[ E[\mathbf{h} | \mathbf{y}] = \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]
<p>Where:</p>
<ul>
    <li>\\(\mathbf{R}_{hy}\\) is the cross-covariance matrix between \\(\mathbf{h}\\) and \\(\mathbf{y}\\), defined as \\(E[\mathbf{h}\mathbf{y}^T]\\).</li>
    <li>\\(\mathbf{R}_y\\) is the auto-covariance matrix of \\(\mathbf{y}\\), defined as \\(E[\mathbf{y}\mathbf{y}^T]\\). We assume it is invertible.</li>
</ul>

<h3>3. Derivation of the Conditional Covariance Matrix</h3>
<p>The main focus of the transcript is to derive the second parameter needed to define the conditional Gaussian PDF: the <b>conditional covariance matrix</b>, denoted \\(\text{cov}(\mathbf{h}|\mathbf{y})\\).</p>

<h4>Step 1: Define an Auxiliary Vector \\(\mathbf{z}\\)</h4>
<p>To simplify the derivation, an auxiliary random vector \\(\mathbf{z}\\) is constructed. This vector represents the part of \\(\mathbf{h}\\) that is "unrelated" or orthogonal to \\(\mathbf{y}\\).</p>
\\[ \mathbf{z} = \mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]

<h4>Step 2: Establish Independence of \\(\mathbf{z}\\) and \\(\mathbf{y}\\)</h4>
<p>A crucial property, established previously, is that \\(\mathbf{z}\\) and \\(\mathbf{y}\\) are uncorrelated. This is shown by computing their cross-correlation:</p>
\\[ E[\mathbf{z} \mathbf{y}^T] = \mathbf{0} \\]
<p>Since \\(\mathbf{h}\\) and \\(\mathbf{y}\\) are jointly Gaussian, and \\(\mathbf{z}\\) is a linear combination of them, \\(\mathbf{z}\\) and \\(\mathbf{y}\\) are also jointly Gaussian. For Gaussian random vectors, being uncorrelated implies that they are also <b>independent</b>.</p>

<h4>Step 3: Simplify the Covariance Calculation</h4>
<p>The independence of \\(\mathbf{z}\\) and \\(\mathbf{y}\\) is a powerful result. It means that knowing the value of \\(\mathbf{y}\\) gives no new information about \\(\mathbf{z}\\). Therefore, the conditional covariance of \\(\mathbf{z}\\) given \\(\mathbf{y}\\) is the same as its unconditional covariance:</p>
\\[ \text{cov}(\mathbf{z} | \mathbf{y}) = \text{cov}(\mathbf{z}) \\]
<p>Since \\(\mathbf{z}\\) is a zero-mean random vector, its covariance is simply \\(E[\mathbf{z}\mathbf{z}^T]\\).</p>

<h4>Step 4: Expand and Calculate \\(\text{cov}(\mathbf{z})\\)</h4>
<p>The covariance of \\(\mathbf{z}\\) is calculated by expanding the expression \\(E[\mathbf{z}\mathbf{z}^T]\\):</p>
\\[ \text{cov}(\mathbf{z}) = E[\mathbf{z}\mathbf{z}^T] = E\left[ (\mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y})(\mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y})^T \right] \\]
<p>Using the transpose property \\((AB)^T = B^T A^T\\), the second term becomes:</p>
\\[ (\mathbf{h} - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y})^T = \mathbf{h}^T - (\mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y})^T = \mathbf{h}^T - \mathbf{y}^T (\mathbf{R}_y^{-1})^T (\mathbf{R}_{hy})^T \\]
<p>We use the facts that \\(\mathbf{R}_y\\) is symmetric (so \\(\mathbf{R}_y^{-1}\\) is also symmetric) and that \\((\mathbf{R}_{hy})^T = \mathbf{R}_{yh}\\). The expression becomes:</p>
\\[ \mathbf{h}^T - \mathbf{y}^T \mathbf{R}_y^{-1} \mathbf{R}_{yh} \\]
<p>Expanding the full product gives four terms. After taking the expectation of each term:</p>
\\[ E[\mathbf{z}\mathbf{z}^T] = E[\mathbf{h}\mathbf{h}^T] - E[\mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{y}\mathbf{h}^T] - E[\mathbf{h}\mathbf{y}^T\mathbf{R}_y^{-1}\mathbf{R}_{yh}] + E[\mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{y}\mathbf{y}^T\mathbf{R}_y^{-1}\mathbf{R}_{yh}] \\]
<p>Substituting the definitions of the covariance matrices (e.g., \\(E[\mathbf{h}\mathbf{h}^T] = \mathbf{R}_h\\), \\(E[\mathbf{y}\mathbf{h}^T] = \mathbf{R}_{yh}\\), etc.):</p>
\\[ = \mathbf{R}_h - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} + \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_y\mathbf{R}_y^{-1}\mathbf{R}_{yh} \\]
<p>Since \\(\mathbf{R}_y\mathbf{R}_y^{-1} = \mathbf{I}\\) (the identity matrix), the expression simplifies to:</p>
\\[ \text{cov}(\mathbf{z}) = \mathbf{R}_h - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} + \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} \\]
\\[ \text{cov}(\mathbf{z}) = \mathbf{R}_h - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} \\]

<h4>Step 5: Relate Back to \\(\text{cov}(\mathbf{h} | \mathbf{y})\\)</h4>
<p>The final step is to show that the conditional covariance of \\(\mathbf{h}\\) is the same as that of \\(\mathbf{z}\\). From the definition of \\(\mathbf{z}\\), we can write \\(\mathbf{h} = \mathbf{z} + \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{y}\\).</p>
\\[ \text{cov}(\mathbf{h} | \mathbf{y}) = \text{cov}(\mathbf{z} + \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} | \mathbf{y}) \\]
<p>When we condition on \\(\mathbf{y}\\), the term \\(\mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y}\\) is no longer random; it is a known, constant vector. Adding a constant to a random vector does not change its covariance. Therefore:</p>
\\[ \text{cov}(\mathbf{h} | \mathbf{y}) = \text{cov}(\mathbf{z} | \mathbf{y}) \\]
<p>Combining this with the result from Step 3 and Step 4, we get the final formula:</p>
\\[ \text{cov}(\mathbf{h} | \mathbf{y}) = \mathbf{R}_h - \mathbf{R}_{hy}\mathbf{R}_y^{-1}\mathbf{R}_{yh} \\]

<h3>4. The Complete Conditional Gaussian PDF</h3>
<p>With both the conditional mean and conditional covariance derived, we can fully specify the conditional PDF of \\(\mathbf{h}\\) given \\(\mathbf{y}\\). It is a Gaussian distribution with:</p>
<ul>
    <li><b>Conditional Mean:</b> \\[ \boldsymbol{\mu}_{\mathbf{h}|\mathbf{y}} = \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y} \\]</li>
    <li><b>Conditional Covariance:</b> \\[ \boldsymbol{\Sigma}_{\mathbf{h}|\mathbf{y}} = \mathbf{R}_h - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{R}_{yh} \\]</li>
</ul>

<h3>5. Connection to MMSE and LMMSE Estimation</h3>
<p>The transcript highlights a deep connection between these results and estimation theory.</p>
<ul>
    <li><b>MMSE (Minimum Mean Squared Error) Estimate:</b> The estimate \\(\hat{\mathbf{h}}\\) that minimizes the mean squared error \\(E[\|\mathbf{h} - \hat{\mathbf{h}}\|^2]\\) is the conditional mean, \\(\hat{\mathbf{h}}_{MMSE} = E[\mathbf{h}|\mathbf{y}]\\). This is generally the best possible estimate but can be hard to compute for non-Gaussian distributions.</li>
    <li><b>LMMSE (Linear Minimum Mean Squared Error) Estimate:</b> This is the best estimate that is restricted to be a linear function of the observations \\(\mathbf{y}\\). Its formula is \\(\hat{\mathbf{h}}_{LMMSE} = \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{y}\\).</li>
</ul>
<p>For jointly Gaussian random vectors, a remarkable property emerges: the conditional mean is a linear function of \\(\mathbf{y}\\). This means:</p>
<p><b>For Gaussian distributions: MMSE Estimate = LMMSE Estimate</b></p>
<p>This is a unique and powerful property of the Gaussian distribution. It implies that the best linear estimator is, in fact, the best possible estimator overall (in the mean squared error sense). For most other distributions, the MMSE estimate is non-linear, and the LMMSE estimate is suboptimal.</p>

<h3>6. The Case of Non-Zero Means</h3>
<p>The derivation above assumes \\(\mathbf{h}\\) and \\(\mathbf{y}\\) are zero-mean. The results can be easily extended to the case where they have non-zero means, \\(\boldsymbol{\mu}_h = E[\mathbf{h}]\\) and \\(\boldsymbol{\mu}_y = E[\mathbf{y}]\\).</p>
<p>This is done by working with the centralized (zero-mean) vectors:</p>
<ul>
    <li>\\( \tilde{\mathbf{h}} = \mathbf{h} - \boldsymbol{\mu}_h \\)</li>
    <li>\\( \tilde{\mathbf{y}} = \mathbf{y} - \boldsymbol{\mu}_y \\)</li>
</ul>
<p>The covariance matrices \\(\mathbf{R}_h, \mathbf{R}_y, \mathbf{R}_{hy}\\) are defined based on these centralized vectors. Applying the zero-mean formulas to \\(\tilde{\mathbf{h}}\\) and \\(\tilde{\mathbf{y}}\\) and then transforming back gives the general result:</p>
<ul>
    <li><b>Conditional Mean (Non-Zero Mean Case):</b>
        \\[ E[\mathbf{h} | \mathbf{y}] = \boldsymbol{\mu}_h + \mathbf{R}_{hy} \mathbf{R}_y^{-1} (\mathbf{y} - \boldsymbol{\mu}_y) \\]
        This formula has an intuitive interpretation: start with the prior mean of \\(\mathbf{h}\\) (\\(\boldsymbol{\mu}_h\\)) and add a correction term that is linear in the new information from the observation, \\(\mathbf{y} - \boldsymbol{\mu}_y\\).
    </li>
    <li><b>Conditional Covariance (Non-Zero Mean Case):</b>
        \\[ \text{cov}(\mathbf{h} | \mathbf{y}) = \mathbf{R}_h - \mathbf{R}_{hy} \mathbf{R}_y^{-1} \mathbf{R}_{yh} \\]
        The conditional covariance is unaffected by the means and remains the same. This reflects that observing \\(\mathbf{y}\\) reduces the uncertainty about \\(\mathbf{h}\\), and this reduction in uncertainty (variance) does not depend on the mean values.
    </li>
</ul>
<p>These formulas are central to many algorithms in signal processing, machine learning, and communications, such as the Kalman filter.</p>
</div></div><div class="chapter" id="noc21_ee33-Lec 69"><h3 class="heading">noc21_ee33-Lec 69</h3><div>
<p>This document provides a detailed explanation of the concepts presented in the transcript, focusing on the application of conditional Gaussian inference for learning a parameter in a linear model.</p>

<h3>1. The Problem: Learning a Parameter from a Linear Model</h3>
<p>The core task is to estimate or "learn" an unknown parameter, denoted by \\(h\\), from a set of observations. The relationship between the parameter, the inputs, and the observations is described by a linear model. This is a fundamental problem in many fields, including machine learning, signal processing, and statistics.</p>

<p>The model is formulated as:</p>
\\[ \mathbf{y} = \mathbf{x}h + \mathbf{v} \\]

<p>Let's break down the components of this model:</p>
<ul>
    <li><b>Observations (\\(\mathbf{y}\\)):</b> This is a vector of \\(n\\) observed outputs, often called the training data.
    \\[ \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \\]
    This is an \\(n \times 1\\) column vector.</li>

    <li><b>Inputs (\\(\mathbf{x}\\)):</b> This is a vector of \\(n\\) known inputs, also part of the training data.
    \\[ \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \\]
    This is also an \\(n \times 1\\) column vector.</li>

    <li><b>Parameter (\\(h\\)):</b> This is the unknown scalar (a \\(1 \times 1\\) quantity) that we want to learn or estimate. The model assumes a linear relationship between the inputs and outputs, scaled by this parameter.</li>

    <li><b>Noise (\\(\mathbf{v}\\)):</b> This is a vector of \\(n\\) unobserved random noise values that corrupt the linear relationship.
    \\[ \mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} \\]
    This is an \\(n \times 1\\) column vector.</li>
</ul>

<h3>2. Statistical Assumptions (The Gaussian Framework)</h3>
<p>To infer \\(h\\) from \\(\mathbf{y}\\), we make specific statistical assumptions about the random components of the model. The entire framework is built on the properties of Gaussian (or normal) distributions.</p>

<ul>
    <li><b>The Parameter \\(h\\) is Gaussian:</b> Our prior belief about the parameter \\(h\\) is that it is a Gaussian random variable with a known mean and variance.
        <ul>
            <li>Mean: \\( E[h] = \mu_h \\)</li>
            <li>Variance: \\( \text{Var}(h) = E[(h-\mu_h)^2] = \sigma_h^2 \\)</li>
        </ul>
    </li>
    <li><b>The Noise \\(\mathbf{v}\\) is Gaussian:</b> The noise vector \\(\mathbf{v}\\) is assumed to consist of <b>independent and identically distributed (i.i.d.)</b> Gaussian random variables.
        <ul>
            <li><b>Zero Mean:</b> Each noise sample \\(v_i\\) has a mean of zero, so \\(E[\mathbf{v}] = \mathbf{0}\\).</li>
            <li><b>i.i.d.:</b> The noise samples are statistically independent of each other and share the same variance, \\(\sigma^2\\).</li>
        </ul>
        This i.i.d. assumption leads to a simple structure for the <b>noise covariance matrix</b>:
        \\[ E[\mathbf{v}\mathbf{v}^T] = \sigma^2 \mathbf{I} \\]
        where \\(\mathbf{I}\\) is the \\(n \times n\\) identity matrix. The off-diagonal elements are zero because different noise samples are uncorrelated (due to independence), and the diagonal elements are all \\(\sigma^2\\), the variance of each noise sample.
    </li>
    <li><b>Uncorrelated Parameter and Noise:</b> The parameter \\(h\\) and the noise vector \\(\mathbf{v}\\) are assumed to be uncorrelated. This is a standard assumption, meaning the noise process does not depend on the parameter we are trying to estimate. Mathematically, \\(E[(h-\mu_h)\mathbf{v}^T] = \mathbf{0}\\).</li>
</ul>

<h3>3. The Estimation Strategy: Minimum Mean Squared Error (MMSE)</h3>
<p>The goal is to find the "best" estimate of \\(h\\) given the observations \\(\mathbf{y}\\). In this context, "best" is defined in the sense of <b>Minimum Mean Squared Error (MMSE)</b>. The MMSE estimate, denoted \\(\hat{h}\\), is the one that minimizes the expected squared difference between the true parameter and its estimate: \\(E[(h-\hat{h})^2]\\).</p>

<p>A key result from statistical theory is that for Gaussian random variables, the MMSE estimate is the <b>conditional mean</b>.</p>
\\[ \hat{h}_{MMSE} = E[h | \mathbf{y}] \\]

<p>Because our model \\(\mathbf{y} = \mathbf{x}h + \mathbf{v}\\) is a linear combination of Gaussian variables (\\(h\\) and \\(\mathbf{v}\\)), the observation vector \\(\mathbf{y}\\) is also Gaussian. Therefore, we can use the formula for the conditional mean of jointly Gaussian variables to find the estimate. For non-zero mean variables, this formula is:</p>
\\[ \hat{h} = E[h|\mathbf{y}] = \mathbf{R}_{hy} \mathbf{R}_{y}^{-1} (\mathbf{y} - \boldsymbol{\mu}_y) + \mu_h \\]

<p>To use this formula, we must first derive the following three quantities:</p>
<ol>
    <li>\\(\boldsymbol{\mu}_y\\): The mean vector of the observations \\(\mathbf{y}\\).</li>
    <li>\\(\mathbf{R}_y\\): The covariance matrix of \\(\mathbf{y}\\).</li>
    <li>\\(\mathbf{R}_{hy}\\): The cross-covariance between \\(h\\) and \\(\mathbf{y}\\).</li>
</ol>

<h3>4. Deriving the Necessary Quantities</h3>

<h4>A. Mean of the Observation Vector (\\(\boldsymbol{\mu}_y\\))</h4>
<p>We find the mean by taking the expectation of the linear model.</p>
\\[ \boldsymbol{\mu}_y = E[\mathbf{y}] = E[\mathbf{x}h + \mathbf{v}] \\]
<p>Using the linearity of expectation:</p>
\\[ \boldsymbol{\mu}_y = \mathbf{x}E[h] + E[\mathbf{v}] \\]
<p>Substituting the known means (\\(E[h] = \mu_h\\) and \\(E[\mathbf{v}] = \mathbf{0}\\)):</p>
\\[ \boldsymbol{\mu}_y = \mathbf{x}\mu_h \\]

<h4>B. Covariance Matrix of the Observation Vector (\\(\mathbf{R}_y\\))</h4>
<p>The covariance matrix is defined as \\(\mathbf{R}_y = E[(\mathbf{y} - \boldsymbol{\mu}_y)(\mathbf{y} - \boldsymbol{\mu}_y)^T]\\). We first find the term \\(\mathbf{y} - \boldsymbol{\mu}_y\\):</p>
\\[ \mathbf{y} - \boldsymbol{\mu}_y = (\mathbf{x}h + \mathbf{v}) - \mathbf{x}\mu_h = \mathbf{x}(h - \mu_h) + \mathbf{v} \\]
<p>Now, we substitute this into the covariance definition and expand:</p>
\\[ \mathbf{R}_y = E[ (\mathbf{x}(h - \mu_h) + \mathbf{v}) (\mathbf{x}(h - \mu_h) + \mathbf{v})^T ] \\]
\\[ \mathbf{R}_y = E[ \mathbf{x}(h - \mu_h)(h - \mu_h)\mathbf{x}^T + \mathbf{x}(h - \mu_h)\mathbf{v}^T + \mathbf{v}(h - \mu_h)\mathbf{x}^T + \mathbf{v}\mathbf{v}^T ] \\]
<p>Applying expectation to each term and using the fact that \\(h\\) and \\(\mathbf{v}\\) are uncorrelated (making the two middle cross-terms zero), we get:</p>
\\[ \mathbf{R}_y = \mathbf{x} E[(h-\mu_h)^2] \mathbf{x}^T + E[\mathbf{v}\mathbf{v}^T] \\]
<p>Substituting the known variance of \\(h\\) (\\(\sigma_h^2\\)) and the covariance of \\(\mathbf{v}\\) (\\(\sigma^2 \mathbf{I}\\)), we arrive at the result:</p>
\\[ \mathbf{R}_y = \sigma_h^2 \mathbf{x}\mathbf{x}^T + \sigma^2 \mathbf{I} \\]

<h4>C. Cross-Covariance of \\(h\\) and \\(\mathbf{y}\\) (\\(\mathbf{R}_{hy}\\))</h4>
<p>The cross-covariance is defined as \\(\mathbf{R}_{hy} = E[(h - \mu_h)(\mathbf{y} - \boldsymbol{\mu}_y)^T]\\).</p>
\\[ \mathbf{R}_{hy} = E[(h - \mu_h) (\mathbf{x}(h - \mu_h) + \mathbf{v})^T] \\]
\\[ \mathbf{R}_{hy} = E[(h - \mu_h) ((h - \mu_h)\mathbf{x}^T + \mathbf{v}^T)] \\]
\\[ \mathbf{R}_{hy} = E[(h - \mu_h)^2 \mathbf{x}^T + (h - \mu_h)\mathbf{v}^T] \\]
<p>Applying expectation and using the uncorrelation of \\(h\\) and \\(\mathbf{v}\\):</p>
\\[ \mathbf{R}_{hy} = E[(h - \mu_h)^2] \mathbf{x}^T + E[(h - \mu_h)\mathbf{v}^T] \\]
<p>The second term is zero, and the first term is the variance of \\(h\\). So, we get:</p>
\\[ \mathbf{R}_{hy} = \sigma_h^2 \mathbf{x}^T \\]

<h3>5. The Final MMSE Estimate Formula</h3>
<p>Now we have all the components to assemble the final formula for the MMSE estimate \\(\hat{h}\\). We substitute the expressions for \\(\mathbf{R}_{hy}\\), \\(\mathbf{R}_y\\), and \\(\boldsymbol{\mu}_y\\) into the general conditional mean equation.</p>
<p>The estimate \\(\hat{h}\\) is given by:</p>
\\[ \hat{h} = \underbrace{(\sigma_h^2 \mathbf{x}^T)}_{\mathbf{R}_{hy}} \underbrace{(\sigma_h^2 \mathbf{x}\mathbf{x}^T + \sigma^2 \mathbf{I})^{-1}}_{\mathbf{R}_y^{-1}} \underbrace{(\mathbf{y} - \mathbf{x}\mu_h)}_{(\mathbf{y} - \boldsymbol{\mu}_y)} + \mu_h \\]

<p>This is the central result. It provides a concrete algorithm to compute the best estimate of the parameter \\(h\\) from the observed data \\(\mathbf{y}\\). It optimally combines the prior information about \\(h\\) (captured by \\(\mu_h\\) and \\(\sigma_h^2\\)) with the information contained in the new observations (captured by \\(\mathbf{y}\\) and \\(\mathbf{x}\\)).</p>

<p>The transcript notes that while this formula involves inverting an \\(n \times n\\) matrix, the specific structure of this matrix (a rank-one matrix \\(\mathbf{x}\mathbf{x}^T\\) plus a scaled identity matrix) allows for a much more efficient computation using tools like the matrix inversion lemma, a topic for further discussion.</p>
</div></div><h2>Weekly Summary</h2><div><h3>1. Weighted Least Squares (WLS)</h3>
<p>This section introduces Weighted Least Squares (WLS) as a generalization of the standard Least Squares (LS) problem. Instead of minimizing the simple sum of squared errors, WLS introduces a weighting matrix to give different levels of importance to different errors.</p>
<b>Key Takeaways:</b>
<ul>
<li>The standard LS cost function is \\(||\mathbf{y} - A\mathbf{x}||^2\\). The WLS cost function modifies this to \\((\mathbf{y} - A\mathbf{x})^T W (\mathbf{y} - A\mathbf{x})\\), where \\(W\\) is a <b>weighting matrix</b>.</li>
<li>To ensure the cost is always non-negative, the weighting matrix \\(W\\) must be <b>positive semi-definite (PSD)</b>.</li>
<li>Any PSD matrix \\(W\\) can be decomposed using Cholesky decomposition as \\(W = W^{1/2 T} W^{1/2}\\), allowing the cost function to be viewed as a standard LS problem on weighted data: \\(||W^{1/2}(\mathbf{y} - A\mathbf{x})||^2\\).</li>
<li>The solution to the WLS problem is given by:
\\[ \hat{\mathbf{x}}_{WLS} = (A^T W A)^{-1} A^T W \mathbf{y} \\]
</li>
<li>Standard LS is a special case of WLS where the weighting matrix \\(W\\) is the identity matrix, meaning all errors are weighted equally.</li>
</ul>

<h3>2. Optimal Weighting in WLS</h3>
<p>This section provides a practical example of WLS, demonstrating its application in scenarios with non-uniform noise. It shows that the optimal choice for the weighting matrix is related to the noise characteristics.</p>
<b>Key Takeaways:</b>
<ul>
<li>WLS is particularly useful when observations are corrupted by noise that is independent but <b>not identically distributed</b> (i.e., different noise samples have different variances).</li>
<li>The optimal weighting matrix \\(W\\) is the inverse of the noise covariance matrix, \\(W = R^{-1}\\). For uncorrelated noise with variances \\(\sigma_i^2\\), \\(R\\) is a diagonal matrix with entries \\(\sigma_i^2\\), so \\(W\\) is a diagonal matrix with entries \\(1/\sigma_i^2\\).</li>
<li>This choice is intuitive: observations with <b>higher noise variance</b> (less reliable) are given a <b>lower weight</b>, while observations with <b>lower noise variance</b> (more reliable) are given a <b>higher weight</b>.</li>
<li>If the noise is independent and identically distributed (i.i.d.), the noise covariance matrix is \\(R = \sigma^2 I\\), making \\(W \propto I\\). In this case, WLS reduces to the standard LS problem.</li>
</ul>

<h3>3. The Woodbury Matrix Identity</h3>
<p>This section introduces a powerful matrix algebra tool known as the Woodbury Matrix Identity or the Matrix Inversion Lemma. It provides an alternative way to compute the inverse of a matrix that is expressed as a sum.</p>
<b>Key Takeaways:</b>
<ul>
<li>The Woodbury Matrix Identity is stated as:
\\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} \\]
</li>
<li>While it looks more complex, this identity is extremely useful when \\(A\\) is large and easily invertible (e.g., diagonal or identity) and the matrix \\(UCV\\) has a <b>low rank</b>.</li>
<li>The key advantage is that it replaces the inversion of a large matrix (\\(A+UCV\\)) with the inversion of a much smaller matrix (\\(C^{-1} + VA^{-1}U\\)), significantly reducing computational complexity.</li>
<li>A common application is when \\(U\\) is a tall matrix and \\(V\\) is a flat matrix, making their product \\(UCV\\) a low-rank update to \\(A\\).</li>
</ul>

<h3>4. Application of the Woodbury Identity</h3>
<p>This section illustrates the utility of the Woodbury Identity with a specific, common example: inverting a rank-one update to the identity matrix.</p>
<b>Key Takeaways:</b>
<ul>
<li>The problem is to compute the inverse of \\((I + \mathbf{x}\mathbf{x}^T)^{-1}\\), where \\(I\\) is the identity matrix and \\(\mathbf{x}\mathbf{x}^T\\) is a rank-one matrix.</li>
<li>By applying the Woodbury Identity with \\(A=I\\), \\(U=\mathbf{x}\\), \\(C=1\\), and \\(V=\mathbf{x}^T\\), the inverse can be efficiently calculated without performing a full matrix inversion.</li>
<li>The simplified result is:
\\[ (I + \mathbf{x}\mathbf{x}^T)^{-1} = I - \frac{\mathbf{x}\mathbf{x}^T}{1 + ||\mathbf{x}||^2} \\]
</li>
<li>This formula replaces a computationally expensive matrix inversion with simple vector and scalar operations.</li>
</ul>

<h3>5. The Conditional Gaussian Distribution</h3>
<p>This section introduces the concept of the conditional Gaussian probability density function (PDF). It addresses the question of how the distribution of one Gaussian random vector changes when we observe another, correlated Gaussian random vector.</p>
<b>Key Takeaways:</b>
<ul>
<li>If two random vectors, \\(\mathbf{h}\\) and \\(\mathbf{y}\\), are <b>jointly Gaussian</b>, then the conditional distribution of \\(\mathbf{h}\\) given an observation of \\(\mathbf{y}\\) is also Gaussian.</li>
<li>The main goal is to find the parameters (mean and covariance) of this new conditional Gaussian distribution, \\(p(\mathbf{h}|\mathbf{y})\\).</li>
<li>This concept is fundamental in estimation and machine learning, where \\(\mathbf{h}\\) might be an unknown parameter (like stock prices or a channel state) and \\(\mathbf{y}\\) is a set of related observations or data.</li>
<li>For zero-mean, jointly Gaussian vectors, the <b>conditional mean</b> (or conditional expectation) is derived as:
\\[ E[\mathbf{h}|\mathbf{y}] = R_{hy}R_y^{-1}\mathbf{y} \\]
where \\(R_{hy}\\) is the cross-covariance matrix and \\(R_y\\) is the covariance matrix of \\(\mathbf{y}\\).</li>
<li>The <b>conditional covariance</b> is derived as:
\\[ Cov(\mathbf{h}|\mathbf{y}) = R_h - R_{hy}R_y^{-1}R_{yh} \\]
</li>
</ul>

<h3>6. Estimation and the Linear Model</h3>
<p>This section connects the conditional Gaussian results to the concept of estimation and applies them to a standard linear model. It establishes the relationship between the conditional mean and the Minimum Mean Squared Error (MMSE) estimate.</p>
<b>Key Takeaways:</b>
<ul>
<li>The conditional mean, \\(E[\mathbf{h}|\mathbf{y}]\\), is the <b>Minimum Mean Squared Error (MMSE)</b> estimate of \\(\mathbf{h}\\) given \\(\mathbf{y}\\). It is the best possible estimate in terms of minimizing the average squared error.</li>
<li>A special property of Gaussian distributions is that the MMSE estimate is a linear function of the observations \\(\mathbf{y}\\). This means that for Gaussians, the best overall estimate (MMSE) is the same as the best linear estimate (LMMSE). This is not true in general for other distributions.</li>
<li>The framework is applied to the linear model \\(\mathbf{y} = \mathbf{x}h + \mathbf{v}\\), where `h` is an unknown Gaussian parameter and \\(\mathbf{v}\\) is Gaussian noise. The goal is to estimate `h` from the observation \\(\mathbf{y}\\).</li>
<li>By calculating the required covariance matrices (\\(R_y\\) and \\(R_{hy}\\)) for this specific model, the MMSE estimate of `h` can be found using the conditional mean formula. This provides a principled way to learn or infer a parameter from noisy, linear observations.</li>
</ul>
</div><h2>Assignment Explanation</h2><div>
<h3>Assignment Questions and Answers Explained</h3>

<p>Here is a detailed explanation of each question in the assignment.</p>

<hr>

<h4>Question 1</h4>
<p><b>Question:</b> Consider a discrete time stochastic process with \\(X_n\\) denoting the state at time \\(n\\). The Markov property states that</p>
<p><b>Options:</b></p>
<ul>
  <li>\\(Pr(X_n | X_{n-1}, X_{n-2}, \dots, X_1) = Pr(X_n | X_1)\\)</li>
  <li>\\(Pr(X_n | X_{n-1}, X_{n-2}, \dots, X_1) = Pr(X_{n-1}, X_{n-2}, \dots, X_1 | X_n)\\)</li>
  <li>\\(Pr(X_n | X_{n-1}, X_{n-2}, \dots, X_1) = Pr(X_n)\\)</li>
  <li>\\(Pr(X_n | X_{n-1}, X_{n-2}, \dots, X_1) = Pr(X_n | X_{n-1})\\)</li>
</ul>
<p><b>Explanation:</b></p>
<p>The Markov property, also known as the "memoryless" property, is the fundamental characteristic of a Markov chain. It states that the future state of the process depends only on the present state, and not on the sequence of states that preceded it. Mathematically, the probability of being in state \\(X_n\\) at time \\(n\\), given the entire history of the process up to time \\(n-1\\), is the same as the probability given only the state at time \\(n-1\\).</p>
<p>Therefore, the correct mathematical expression is:<br><b>\\(Pr(X_n | X_{n-1}, X_{n-2}, \dots, X_1) = Pr(X_n | X_{n-1})\\)</b></p>
<p>This matches the last option.</p>

<hr>

<h4>Question 2</h4>
<p><b>Question:</b> Consider a Discrete Time Markov Chain (DTMC) used to model a machine, where the state \\(W_n\\) on day \\(n\\) can belong to one of two possible states \\(S = \{s_1, s_2\}\\), with \\(s_1\\) denoting functional and \\(s_2\\) denoting non-functional. Given the machine is functional on a particular day, the next day it can be non-functional with a probability 0.20, while when it is non-functional on a particular day, it can also be non-functional on the next day with probability 0.35. The one step transition probablity matrix for the DTMC is given as</p>
<p><b>Explanation:</b></p>
<p>Let's construct the transition probability matrix \\(P\\). The entry \\(P_{ij}\\) represents the probability of transitioning from state \\(j\\) to state \\(i\\). This is a column-stochastic convention where each column sums to 1.</p>
<ul>
    <li><b>States:</b> \\(s_1\\) = functional, \\(s_2\\) = non-functional.</li>
    <li><b>From state \\(s_1\\) (functional):</b>
        <ul>
            <li>The probability of transitioning to \\(s_2\\) (non-functional) is given as 0.20. In the matrix, this is the probability of going from column 1 to row 2, so \\(P_{21} = 0.20\\).</li>
            <li>Since the column must sum to 1, the probability of remaining in state \\(s_1\\) (functional) is \\(P_{11} = 1 - P_{21} = 1 - 0.20 = 0.80\\).</li>
        </ul>
    </li>
    <li><b>From state \\(s_2\\) (non-functional):</b>
        <ul>
            <li>The probability of transitioning to \\(s_2\\) (staying non-functional) is given as 0.35. This is the probability of going from column 2 to row 2, so \\(P_{22} = 0.35\\).</li>
            <li>Since the column must sum to 1, the probability of transitioning to state \\(s_1\\) (functional) is \\(P_{12} = 1 - P_{22} = 1 - 0.35 = 0.65\\).</li>
        </ul>
    </li>
</ul>
<p>The resulting transition matrix is:</p>
\\[ P = \begin{bmatrix} P_{11} & P_{12} \\ P_{21} & P_{22} \end{bmatrix} = \begin{bmatrix} 0.80 & 0.65 \\ 0.20 & 0.35 \end{bmatrix} \\]
<p>The correct option is the one that matches this matrix.</p>

<hr>

<h4>Question 3</h4>
<p><b>Question:</b> Consider the one step transition probability matrix \\(P\\) for a DTMC. The stationary probability distribution of the DTMC is given as</p>
<p><b>Explanation:</b></p>
<p>The stationary distribution \\(\pi\\) of a Markov chain is a probability distribution that does not change as the process evolves. If we start with the stationary distribution, the distribution at any subsequent time will remain the same.</p>
<p>For a regular (ergodic) Markov chain, as the number of steps \\(n\\) approaches infinity, the n-step transition matrix \\(P^n\\) converges to a matrix where every row is identical. Each of these identical rows is the unique stationary probability distribution \\(\pi\\). </p>
<p>Therefore, the stationary distribution is given by <b>Each row of the matrix \\(\lim_{n\to\infty} P^n\\)</b>.</p>

<hr>

<h4>Question 4</h4>
<p><b>Question:</b> Consider a DTMC with the one-step transition probability matrix
\\[ P = \begin{bmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{2}{3} & \frac{1}{3} \end{bmatrix} \\]
The two-step transition probability matrix is given as</p>
<p><b>Explanation:</b></p>
<p>The n-step transition matrix is found by raising the one-step transition matrix \\(P\\) to the power of \\(n\\). For the two-step transition matrix, we need to calculate \\(P^2\\).</p>
\\[ P^2 = P \times P = \begin{bmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{2}{3} & \frac{1}{3} \end{bmatrix} \begin{bmatrix} \frac{1}{3} & \frac{2}{3} \\ \frac{2}{3} & \frac{1}{3} \end{bmatrix} \\]
<p>Performing the matrix multiplication:</p>
\\[ P^2 = \begin{bmatrix} (\frac{1}{3})(\frac{1}{3}) + (\frac{2}{3})(\frac{2}{3}) & (\frac{1}{3})(\frac{2}{3}) + (\frac{2}{3})(\frac{1}{3}) \\ (\frac{2}{3})(\frac{1}{3}) + (\frac{1}{3})(\frac{2}{3}) & (\frac{2}{3})(\frac{2}{3}) + (\frac{1}{3})(\frac{1}{3}) \end{bmatrix} \\]
\\[ P^2 = \begin{bmatrix} \frac{1}{9} + \frac{4}{9} & \frac{2}{9} + \frac{2}{9} \\ \frac{2}{9} + \frac{2}{9} & \frac{4}{9} + \frac{1}{9} \end{bmatrix} \\]
\\[ P^2 = \begin{bmatrix} \frac{5}{9} & \frac{4}{9} \\ \frac{4}{9} & \frac{5}{9} \end{bmatrix} \\]
<p>The correct option is the one that matches this resulting matrix.</p>

<hr>

<h4>Question 5</h4>
<p><b>Question:</b> Consider a Discrete Time Markov Chain (DTMC) used to model the weather, where the state \\(W_n\\) on day \\(n\\) can belong to one of two possible states \\(S = \{s_1, s_2\}\\), with \\(s_1\\) denoting sunny and \\(s_2\\) denoting rainy. Given a particular day is sunny, the next day can be rainy with a probability 0.25, while when a particular day is rainy, the next day can also be rainy with a probability 0.15. The one step transition probablity matrix for the DTMC is given as</p>
<p><b>Explanation:</b></p>
<p><i>Note: There appears to be a typo in the question's text. Based on the options, the second condition should likely be "when a particular day is rainy, the next day can be <b>sunny</b> with a probability 0.15". The explanation below assumes this correction.</i></p>
<p>Using the same column-stochastic convention as in Question 2 (where columns sum to 1):</p>
<ul>
    <li><b>States:</b> \\(s_1\\) = sunny, \\(s_2\\) = rainy.</li>
    <li><b>From state \\(s_1\\) (sunny):</b>
        <ul>
            <li>The probability of transitioning to \\(s_2\\) (rainy) is 0.25. This corresponds to the matrix element \\(P_{21} = 0.25\\).</li>
            <li>The probability of remaining \\(s_1\\) (sunny) is \\(P_{11} = 1 - P_{21} = 1 - 0.25 = 0.75\\).</li>
        </ul>
    </li>
    <li><b>From state \\(s_2\\) (rainy):</b>
        <ul>
            <li>Assuming the correction: the probability of transitioning to \\(s_1\\) (sunny) is 0.15. This corresponds to \\(P_{12} = 0.15\\).</li>
            <li>The probability of remaining \\(s_2\\) (rainy) is \\(P_{22} = 1 - P_{12} = 1 - 0.15 = 0.85\\).</li>
        </ul>
    </li>
</ul>
<p>The resulting transition matrix is:</p>
\\[ P = \begin{bmatrix} P_{11} & P_{12} \\ P_{21} & P_{22} \end{bmatrix} = \begin{bmatrix} 0.75 & 0.15 \\ 0.25 & 0.85 \end{bmatrix} \\]
<p>This matrix corresponds to one of the provided options.</p>

<hr>

<h4>Question 6</h4>
<p><b>Question:</b> Consider the sparse signal estimation problem below. The values of the non-zero signal coefficients in the sparse solution are...
\\[ \begin{bmatrix} 1 & 0 & 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 1 & 1 & 0 \\ 1 & 1 & 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \end{bmatrix} = \begin{bmatrix} 0 \\ 2 \\ 8 \\ 10 \end{bmatrix} \\]
</p>
<p><b>Explanation:</b></p>
<p>This question asks for the sparse solution to an underdetermined system of linear equations, \\(Ax=b\\). Let's write out the equations:</p>
<ol>
    <li>\\(x_1 + x_3 + x_6 = 0\\)</li>
    <li>\\(x_2 + x_4 + x_5 = 2\\)</li>
    <li>\\(x_1 + x_2 + x_6 = 8\\)</li>
    <li>\\(x_3 + x_4 + x_5 = 10\\)</li>
</ol>
<p>We can analyze these equations to check for consistency.
Subtracting Equation (1) from Equation (3):
\\[ (x_1 + x_2 + x_6) - (x_1 + x_3 + x_6) = 8 - 0 \\]
\\[ x_2 - x_3 = 8 \\]
Now, subtracting Equation (2) from Equation (4):
\\[ (x_3 + x_4 + x_5) - (x_2 + x_4 + x_5) = 10 - 2 \\]
\\[ x_3 - x_2 = 8 \\]
We have derived two conditions: \\(x_2 - x_3 = 8\\) and \\(x_3 - x_2 = 8\\). The second condition is equivalent to \\(-(x_2 - x_3) = 8\\), or \\(x_2 - x_3 = -8\\).
This leads to the contradiction \\(8 = -8\\).
Because the system of equations is inconsistent, there is <b>no solution</b> for \\(x\\). Therefore, the question is flawed as stated, likely due to a typo in the matrix \\(A\\) or the vector \\(b\\).</p>

<hr>

<h4>Question 7</h4>
<p><b>Question:</b> Consider the one-step transition probability matrix \\(P\\) for a Discrete Time Markov Chain (DTMC) given as
\\[ P = \begin{bmatrix} \frac{2}{3} & \frac{1}{3} \\ \frac{4}{5} & \frac{1}{5} \end{bmatrix} \\]
The two step transition probability for starting from state 2 and ending in state 1 is given as</p>
<p><b>Explanation:</b></p>
<p>This question asks for the probability of transitioning from state 2 to state 1 in two steps. This corresponds to the entry at row 1, column 2 of the two-step transition matrix \\(P^2\\) if using a column-stochastic convention, or the entry at row 2, column 1 if using a row-stochastic convention. Since the rows of \\(P\\) sum to 1, we use the row-stochastic convention where \\(P_{ij}\\) is the probability of moving from state \\(i\\) to state \\(j\\). We need to find the \\((P^2)_{21}\\) element.</p>
<p>First, we calculate \\(P^2\\):</p>
\\[ P^2 = P \times P = \begin{bmatrix} \frac{2}{3} & \frac{1}{3} \\ \frac{4}{5} & \frac{1}{5} \end{bmatrix} \begin{bmatrix} \frac{2}{3} & \frac{1}{3} \\ \frac{4}{5} & \frac{1}{5} \end{bmatrix} \\]
<p>We only need to compute the element in the second row, first column:</p>
\\[ (P^2)_{21} = (\text{row 2 of P}) \cdot (\text{column 1 of P}) \\]
\\[ (P^2)_{21} = \left(\frac{4}{5}\right)\left(\frac{2}{3}\right) + \left(\frac{1}{5}\right)\left(\frac{4}{5}\right) \\]
\\[ (P^2)_{21} = \frac{8}{15} + \frac{4}{25} \\]
<p>To add these fractions, we find a common denominator, which is 75:</p>
\\[ (P^2)_{21} = \frac{8 \times 5}{15 \times 5} + \frac{4 \times 3}{25 \times 3} = \frac{40}{75} + \frac{12}{75} = \frac{52}{75} \\]
<p>The correct probability is <b>52/75</b>.</p>

<hr>

<h4>Question 8</h4>
<p><b>Question:</b> Consider the state sequence \\(X_0, X_1, \dots, X_n\\). The time homogeneous stationary property states that</p>
<p><b>Explanation:</b></p>
<p>A Markov chain is <b>time-homogeneous</b> if its transition probabilities are independent of time. This means the probability of transitioning from one state to another does not depend on how many steps the process has already taken. It only depends on the starting and ending states.</p>
<p>Mathematically, the probability of moving from state \\(s_i\\) to state \\(s_j\\) is the same at any time \\(n\\) as it is at time 0. The option that best represents this is:</p>
<p><b>\\(Pr(X_{n+1} = s_j | X_n) = Pr(X_1 = s_j | X_0)\\)</b></p>
<p>This equation says that the transition probability at step \\(n\\) is the same as the transition probability at step 0, which is the definition of time-homogeneity. (Note: The property itself is just called time-homogeneity; stationarity refers to the distribution, not the transitions).</p>

<hr>

<h4>Question 9</h4>
<p><b>Question:</b> Consider the one step transition probability matrix \\(P\\) for a DTMC and the stationary probability distribution \\(\bar{\pi}\\) such that the sum of elements of \\(\bar{\pi}\\) equals one. Then, \\(\bar{\pi}\\) is</p>
<p><b>Explanation:</b></p>
<p>The stationary distribution \\(\pi\\) is a probability vector that remains unchanged after being multiplied by the transition matrix \\(P\\).
<ul>
    <li>If \\(\pi\\) is a <b>row vector</b>, the condition is \\(\pi P = \pi\\). This can be written as \\(\pi P = 1 \cdot \pi\\), which means \\(\pi\\) is a <b>left eigenvector</b> of \\(P\\) with an eigenvalue of 1. Taking the transpose gives \\(P^T \pi^T = \pi^T\\), meaning \\(\pi^T\\) is a <b>right eigenvector</b> of \\(P^T\\).</li>
    <li>If \\(\pi\\) is a <b>column vector</b>, the condition is \\(P \pi = \pi\\), which means \\(\pi\\) is a <b>right eigenvector</b> of \\(P\\) with an eigenvalue of 1.</li>
</ul>
<p>According to the Perron-Frobenius theorem for stochastic matrices, the eigenvalue 1 is the largest eigenvalue (dominant eigenvalue). The options provided are:</p>
<ul>
    <li>Dominant right singular vector of P</li>
    <li>Belongs to the null-space of P</li>
    <li>An eigenvector of \\(P^T\\)</li>
    <li>Dominant left singular vector of P</li>
</ul>
<p>Based on the analysis, if \\(\pi\\) is treated as a row vector (a common convention), then its transpose \\(\pi^T\\) is a right eigenvector of \\(P^T\\) corresponding to the dominant eigenvalue 1. Therefore, the statement that \\(\pi\\) is related to "<b>An eigenvector of \\(P^T\\)</b>" is correct.</p>

<hr>

<h4>Question 10</h4>
<p><b>Question:</b> The transition probability matrix of a DTMC has the property</p>
<p><b>Explanation:</b></p>
<p>In a transition probability matrix \\(P\\), the element \\(P_{ij}\\) represents the probability of moving from state \\(i\\) to state \\(j\\). For any given starting state \\(i\\), the process must transition to one of the possible states in the next step. The sum of the probabilities of all possible transitions from state \\(i\\) must therefore be equal to 1.</p>
<p>This means that for any row \\(i\\), the sum of all elements in that row must be 1:</p>
\\[ \sum_{j} P_{ij} = 1 \quad \text{for all } i \\]
<p>This property defines a (right) stochastic matrix, which is the standard convention for transition matrices.</p>
<p>Therefore, the correct property is: <b>Elements in each row sum to one</b>.</p>

</div></div>
</body>
</html>
